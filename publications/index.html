
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Publications - AirLab</title>
    
    <link rel="stylesheet" href="/assets/css/app.css">

    <link rel="shortcut icon" type="image/ico" href="/img/favicon/favicon.ico" />

    <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon/favicon-16x16.png">
    <link rel="manifest" href="/img/favicon/site.webmanifest">
    <link rel="mask-icon" href="/img/favicon/safari-pinned-tab.svg" color="#cc002b">
    <meta name="msapplication-TileColor" content="#b91d47">
    <meta name="theme-color" content="#ffffff">

    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  
  

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Publications | AirLab</title>
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Researching, developing, and testing autonomous flying robots at Carnegie Mellon University" />
<meta property="og:description" content="Researching, developing, and testing autonomous flying robots at Carnegie Mellon University" />
<link rel="canonical" href="https://theairlab.org/publications/" />
<meta property="og:url" content="https://theairlab.org/publications/" />
<meta property="og:site_name" content="AirLab" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"description":"Researching, developing, and testing autonomous flying robots at Carnegie Mellon University","headline":"Publications","url":"https://theairlab.org/publications/","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157913889-1"></script>
<script>
  window['ga-disable-UA-157913889-1'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-157913889-1');
  gtag('config', 'G-ERWFHDG4GX');
</script><!-- head scripts --><!-- Added by Chen -->
    <script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5ee902d30e78e50012567eb4&product=inline-follow-buttons&cms=sop' async='async'></script>

    <!-- for mathjax support -->
    

</head>

<body>
    
<nav class="navbar is-primary" >
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item navbar-logo">
                AirLab
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                
                    
                    <div class="navbar-item has-dropdown is-hoverable">
                        <a href="/current-members/" class="navbar-link ">Team</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/current-members/" class="navbar-item ">Current Members</a>
                            
                            <a href="/alumni/" class="navbar-item ">Alumni</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/research/" class="navbar-item ">Research</a>
                    
                
                    
                    <a href="/publications/" class="navbar-item is-active">Publications</a>
                    
                
                    
                    <a href="/news/" class="navbar-item ">News</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable">
                        <a href="" class="navbar-link ">Resources</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/summer2020/" class="navbar-item ">Summer School 2020</a>
                            
                            <a href="https://bitbucket.org/castacks/" class="navbar-item ">BitBucket</a>
                            
                            <a href="https://github.com/castacks" class="navbar-item ">GitHub</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/datasets/" class="navbar-item ">Datasets</a>
                    
                
                    
                    <a href="/openings/" class="navbar-item ">Openings</a>
                    
                
                    
                    <a href="/contact/" class="navbar-item ">Contact</a>
                    
                
                
            </div>
        </div>
    </div>
</nav>
  <section class="hero  is-medium  is-bold is-primary" >
    <div class="hero-body">
        <div class="container">
            <p class="title is-2">Publications</p>
            <p class="subtitle is-3"></p>
            <p class="subtitle is-3">
            
            
        </p>
        </div>
    </div>
</section>

<style>
    .button.is-info {
     background-color:#cc002b;
     border-color:transparent;
     color:#fff
    }
    .button.is-info.is-hovered {
     background-color:#cc002b;
     border-color:transparent;
     color:#fff
    }
</style>
  


    <section class="section">
        <div class="container">
            <div class="columns">
                
                
                <div class="column is-12">
                      
<div class="content">
    <style>
.csl-block {
    font-size: 16px;
}
.csl-title, .csl-author, .csl-event, .csl-editor, .csl-venue {
    display: block;
    position: relative;
    font-size: 16px;
}

.csl-title b {
    font-weight: 600;
}

.csl-content {
    display: inline-block;
    vertical-align: top;
    padding-left: 20px;
}

.bibliography {
   list-style-type: none;
}
</style>

<h1 id="2021">2021</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="rodrigues2021inflight">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>In-flight positional and energy use data set of a DJI Matrice 100 quadcopter for small package delivery</b>. </div><div class="csl-block csl-author">By Rodrigues, T.A., Patrikar, J., Choudhry, A., Feldgoise, J., Arcot, V., Gahlaut, A., Lau, S., Moon, B., Wagner, B., Matthews, H.S., Scherer, S. and Samaras, C.</div><div class="csl-block csl-event">In <i>Submitted to Scientific Data</i>, 2021.</div></div></span>
    <br />
<button class="button0" onclick="togglerodrigues2021inflight()">bibtex</button>

<script>
    function togglerodrigues2021inflight() {
        var x= document.getElementById('arodrigues2021inflight');
        // console.log("haha %o",typeof rodrigues2021inflight);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2rodrigues2021inflight()">abstract</button>


<script>
    function toggle2rodrigues2021inflight() {
        var x= document.getElementById('brodrigues2021inflight');
        // console.log("haha %o",typeof rodrigues2021inflight);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://arxiv.org/abs/2103.13313"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="arodrigues2021inflight" style="display:none"><pre>@article{rodrigues2021inflight,
  title = {In-flight positional and energy use data set of a DJI Matrice 100 quadcopter for small package delivery},
  author = {Rodrigues, Thiago A. and Patrikar, Jay and Choudhry, Arnav and Feldgoise, Jacob and Arcot, Vaibhav and Gahlaut, Aradhana and Lau, Sophia and Moon, Brady and Wagner, Bastian and Matthews, H. Scott and Scherer, Sebastian and Samaras, Constantine},
  year = {2021},
  journal = {Submitted to Scientific Data},
  url = {https://arxiv.org/abs/2103.13313}
}
</pre></div>
<div id="brodrigues2021inflight" style="display:none"><pre>We autonomously direct a small quadcopter package delivery Uncrewed Aerial Vehicle (UAV) or "drone" to take off, fly a specified route, and land for a total of 209 flights while varying a set of operational parameters. The vehicle was equipped with onboard sensors, including GPS, IMU, voltage and current sensors, and an ultrasonic anemometer, to collect high-resolution data on the inertial states, wind speed, and power consumption. Operational parameters, such as commanded ground speed, payload, and cruise altitude, are varied for each flight. This large data set has a total flight time of 10 hours and 45 minutes and was collected from April to October of 2019 covering a total distance of approximately 65 kilometers. The data collected were validated by comparing flights with similar operational parameters. We believe these data will be of great interest to the research and industrial communities, who can use the data to improve UAV designs, safety, and energy efficiency, as well as advance the physical understanding of in-flight operations for package delivery drones.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Scherer:2021">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Resilient and Modular Subterranean Exploration with a Team of Roving and Flying Robots</b>. </div><div class="csl-block csl-author">By Scherer, S., Agrawal, V., Best, G., Cao, C., Cujic, K., Darnley, R., DeBortoli, R., Dexheimer, E., Drozd, B., Garg, R., Higgins, I., Keller, J., Kohanbash, D., Nogueira, L., Pradhan, R., Tatum, M., K. Viswanathan, V., Willits, S., Zhao, S., Zhu, H., Abad, D., Angert, T., Armstrong, G., Boirum, R., Dongare, A., Dworman, M., Hu, S., Jaekel, J., Ji, R., Lai, A., Hsuan Lee, Y., Luong, A., Mangelson, J., Maier, J., Picard, J., Pluckter, K., Saba, A., Saroya, M., Scheide, E., Shoemaker-Trejo, N., Spisak, J., Teza, J., Yang, F., Wilson, A., Zhang, H., Choset, H., Kaess, M., Rowe, A., Singh, S., Zhang, J., A. Hollinger, G. and Travers, M.</div><div class="csl-block csl-event">In <i>Submitted to the Journal of Field Robotics</i>, 2021.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2021()">bibtex</button>

<script>
    function toggleScherer2021() {
        var x= document.getElementById('aScherer:2021');
        // console.log("haha %o",typeof Scherer:2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2021()">abstract</button>


<script>
    function toggle2Scherer2021() {
        var x= document.getElementById('bScherer:2021');
        // console.log("haha %o",typeof Scherer:2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aScherer:2021" style="display:none"><pre>@article{Scherer:2021,
  author = {Scherer, Sebastian and Agrawal, Vasu and Best, Graeme and Cao, Chao and Cujic, Katarina and Darnley, Ryan and DeBortoli, Robert and Dexheimer, Eric and Drozd, Bill and Garg, Rohit and Higgins, Ian and Keller, John and Kohanbash, David and Nogueira, Lucas and Pradhan, Roshan and Tatum, Michael and K. Viswanathan, Vaibhav and Willits, Steven and Zhao, Shibo and Zhu, Hongbiao and Abad, Dan and Angert, Tim and Armstrong, Greg and Boirum, Ralph and Dongare, Adwait and Dworman, Matthew and Hu, Shengjie and Jaekel, Joshua and Ji, Ran and Lai, Alice and Hsuan Lee, Yu and Luong, Anh and Mangelson, Joshua and Maier, Jay and Picard, James and Pluckter, Kevin and Saba, Andrew and Saroya, Manish and Scheide, Emily and Shoemaker-Trejo, Nathaniel and Spisak, Joshua and Teza, Jim and Yang, Fan and Wilson, Andrew and Zhang, Henry and Choset, Howie and Kaess, Michael and Rowe, Anthony and Singh, Sanjiv and Zhang, Ji and A. Hollinger, Geoffrey and Travers, Matthew},
  journal = {Submitted to the Journal of Field Robotics},
  title = {Resilient and Modular Subterranean Exploration with a Team of Roving and Flying Robots},
  year = {2021}
}
</pre></div>
<div id="bScherer:2021" style="display:none"><pre>Subterranean robot exploration is difficult with many mobility, communications, and navi- gation challenges that require an approach with a diverse set of systems, and reliable auton- omy. While prior work has demonstrated partial successes in addressing the problem here we convey a comprehensive approach to address the problem of SubT exploration in a wide range of tunnel, urban, and cave environments. Our approach that is driven by the themes of resiliency and modularity, and we show examples of how these themes influence the design of the different modules. In particular, we detail our approach to artifact detection, pose estimation, coordination, planning, control, and autonomy, and discuss our performance in the tunnel, urban, and self-organized cave circuits.</pre></div>
</li>
<li><div class="text-justify">
    <span id="yang2021">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Graph-Based Topological Exploration Planning in Large-Scale 3D Environments</b>.</div><div class="csl-block csl-author">By Yang, F., Lee, D.-H., Keller, J. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Xi’an, China2021.</div></div></span>
    <br />
<button class="button0" onclick="toggleyang2021()">bibtex</button>

<script>
    function toggleyang2021() {
        var x= document.getElementById('ayang2021');
        // console.log("haha %o",typeof yang2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2yang2021()">abstract</button>


<script>
    function toggle2yang2021() {
        var x= document.getElementById('byang2021');
        // console.log("haha %o",typeof yang2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="ayang2021" style="display:none"><pre>@inproceedings{yang2021,
  author = {Yang, Fan and Lee, Dung-Han and Keller, John and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  title = {Graph-Based Topological Exploration Planning in Large-Scale {3D} Environments},
  year = {2021},
  month = may,
  address = {Xi'an, China}
}
</pre></div>
<div id="byang2021" style="display:none"><pre>Currently, state-of-the-art exploration methods make
various efforts in constructing and maintaining
high-resolution world representations to acquire positions
in configuration space that maximize information gain.
However, those “optimal” selections could quickly become
obsolete due to the influx of new information, especially
in large-scale environments, which results in
high-frequency re-planning that hinders the exploration
efﬁciency. In this paper, we propose a graph-based
topological planning framework, building a sparse
topological map in three-dimensional (3D) space to guide
exploration steps with high-level intents so as to render
consistent exploration maneuvers. Specifically, this work
presents a novel method to represent 3D spaces as convex
polyhedrons, whose geometry information is utilized to
group spaces into distinctive regions. These distinctive
regions are then added as nodes into the topological map,
directing the exploration process. We compared our method
with the state-of-art in simulated environments. The
proposed method achieves better exploration performance in
space coverage and outperforms exploration efﬁciency
by more than 40%. Finally, a field experiment was conducted
to further evaluate the applicability of our method to
empower efficient and robust exploration in real-world
environments.</pre></div>
</li>
<li><div class="text-justify">
    <span id="bonatti2021">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Batteries, Camera, Action! Learning a Semantic Control Space for Expressive Robot Cinematography </b>.</div><div class="csl-block csl-author">By Bonatti, R., Bucker, A., Scherer, S., Mukadam, M. and Hodgins, J.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Xi’an, China2021.</div></div></span>
    <br />
<button class="button0" onclick="togglebonatti2021()">bibtex</button>

<script>
    function togglebonatti2021() {
        var x= document.getElementById('abonatti2021');
        // console.log("haha %o",typeof bonatti2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2bonatti2021()">abstract</button>


<script>
    function toggle2bonatti2021() {
        var x= document.getElementById('bbonatti2021');
        // console.log("haha %o",typeof bonatti2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<a href="https://www.youtube.com/watch?v=6WX2yEUE9_k"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="abonatti2021" style="display:none"><pre>@inproceedings{bonatti2021,
  author = {Bonatti, Rogerio and Bucker, Arthur and Scherer, Sebastian and Mukadam, Mustafa and Hodgins, Jessica},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  title = {Batteries, Camera, Action! Learning a Semantic Control Space for Expressive Robot Cinematography },
  year = {2021},
  month = may,
  address = {Xi'an, China},
  video = {https://www.youtube.com/watch?v=6WX2yEUE9_k}
}
</pre></div>
<div id="bbonatti2021" style="display:none"><pre>Aerial vehicles are revolutionizing the way film-makers can
capture shots of actors by composing novel aerial and
dynamic viewpoints. However, despite great advancements in
autonomous flight technology, generating expressive camera
behaviors is still a challenge and requires non-technical
users to edit a large number of unintuitive control
parameters. In this work we develop a data-driven framework
that enables editing of these complex camera positioning
parameters in a semantic space (e.g. calm, enjoyable,
establishing). First, we generate a database of video clips
with a diverse range of shots in a photo-realistic
simulator, and use hundreds of participants in a
crowd-sourcing framework to obtain scores for a set of
semantic descriptors for each clip. Next, we analyze
correlations between descriptors and build a semantic
control space based on cinematography guidelines and human
perception studies. Finally, we learn a generative model
that can map a set of desired semantic video descriptors
into low-level camera trajectory parameters. We evaluate
our system by demonstrating that our model successfully
generates shots that are rated by participants as having
the expected degrees of expression for each descriptor. We
also show that our models generalize to different scenes in
both simulation and real-world experiments. More results
found on the supplementary video. </pre></div>
</li>
<li><div class="text-justify">
    <span id="bucker2021">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Do You See What I See? Coordinating Multiple Aerial Cameras for Robot Cinematography</b>.</div><div class="csl-block csl-author">By Bucker, A., Bonatti, R. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Xi’an, China2021.</div></div></span>
    <br />
<button class="button0" onclick="togglebucker2021()">bibtex</button>

<script>
    function togglebucker2021() {
        var x= document.getElementById('abucker2021');
        // console.log("haha %o",typeof bucker2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2bucker2021()">abstract</button>


<script>
    function toggle2bucker2021() {
        var x= document.getElementById('bbucker2021');
        // console.log("haha %o",typeof bucker2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<a href="https://www.youtube.com/watch?v=m2R3anv2ADE"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="abucker2021" style="display:none"><pre>@inproceedings{bucker2021,
  author = {Bucker, Arthur and Bonatti, Rogerio and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  title = {Do You See What I See? Coordinating Multiple Aerial Cameras for Robot Cinematography},
  year = {2021},
  month = may,
  address = {Xi'an, China},
  video = {https://www.youtube.com/watch?v=m2R3anv2ADE}
}
</pre></div>
<div id="bbucker2021" style="display:none"><pre>Aerial cinematography is significantly expanding the
capabilities of film-makers. Recent progress in autonomous
unmanned aerial vehicles (UAVs) has further increased the
potential impact of aerial cameras, with systems that can
safely track actors in unstructured cluttered environments.
Professional productions, however, require the use of
multiple cameras simultaneously to record different
viewpoints of the same scene, which are edited into the
final footage either in real time or in post-production.
Such extreme motion coordination is particularly hard for
unscripted action scenes, which are a common use case of
aerial cameras. In this work we develop a real-time
multi-UAV coordination system that is capable of recording
dynamic targets while maximizing shot diversity and
avoiding collisions and mutual visibility between cameras.
We validate our approach in multiple cluttered environments
of a photo-realistic simulator, and deploy the system using
two UAVs in real-world experiments. We show that our
coordination scheme has low computational cost and takes
only 1.17 ms on average to plan for a team of 3 UAVs over a
10 s time horizon. More results can be found on the
supplementary video. </pre></div>
</li>
<li><div class="text-justify">
    <span id="sivaprakasm2021">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Improving Off-Road Planning Techniques with Learned Costs from Physical Interactions</b>.</div><div class="csl-block csl-author">By Sivaprakasam, M., Triest, S., Wang, W., Yin, P. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Xi’an, China2021.</div></div></span>
    <br />
<button class="button0" onclick="togglesivaprakasm2021()">bibtex</button>

<script>
    function togglesivaprakasm2021() {
        var x= document.getElementById('asivaprakasm2021');
        // console.log("haha %o",typeof sivaprakasm2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2sivaprakasm2021()">abstract</button>


<script>
    function toggle2sivaprakasm2021() {
        var x= document.getElementById('bsivaprakasm2021');
        // console.log("haha %o",typeof sivaprakasm2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="asivaprakasm2021" style="display:none"><pre>@inproceedings{sivaprakasm2021,
  author = {Sivaprakasam, Matthew and Triest, Samuel and Wang, Wenshan and Yin, Peng and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  title = {Improving Off-Road Planning Techniques with Learned Costs from Physical Interactions},
  year = {2021},
  month = may,
  address = {Xi'an, China}
}
</pre></div>
<div id="bsivaprakasm2021" style="display:none"><pre>Autonomous ground vehicles have improved greatly over the
past decades, but they still have their limitations when it
comes to off-road environments. There is still a need for
planning techniques that effectively handle physical
interactions between a vehicle and its surroundings. We
present a method of modifying a standard path planning
algorithm to address these problems by incorporating a
learned model to account for complexities that would be too
hard to address manually. The model predicts how well a
vehicle will be able to follow a potential plan in a given
environment. These predictions are then used to assign
costs to their associated paths, where the path predicted
to be the most feasible will be output as the final path.
This results in a planner that doesn’t rely solely on
engineered features to evaluate traversability of
obstacles, and can also choose a better path based on an
understanding of its own capability that it has learned
from previous interactions. This modification was
integrated into the Hybrid A* algorithm and experimental
results demonstrated an improvement of 14.29% over the
original version on a physical platform. </pre></div>
</li>
<li><div class="text-justify">
    <span id="choudhry2021">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>CVaR-Based Flight Energy Risk Assessment for Multirotor UAVs Using a Deep Energy Model</b>.</div><div class="csl-block csl-author">By Choudhry, A., Moon, B., Patrikar, J., Samaras, C. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Xi’an, China2021.</div></div></span>
    <br />
<button class="button0" onclick="togglechoudhry2021()">bibtex</button>

<script>
    function togglechoudhry2021() {
        var x= document.getElementById('achoudhry2021');
        // console.log("haha %o",typeof choudhry2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2choudhry2021()">abstract</button>


<script>
    function toggle2choudhry2021() {
        var x= document.getElementById('bchoudhry2021');
        // console.log("haha %o",typeof choudhry2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<a href="https://t.co/qcYHxa6oOW?amp=1"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="achoudhry2021" style="display:none"><pre>@inproceedings{choudhry2021,
  author = {Choudhry, Arnav and Moon, Brady and Patrikar, Jay and Samaras, Constantine and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  title = {{CVaR}-Based Flight Energy Risk Assessment for Multirotor {UAVs} Using a Deep Energy Model},
  year = {2021},
  month = may,
  address = {Xi'an, China},
  video = {https://t.co/qcYHxa6oOW?amp=1}
}
</pre></div>
<div id="bchoudhry2021" style="display:none"><pre>An important aspect of risk assessment for UAV flights is
energy consumption, as running out of battery during a
flight brings almost guaranteed vehicle damage and a high
risk of property damage or human injuries. Predicting the
amount of energy a flight will consume is challenging as
many factors affect the overall consumption. In this work,
we propose a deep energy model that uses Temporal
Convolutional Networks (TCNs) to capture the time varying
features while incorporating static contextual information.
Our energy model is trained on a real world dataset and
doesn’t require segregating flights into regimes. We
showcase an improvement in power predictions by 35.6% on
test flights when compared to a state-of-the-art analytical
method. Once we have an accurate energy model, we can use
it to predict the energy usage for a given trajectory and
evaluate the risk of running out of battery during flight.
We propose using Conditional Value-at-Risk (CVaR) as a
metric for quantifying this risk. We show that CVaR
captures the risk associated with worst-case energy
consumption on a nominal path by transforming the output
distribution of Monte Carlo forward simulations into a risk
space and computing the CVaR on the risk-space
distribution. Our state-of-the-art energy model and risk
evaluation method helps guarantee safe flights and evaluate
the coverage area from a proposed takeoff location.</pre></div>
</li>
<li><div class="text-justify">
    <span id="keipour:ral:2021">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Real-Time Ellipse Detection for Robotics Applications</b>. </div><div class="csl-block csl-author">By Keipour, A., Pereira, G.A.S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2102.12670</i>, 2021.</div></div></span>
    <br />
<button class="button0" onclick="togglekeipourral2021()">bibtex</button>

<script>
    function togglekeipourral2021() {
        var x= document.getElementById('akeipour:ral:2021');
        // console.log("haha %o",typeof keipour:ral:2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2keipourral2021()">abstract</button>


<script>
    function toggle2keipourral2021() {
        var x= document.getElementById('bkeipour:ral:2021');
        // console.log("haha %o",typeof keipour:ral:2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://arxiv.org/abs/2102.12670"><input type="button" class="button4" value="link" /></a>


<a href="https://www.youtube.com/watch?v=CzR-4aqlOhQ"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="akeipour:ral:2021" style="display:none"><pre>@article{keipour:ral:2021,
  archiveprefix = {arXiv},
  arxivid = {2102.12670},
  journal = {arXiv preprint arXiv:2102.12670},
  author = {Keipour, Azarakhsh and Pereira, Guilherme A. S. and Scherer, Sebastian},
  eprint = {2102.12670},
  title = {Real-Time Ellipse Detection for Robotics Applications},
  url = {https://arxiv.org/abs/2102.12670},
  video = {https://www.youtube.com/watch?v=CzR-4aqlOhQ},
  year = {2021}
}
</pre></div>
<div id="bkeipour:ral:2021" style="display:none"><pre>We propose a new algorithm for real-time detection and tracking of elliptic patterns suitable for real-world robotics applications. The method fits ellipses to each contour in the image frame and rejects ellipses that do not yield a good fit. It can detect complete, partial, and imperfect ellipses in extreme weather and lighting conditions and is lightweight enough to be used on robots’ resource-limited onboard computers. The method is used on an example application of autonomous UAV landing on a fast-moving vehicle to show its performance indoors, outdoors, and in simulation on a real-world robotics task. The comparison with other well-known ellipse detection methods shows that our proposed algorithm outperforms other methods with the F1 score of 0.981 on a dataset with over 1500 frames. The videos of experiments, the source codes, and the collected dataset are provided with the paper. For more information, please visit https://theairlab.org/landing-on-vehicle/.</pre></div>
</li>
<li><div class="text-justify">
    <span id="keipour:iros:2021">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Integration of Fully-Actuated Multirotors into Real-World Applications</b>. </div><div class="csl-block csl-author">By Keipour, A., Mousaei, M., Ashley, A. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2011.06666</i>, 2021.</div></div></span>
    <br />
<button class="button0" onclick="togglekeipouriros2021()">bibtex</button>

<script>
    function togglekeipouriros2021() {
        var x= document.getElementById('akeipour:iros:2021');
        // console.log("haha %o",typeof keipour:iros:2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2keipouriros2021()">abstract</button>


<script>
    function toggle2keipouriros2021() {
        var x= document.getElementById('bkeipour:iros:2021');
        // console.log("haha %o",typeof keipour:iros:2021);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://arxiv.org/abs/2011.06666"><input type="button" class="button4" value="link" /></a>


<a href="https://www.youtube.com/watch?v=lZ3ye1il0W0"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="akeipour:iros:2021" style="display:none"><pre>@article{keipour:iros:2021,
  archiveprefix = {arXiv},
  arxivid = {2011.06666},
  journal = {arXiv preprint arXiv:2011.06666},
  author = {Keipour, Azarakhsh and Mousaei, Mohammadreza and Ashley, Andrew and Scherer, Sebastian},
  eprint = {2011.06666},
  title = {Integration of Fully-Actuated Multirotors into Real-World Applications},
  url = {https://arxiv.org/abs/2011.06666},
  video = {https://www.youtube.com/watch?v=lZ3ye1il0W0},
  year = {2021}
}
</pre></div>
<div id="bkeipour:iros:2021" style="display:none"><pre>The introduction of fully-actuated multirotors has opened the door to new possibilities and more efficient solutions to many real-world applications. However, their integration had been slower than expected, partly due to the need for new tools to take full advantage of these robots.
As far as we know, all the groups currently working on the fully-actuated multirotors develop new full-pose (6-D) tools and methods to use their robots, which is inefficient, time-consuming, and requires many resources.
We propose a way of bridging the gap between the tools already available for underactuated robots and the new fully-actuated vehicles. The approach can extend the existing underactuated flight controllers to support the fully-actuated robots, or enhance the existing fully-actuated controllers to support existing underactuated flight stacks. We introduce attitude strategies that work with the underactuated controllers, tools, planners and remote control interfaces, all while allowing taking advantage of the full actuation. Moreover, new methods are proposed that can properly handle the limited lateral thrust suffered by many fully-actuated UAV designs. The strategies are lightweight, simple, and allow rapid integration of the available tools with these new vehicles for the fast development of new real-world applications.
The real experiments on our robots and simulations on several UAV architectures with different underlying controller methods show how these strategies can be utilized to extend existing flight controllers for fully-actuated applications. We have provided the source code for the PX4 firmware enhanced with our proposed methods to showcase an example flight controller for underactuated multirotors that can be modified to seamlessly support fully-actuated vehicles while retaining the rest of the flight stack unchanged. For more information, please visit https://theairlab.org/fully-actuated/.</pre></div>
</li>
<li><div class="text-justify">
    <span id="he2021model">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Model-based real-time robust controller for a small helicopter</b>. </div><div class="csl-block csl-author">By He, M., He, J. and Scherer, S.</div><div class="csl-block csl-event">In <i>Mechanical Systems and Signal Processing</i>, vol. 146, p. 107022, 2021.</div></div></span>
    <br />
<button class="button0" onclick="togglehe2021model()">bibtex</button>

<script>
    function togglehe2021model() {
        var x= document.getElementById('ahe2021model');
        // console.log("haha %o",typeof he2021model);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2he2021model() {
        var x= document.getElementById('bhe2021model');
        // console.log("haha %o",typeof he2021model);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="ahe2021model" style="display:none"><pre>@article{he2021model,
  title = {Model-based real-time robust controller for a small helicopter},
  author = {He, Miaolei and He, Jilin and Scherer, Sebastian},
  journal = {Mechanical Systems and Signal Processing},
  volume = {146},
  pages = {107022},
  year = {2021},
  publisher = {Academic Press}
}
</pre></div>
<div id="bhe2021model" style="display:none"><pre></pre></div>
</li></ul.no-bullet>

<h1 id="2020">2020</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="zhao2020robust">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A Robust Laser-Inertial Odometry and Mapping Method for Large-Scale Highway Environments</b>. </div><div class="csl-block csl-author">By Zhao, S., Fang, Z., Li, H.L. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2009.02622</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglezhao2020robust()">bibtex</button>

<script>
    function togglezhao2020robust() {
        var x= document.getElementById('azhao2020robust');
        // console.log("haha %o",typeof zhao2020robust);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2zhao2020robust() {
        var x= document.getElementById('bzhao2020robust');
        // console.log("haha %o",typeof zhao2020robust);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="azhao2020robust" style="display:none"><pre>@article{zhao2020robust,
  title = {A Robust Laser-Inertial Odometry and Mapping Method for Large-Scale Highway Environments},
  author = {Zhao, Shibo and Fang, Zheng and Li, HaoLai and Scherer, Sebastian},
  journal = {arXiv preprint arXiv:2009.02622},
  year = {2020}
}
</pre></div>
<div id="bzhao2020robust" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="izquierdo2020feasibility">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Feasibility of discriminating UAV propellers noise from distress signals to locate people in enclosed environments using MEMS microphone arrays</b>. </div><div class="csl-block csl-author">By Izquierdo, A., Val, L.D., Villacorta, J.J., Zhen, W., Scherer, S. and Fang, Z.</div><div class="csl-block csl-event">In <i>Sensors</i>, vol. 20, no. 3, p. 597, 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleizquierdo2020feasibility()">bibtex</button>

<script>
    function toggleizquierdo2020feasibility() {
        var x= document.getElementById('aizquierdo2020feasibility');
        // console.log("haha %o",typeof izquierdo2020feasibility);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2izquierdo2020feasibility() {
        var x= document.getElementById('bizquierdo2020feasibility');
        // console.log("haha %o",typeof izquierdo2020feasibility);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aizquierdo2020feasibility" style="display:none"><pre>@article{izquierdo2020feasibility,
  title = {Feasibility of discriminating {UAV} propellers noise from distress signals to locate people in enclosed environments using {MEMS} microphone arrays},
  author = {Izquierdo, Alberto and Val, Lara Del and Villacorta, Juan J and Zhen, Weikun and Scherer, Sebastian and Fang, Zheng},
  journal = {Sensors},
  volume = {20},
  number = {3},
  pages = {597},
  year = {2020},
  publisher = {Multidisciplinary Digital Publishing Institute}
}
</pre></div>
<div id="bizquierdo2020feasibility" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="chandarana2020human">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Human-in-the-loop Planning and Monitoring of Swarm Search and Service Missions</b>.</div><div class="csl-block csl-author">By Chandarana, M., Lewis, M., Sycara, K. and Scherer, S.</div><div class="csl-block csl-event">In <i>International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS’2020)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="togglechandarana2020human()">bibtex</button>

<script>
    function togglechandarana2020human() {
        var x= document.getElementById('achandarana2020human');
        // console.log("haha %o",typeof chandarana2020human);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2chandarana2020human() {
        var x= document.getElementById('bchandarana2020human');
        // console.log("haha %o",typeof chandarana2020human);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="achandarana2020human" style="display:none"><pre>@inproceedings{chandarana2020human,
  title = {Human-in-the-loop Planning and Monitoring of Swarm Search and Service Missions},
  author = {Chandarana, Meghan and Lewis, Michael and Sycara, Katia and Scherer, Sebastian},
  booktitle = {International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS'2020)},
  year = {2020},
  organization = {IFMAS}
}
</pre></div>
<div id="bchandarana2020human" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="dai2020rgb">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>RGB-D SLAM in Dynamic Environments Using Point Correlations</b>. </div><div class="csl-block csl-author">By Dai, W., Zhang, Y., Li, P., Fang, Z. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggledai2020rgb()">bibtex</button>

<script>
    function toggledai2020rgb() {
        var x= document.getElementById('adai2020rgb');
        // console.log("haha %o",typeof dai2020rgb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2dai2020rgb() {
        var x= document.getElementById('bdai2020rgb');
        // console.log("haha %o",typeof dai2020rgb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/TPAMI.2020.3010942"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="adai2020rgb" style="display:none"><pre>@article{dai2020rgb,
  title = {{RGB-D SLAM} in Dynamic Environments Using Point Correlations},
  author = {Dai, Weichen and Zhang, Yu and Li, Ping and Fang, Zheng and Scherer, Sebastian},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/TPAMI.2020.3010942}
}
</pre></div>
<div id="bdai2020rgb" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="wang2020lifelong">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Lifelong Graph Learning</b>. </div><div class="csl-block csl-author">By Wang, C., Qiu, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2009.00647</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglewang2020lifelong()">bibtex</button>

<script>
    function togglewang2020lifelong() {
        var x= document.getElementById('awang2020lifelong');
        // console.log("haha %o",typeof wang2020lifelong);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2wang2020lifelong() {
        var x= document.getElementById('bwang2020lifelong');
        // console.log("haha %o",typeof wang2020lifelong);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="awang2020lifelong" style="display:none"><pre>@article{wang2020lifelong,
  title = {Lifelong Graph Learning},
  author = {Wang, Chen and Qiu, Yuheng and Scherer, Sebastian},
  journal = {arXiv preprint arXiv:2009.00647},
  year = {2020}
}
</pre></div>
<div id="bwang2020lifelong" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="patrikar2020real">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Real-time Motion Planning of Curvature Continuous Trajectories for Urban UAV Operations in Wind</b>.</div><div class="csl-block csl-author">By Patrikar, J., Dugar, V., Arcot, V. and Scherer, S.</div><div class="csl-block csl-event">In <i>2020 International Conference on Unmanned Aircraft Systems (ICUAS)</i>, pp. 854–861, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglepatrikar2020real()">bibtex</button>

<script>
    function togglepatrikar2020real() {
        var x= document.getElementById('apatrikar2020real');
        // console.log("haha %o",typeof patrikar2020real);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2patrikar2020real() {
        var x= document.getElementById('bpatrikar2020real');
        // console.log("haha %o",typeof patrikar2020real);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="apatrikar2020real" style="display:none"><pre>@inproceedings{patrikar2020real,
  title = {Real-time Motion Planning of Curvature Continuous Trajectories for Urban {UAV} Operations in Wind},
  author = {Patrikar, Jay and Dugar, Vishal and Arcot, Vaibhav and Scherer, Sebastian},
  booktitle = {2020 International Conference on Unmanned Aircraft Systems (ICUAS)},
  pages = {854--861},
  year = {2020}
}
</pre></div>
<div id="bpatrikar2020real" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="fang20203d">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>3D-SiamRPN: An End-to-end Learning Method for Real-time 3D Single Object Tracking using Raw Point Cloud</b>. </div><div class="csl-block csl-author">By Fang, Z., Zhou, S., Cui, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Sensors Journal</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglefang20203d()">bibtex</button>

<script>
    function togglefang20203d() {
        var x= document.getElementById('afang20203d');
        // console.log("haha %o",typeof fang20203d);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2fang20203d() {
        var x= document.getElementById('bfang20203d');
        // console.log("haha %o",typeof fang20203d);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/JSEN.2020.3033034"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="afang20203d" style="display:none"><pre>@article{fang20203d,
  title = {{3D-SiamRPN}: {An} End-to-end Learning Method for Real-time {3D} Single Object Tracking using Raw Point Cloud},
  author = {Fang, Zheng and Zhou, Sifan and Cui, Yubo and Scherer, Sebastian},
  journal = {IEEE Sensors Journal},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/JSEN.2020.3033034}
}
</pre></div>
<div id="bfang20203d" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="li2020ulsd">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras</b>. </div><div class="csl-block csl-author">By Li, H., Yu, H., Yang, W., Yu, L. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2011.03174</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleli2020ulsd()">bibtex</button>

<script>
    function toggleli2020ulsd() {
        var x= document.getElementById('ali2020ulsd');
        // console.log("haha %o",typeof li2020ulsd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2li2020ulsd() {
        var x= document.getElementById('bli2020ulsd');
        // console.log("haha %o",typeof li2020ulsd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="ali2020ulsd" style="display:none"><pre>@article{li2020ulsd,
  title = {{ULSD:} {Unified} Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras},
  author = {Li, Hao and Yu, Huai and Yang, Wen and Yu, Lei and Scherer, Sebastian},
  journal = {arXiv preprint arXiv:2011.03174},
  year = {2020}
}
</pre></div>
<div id="bli2020ulsd" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="bucker2020you">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Do You See What I See? Coordinating Multiple Aerial Cameras for Robot Cinematography</b>. </div><div class="csl-block csl-author">By Bucker, A., Bonatti, R. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2011.05437</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglebucker2020you()">bibtex</button>

<script>
    function togglebucker2020you() {
        var x= document.getElementById('abucker2020you');
        // console.log("haha %o",typeof bucker2020you);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2bucker2020you() {
        var x= document.getElementById('bbucker2020you');
        // console.log("haha %o",typeof bucker2020you);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="abucker2020you" style="display:none"><pre>@article{bucker2020you,
  title = {Do You See What I See? {Coordinating} Multiple Aerial Cameras for Robot Cinematography},
  author = {Bucker, Arthur and Bonatti, Rogerio and Scherer, Sebastian},
  journal = {arXiv preprint arXiv:2011.05437},
  year = {2020}
}
</pre></div>
<div id="bbucker2020you" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="bonatti2020batteries">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Batteries, camera, action! Learning a semantic control space for expressive robot cinematography</b>. </div><div class="csl-block csl-author">By Bonatti, R., Bucker, A., Scherer, S., Mukadam, M. and Hodgins, J.</div><div class="csl-block csl-event">In <i>arXiv preprint arXiv:2011.10118</i>, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglebonatti2020batteries()">bibtex</button>

<script>
    function togglebonatti2020batteries() {
        var x= document.getElementById('abonatti2020batteries');
        // console.log("haha %o",typeof bonatti2020batteries);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2bonatti2020batteries() {
        var x= document.getElementById('bbonatti2020batteries');
        // console.log("haha %o",typeof bonatti2020batteries);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="abonatti2020batteries" style="display:none"><pre>@article{bonatti2020batteries,
  title = {Batteries, camera, action! Learning a semantic control space for expressive robot cinematography},
  author = {Bonatti, Rogerio and Bucker, Arthur and Scherer, Sebastian and Mukadam, Mustafa and Hodgins, Jessica},
  journal = {arXiv preprint arXiv:2011.10118},
  year = {2020}
}
</pre></div>
<div id="bbonatti2020batteries" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="tartanvo2020corl">[11]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>TartanVO: A Generalizable Learning-based VO</b>.</div><div class="csl-block csl-author">By Wang, W., Hu, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>Conference on Robot Learning (CoRL)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="toggletartanvo2020corl()">bibtex</button>

<script>
    function toggletartanvo2020corl() {
        var x= document.getElementById('atartanvo2020corl');
        // console.log("haha %o",typeof tartanvo2020corl);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2tartanvo2020corl()">abstract</button>


<script>
    function toggle2tartanvo2020corl() {
        var x= document.getElementById('btartanvo2020corl');
        // console.log("haha %o",typeof tartanvo2020corl);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://arxiv.org/abs/2011.00359"><input type="button" class="button4" value="link" /></a>


<a href="https://www.youtube.com/watch?v=NQ1UEh3thbU"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="atartanvo2020corl" style="display:none"><pre>@inproceedings{tartanvo2020corl,
  title = {{TartanVO: A} Generalizable Learning-based {VO}},
  author = {Wang, Wenshan and Hu, Yaoyu and Scherer, Sebastian},
  booktitle = {Conference on Robot Learning (CoRL)},
  url = {https://arxiv.org/abs/2011.00359},
  video = {https://www.youtube.com/watch?v=NQ1UEh3thbU},
  year = {2020}
}
</pre></div>
<div id="btartanvo2020corl" style="display:none"><pre>We present the first learning-based visual odometry (VO) model, which generalizes to multiple datasets and real-world scenarios and outperforms geometry-based methods in challenging scenes. We achieve this by leveraging the SLAM dataset TartanAir, which provides a large amount of diverse synthetic data in challenging environments. Furthermore, to make our VO model generalize across datasets, we propose an up-to-scale loss function and incorporate the camera intrinsic parameters into the model. Experiments show that a single model, TartanVO, trained only on synthetic data, without any finetuning, can be generalized to real-world datasets such as KITTI and EuRoC, demonstrating significant advantages over the geometry-based methods on challenging trajectories. Our code is available at https://github.com/castacks/tartanvo</pre></div>
</li>
<li><div class="text-justify">
    <span id="keipour2020ijrr">[12]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>ALFA: A dataset for UAV fault and anomaly detection</b>. </div><div class="csl-block csl-author">By Keipour, A., Mousaei, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>The International Journal of Robotics Research</i>, Oct. 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglekeipour2020ijrr()">bibtex</button>

<script>
    function togglekeipour2020ijrr() {
        var x= document.getElementById('akeipour2020ijrr');
        // console.log("haha %o",typeof keipour2020ijrr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2keipour2020ijrr()">abstract</button>


<script>
    function toggle2keipour2020ijrr() {
        var x= document.getElementById('bkeipour2020ijrr');
        // console.log("haha %o",typeof keipour2020ijrr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1177/0278364920966642"><input type="button" class="button1" value="doi" /></a>



<a href="https://arxiv.org/abs/1907.06268"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="akeipour2020ijrr" style="display:none"><pre>@article{keipour2020ijrr,
  archiveprefix = {arXiv},
  arxivid = {1907.06268},
  author = {Keipour, Azarakhsh and Mousaei, Mohammadreza and Scherer, Sebastian},
  doi = {10.1177/0278364920966642},
  eprint = {1907.06268},
  issn = {0278-3649},
  journal = {The International Journal of Robotics Research},
  keywords = {Dataset,actuator failure,anomaly detection,autonomous robots,engine failure,evaluation metrics,fault detection and isolation,fixed-wing robots,flight safety,unmanned aerial vehicles},
  month = oct,
  publisher = {SAGE Publications Ltd STM},
  title = {{ALFA:} A dataset for {UAV} fault and anomaly detection},
  url = {https://arxiv.org/abs/1907.06268},
  year = {2020}
}
</pre></div>
<div id="bkeipour2020ijrr" style="display:none"><pre>We present a dataset of several fault types in control surfaces of a fixed-wing unmanned aerial vehicle (UAV) for use in fault detection and isolation (FDI) and anomaly detection (AD) research. Currently, the dataset includes processed data for 47 autonomous flights with 23 sudden full engine failure scenarios and 24 scenarios for 7 other types of sudden control surface (actuator) faults, with a total of 66 minutes of flight under normal conditions and 13 minutes of post-fault flight time. It additionally includes many hours of raw data of fully autonomous, autopilot-assisted and manual flights with tens of fault scenarios. The ground truth of the time and type of faults is provided in each scenario to enable evaluation of the methods using the dataset. We have also provided the helper tools in several programming languages to load and work with the data and to help the evaluation of a detection method using the dataset. A set of metrics is proposed to help to compare different methods using the dataset. Most of the current fault detection methods are evaluated in simulation and, as far as we know, this dataset is the only one providing the real flight data with faults in such capacity. We hope it will help advance the state of the art in AD or FDI research for autonomous aerial vehicles and mobile robots to enhance the safety of autonomous and remote flight operations further. The dataset and the provided tools can be accessed from https://doi.org/10.1184/R1/12707963 .</pre></div>
</li>
<li><div class="text-justify">
    <span id="jaekel2020">[13]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A Robust Multi-Stereo Visual-Inertial Odometry Pipeline</b>.</div><div class="csl-block csl-author">By Jaekel, J., Mangelson, J., Scherer, S. and Kaess, M.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="togglejaekel2020()">bibtex</button>

<script>
    function togglejaekel2020() {
        var x= document.getElementById('ajaekel2020');
        // console.log("haha %o",typeof jaekel2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2jaekel2020()">abstract</button>


<script>
    function toggle2jaekel2020() {
        var x= document.getElementById('bjaekel2020');
        // console.log("haha %o",typeof jaekel2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.cs.cmu.edu/%7Ekaess/pub/Jaekel20iros.html"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="ajaekel2020" style="display:none"><pre>@inproceedings{jaekel2020,
  author = {Jaekel, Joshua and Mangelson, Joshua and Scherer, Sebastian and Kaess, Michael},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {A Robust Multi-Stereo Visual-Inertial Odometry Pipeline},
  year = {2020},
  url = {https://www.cs.cmu.edu/%7Ekaess/pub/Jaekel20iros.html}
}
</pre></div>
<div id="bjaekel2020" style="display:none"><pre>In this paper we present a novel multi-stereo visual-inertial odometry (VIO) framework which aims to improve the robustness of a robotâ€™s state estimate during aggressive motion and in visually challenging environments. Our system uses a fixed-lag smoother which jointly optimizes for poses and landmarks across all stereo pairs. We propose a 1-point RANdom SAmple Consensus (RANSAC) algorithm which is able to perform outlier rejection across features from all stereo pairs. To handle the problem of noisy extrinsics, we account for uncertainty in the calibration of each stereo pair and model it in both our front-end and back-end. The result is a VIO system which is able to maintain an accurate state estimate under conditions that have typically proven to be challenging for traditional state-of-the-art VIO systems. We demonstrate the benefits of our proposed multi-stereo algorithm by evaluating it with both simulated and real world data. We show that our proposed algorithm is able to maintain a state estimate in scenarios where traditional VIO algorithms fail.</pre></div>
</li>
<li><div class="text-justify">
    <span id="dexheimer2020">[14]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Efficient Multiresolution Scrolling Grid for Stereo Vision-based MAV Obstacle Avoidance</b>.</div><div class="csl-block csl-author">By Dexheimer, E., Mangelson, J., Scherer, S. and Kaess, M.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="toggledexheimer2020()">bibtex</button>

<script>
    function toggledexheimer2020() {
        var x= document.getElementById('adexheimer2020');
        // console.log("haha %o",typeof dexheimer2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2dexheimer2020()">abstract</button>


<script>
    function toggle2dexheimer2020() {
        var x= document.getElementById('bdexheimer2020');
        // console.log("haha %o",typeof dexheimer2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.cs.cmu.edu/%7Ekaess/pub/Dexheimer20iros.html"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="adexheimer2020" style="display:none"><pre>@inproceedings{dexheimer2020,
  author = {Dexheimer, Eric and Mangelson, Joshua and Scherer, Sebastian and Kaess, Michael},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Efficient Multiresolution Scrolling Grid for Stereo Vision-based {MAV} Obstacle Avoidance},
  year = {2020},
  url = {https://www.cs.cmu.edu/%7Ekaess/pub/Dexheimer20iros.html}
}
</pre></div>
<div id="bdexheimer2020" style="display:none"><pre>Fast, aerial navigation in cluttered environments requires a suitable map representation for path planning. In this paper, we propose the use of an efficient, structured multiresolution representation that expands the sensor range of dense local grids for memory-constrained platforms. While similar data structures have been proposed, we avoid processing redundant occupancy information and use the organization of the grid to improve efficiency. By layering 3D circular buffers that double in resolution at each level, obstacles near the robot are represented at finer resolutions while coarse spatial information is maintained at greater distances. We also introduce a novel method for efficiently calculating the Euclidean distance transform on the multiresolution grid by leveraging its structure. Lastly, we utilize our proposed framework to demonstrate improved stereo camera-based MAV obstacle avoidance with an optimization-based planner in simulation.</pre></div>
</li>
<li><div class="text-justify">
    <span id="patrikar2020">[15]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Wind and the City: Utilizing UAV-Based In-Situ Measurements for Estimating Urban Wind Fields</b>.</div><div class="csl-block csl-author">By Patrikar, J., Moon, B. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="togglepatrikar2020()">bibtex</button>

<script>
    function togglepatrikar2020() {
        var x= document.getElementById('apatrikar2020');
        // console.log("haha %o",typeof patrikar2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2patrikar2020()">abstract</button>


<script>
    function toggle2patrikar2020() {
        var x= document.getElementById('bpatrikar2020');
        // console.log("haha %o",typeof patrikar2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.ri.cmu.edu/publications/wind-and-the-city-utilizing-uav-based-in-situ-measurements-for-estimating-urban-wind-fields/"><input type="button" class="button4" value="link" /></a>


<a href="https://youtu.be/U4XdYgSJRZM"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="apatrikar2020" style="display:none"><pre>@inproceedings{patrikar2020,
  author = {Patrikar, Jay and Moon, Brady and Scherer, Sebastian},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Wind and the City: Utilizing {UAV}-Based In-Situ Measurements for Estimating Urban Wind Fields},
  year = {2020},
  url = {https://www.ri.cmu.edu/publications/wind-and-the-city-utilizing-uav-based-in-situ-measurements-for-estimating-urban-wind-fields/},
  video = {https://youtu.be/U4XdYgSJRZM}
}
</pre></div>
<div id="bpatrikar2020" style="display:none"><pre>A high-quality estimate of wind fields can potentially improve the safety and performance of Unmanned Aerial Vehicles (UAVs) operating in dense urban areas. Computational Fluid Dynamics (CFD) simulations can help provide a wind field estimate, but their accuracy depends on the knowledge of the distribution of the inlet boundary conditions. This paper provides a real-time methodology using a Particle Filter (PF) that utilizes wind measurements from a UAV to solve the inverse problem of predicting the inlet conditions as the UAV traverses the flow field. A Gaussian Process Regression (GPR) approach is used as a surrogate function to maintain the real-time nature of the proposed methodology. Real-world experiments with a UAV at an urban test-site prove the efficacy of the proposed method. The flight test shows that the 95%confidence interval for the difference between the mean estimated inlet conditions and mean ground truth measurements closely bound zero, with the difference in mean angles being between −3.7° and 1.3° and the difference in mean magnitudes being between −0.2 m/s and 0.0 m/s. Video: https://youtu.be/U4XdYgSJRZM</pre></div>
</li>
<li><div class="text-justify">
    <span id="shibo2020">[16]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>TP-TIO: A Robust Thermal-Inertial Odometry with Deep ThermalPoint</b>.</div><div class="csl-block csl-author">By Zhao, S., Wang, P., Zhang, H., Fang, Z. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleshibo2020()">bibtex</button>

<script>
    function toggleshibo2020() {
        var x= document.getElementById('ashibo2020');
        // console.log("haha %o",typeof shibo2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2shibo2020() {
        var x= document.getElementById('bshibo2020');
        // console.log("haha %o",typeof shibo2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<a href="https://youtu.be/sBqW6GD9Vjg"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="ashibo2020" style="display:none"><pre>@inproceedings{shibo2020,
  author = {Zhao, Shibo and Wang, Peng and Zhang, Hengrui and Fang, Zheng and Scherer, Sebastian},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {{TP-TIO: A} Robust Thermal-Inertial Odometry with Deep ThermalPoint},
  year = {2020},
  video = {https://youtu.be/sBqW6GD9Vjg}
}
</pre></div>
<div id="bshibo2020" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="vai2020">[17]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Efficient Trajectory Library Filtering for Quadrotor Flight in Unknown Environments</b>.</div><div class="csl-block csl-author">By Viswanathan, V., Dexheimer, E., Li, G., Loianno, G., Kaess, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="togglevai2020()">bibtex</button>

<script>
    function togglevai2020() {
        var x= document.getElementById('avai2020');
        // console.log("haha %o",typeof vai2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2vai2020() {
        var x= document.getElementById('bvai2020');
        // console.log("haha %o",typeof vai2020);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<a href="https://youtu.be/y_lVtT8lJMk"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="avai2020" style="display:none"><pre>@inproceedings{vai2020,
  author = {Viswanathan, Vaibhav and Dexheimer, Eric and Li, Guanrui and Loianno, Giuseppe and Kaess, Michael and Scherer, Sebastian},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Efficient Trajectory Library Filtering for Quadrotor Flight in Unknown Environments},
  year = {2020},
  video = {https://youtu.be/y_lVtT8lJMk}
}
</pre></div>
<div id="bvai2020" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="wang2020visual">[18]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Visual Memorability for Robotic Interestingness via Unsupervised Online Learning</b>.</div><div class="csl-block csl-author">By Wang, C., Wang, W., Qiu, Y., Hu, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>European Conference on Computer Vision (ECCV)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="togglewang2020visual()">bibtex</button>

<script>
    function togglewang2020visual() {
        var x= document.getElementById('awang2020visual');
        // console.log("haha %o",typeof wang2020visual);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2wang2020visual()">abstract</button>


<script>
    function toggle2wang2020visual() {
        var x= document.getElementById('bwang2020visual');
        // console.log("haha %o",typeof wang2020visual);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://arxiv.org/abs/2005.08829"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="awang2020visual" style="display:none"><pre>@inproceedings{wang2020visual,
  author = {Wang, Chen and Wang, Wenshan and Qiu, Yuheng and Hu, Yafei and Scherer, Sebastian},
  booktitle = {European Conference on Computer Vision (ECCV)},
  title = {Visual Memorability for Robotic Interestingness via Unsupervised Online Learning},
  year = {2020},
  keywords = {Unsupervised, Online, Memorability, Interestingness},
  url = {https://arxiv.org/abs/2005.08829}
}
</pre></div>
<div id="bwang2020visual" style="display:none"><pre>In this paper, we aim to solve the problem of interesting scene prediction for mobile robots. This area is currently under explored but is crucial for many practical applications such as autonomous exploration and decision making. First, we expect a robot to detect novel and interesting scenes in unknown environments and lose interests over time after repeatedly observing similar objects. Second, we expect the robots to learn from unbalanced data in a short time, as the robots normally only know the uninteresting scenes before they are deployed. Inspired by those industrial demands, we first propose a novel translation-invariant visual memory for recalling and identifying interesting scenes, then design a three-stage architecture of long-term, short-term, and online learning for human-like experience, environmental knowledge, and online adaption, respectively. It is demonstrated that our approach is able to learn online and find interesting scenes for practical exploration tasks. It also achieves a much higher accuracy than the state-of-the-art algorithm on very challenging robotic interestingness prediction datasets.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Bonatti:2019wb">[19]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous aerial cinematography in unstructured environments with learned artistic decision-making</b>. </div><div class="csl-block csl-author">By Bonatti, R., Wang, W., Ho, C., Ahuja, A., Gschwindt, M., Camci, E., Kayacan, E., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Journal of Field Robotics</i>, Jan. 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleBonatti2019wb()">bibtex</button>

<script>
    function toggleBonatti2019wb() {
        var x= document.getElementById('aBonatti:2019wb');
        // console.log("haha %o",typeof Bonatti:2019wb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Bonatti2019wb()">abstract</button>


<script>
    function toggle2Bonatti2019wb() {
        var x= document.getElementById('bBonatti:2019wb');
        // console.log("haha %o",typeof Bonatti:2019wb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1002/rob.21931"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aBonatti:2019wb" style="display:none"><pre>@article{Bonatti:2019wb,
  author = {Bonatti, Rogerio and Wang, Wenshan and Ho, Cherie and Ahuja, Aayush and Gschwindt, Mirko and Camci, Efe and Kayacan, Erdal and Choudhury, Sanjiban and Scherer, Sebastian},
  doi = {10.1002/rob.21931},
  issn = {15564967},
  journal = {Journal of Field Robotics},
  keywords = {aerial robotics,cinematography,computer vision,learning,mapping,motion planning},
  month = jan,
  title = {Autonomous aerial cinematography in unstructured environments with learned artistic decision-making},
  year = {2020}
}
</pre></div>
<div id="bBonatti:2019wb" style="display:none"><pre>Aerial cinematography is revolutionizing industries that require live and dynamic camera viewpoints such as entertainment, sports, and security. However, safely piloting a drone while filming a moving target in the presence of obstacles is immensely taxing, often requiring multiple expert human operators. Hence, there is a demand for an autonomous cinematographer that can reason about both geometry and scene context in real-time. Existing approaches do not address all aspects of this problem; they either require high-precision motion-capture systems or global positioning system tags to localize targets, rely on prior maps of the environment, plan for short time horizons, or only follow fixed artistic guidelines specified before the flight. In this study, we address the problem in its entirety and propose a complete system for real-time aerial cinematography that for the first time combines: (a) vision-based target estimation; (b) 3D signed-distance mapping for occlusion estimation; (c) efficient trajectory optimization for long time-horizon camera motion; and (d) learning-based artistic shot selection. We extensively evaluate our system both in simulation and in field experiments by filming dynamic targets moving through unstructured environments. Our results indicate that our system can operate reliably in the real world without restrictive assumptions. We also provide in-depth analysis and discussions for each module, with the hope that our design tradeoffs can generalize to other related applications. Videos of the complete system can be found at https://youtu.be/ookhHnqmlaU.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Bonatti-2018-111051">[20]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous Drone Cinematographer: Using Artistic Principles to Create Smooth, Safe, Occlusion-Free Trajectories for Aerial Filming</b>.</div><div class="csl-block csl-author">By Bonatti, R., Zhang, Y., Choudhury, S., Wang, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>International Symposium on Experimental Robotics</i>, pp. 119–129, 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleBonatti2018111051()">bibtex</button>

<script>
    function toggleBonatti2018111051() {
        var x= document.getElementById('aBonatti-2018-111051');
        // console.log("haha %o",typeof Bonatti-2018-111051);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Bonatti2018111051()">abstract</button>


<script>
    function toggle2Bonatti2018111051() {
        var x= document.getElementById('bBonatti-2018-111051');
        // console.log("haha %o",typeof Bonatti-2018-111051);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-030-33950-0_11"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aBonatti-2018-111051" style="display:none"><pre>@inproceedings{Bonatti-2018-111051,
  archiveprefix = {arXiv},
  arxivid = {1808.09563},
  author = {Bonatti, Rogerio and Zhang, Yanfu and Choudhury, Sanjiban and Wang, Wenshan and Scherer, Sebastian},
  booktitle = {International Symposium on Experimental Robotics},
  doi = {10.1007/978-3-030-33950-0_11},
  eprint = {1808.09563},
  month = nov,
  pages = {119--129},
  title = {Autonomous Drone Cinematographer: Using Artistic Principles to Create Smooth, Safe, Occlusion-Free Trajectories for Aerial Filming},
  year = {2020}
}
</pre></div>
<div id="bBonatti-2018-111051" style="display:none"><pre>Autonomous aerial cinematography has the potential to enable automatic capture of aesthetically pleasing videos without requiring human intervention, empowering individuals with the capability of high-end film studios. Current approaches either only handle off-line trajectory generation, or offer strategies that reason over short time horizons and simplistic representations for obstacles, which result in jerky movement and low real-life applicability. In this work we develop a method for aerial filming that is able to trade off shot smoothness, occlusion, and cinematography guidelines in a principled manner, even under noisy actor predictions. We present a novel algorithm for real-time covariant gradient descent that we use to efficiently find the desired trajectories by optimizing a set of cost functions. Experimental results show that our approach creates attractive shots, avoiding obstacles and occlusion 65 times over 1.25 hours of flight time, re-planning at 5 Hz with a 10 s time horizon. We robustly film human actors, cars and bicycles performing different motion among obstacles, using various shot types.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Hu:2019vf">[21]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction</b>.</div><div class="csl-block csl-author">By Hu, Y., Zhen, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>2020 IEEE International Conference on Robotics and Automation (ICRA)</i>, pp. 8637–8643, 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleHu2019vf()">bibtex</button>

<script>
    function toggleHu2019vf() {
        var x= document.getElementById('aHu:2019vf');
        // console.log("haha %o",typeof Hu:2019vf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Hu2019vf()">abstract</button>


<script>
    function toggle2Hu2019vf() {
        var x= document.getElementById('bHu:2019vf');
        // console.log("haha %o",typeof Hu:2019vf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA40945.2020.9196655"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aHu:2019vf" style="display:none"><pre>@inproceedings{Hu:2019vf,
  author = {Hu, Yaoyu and Zhen, Weikun and Scherer, Sebastian},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction},
  year = {2020},
  volume = {},
  number = {},
  pages = {8637-8643},
  doi = {10.1109/ICRA40945.2020.9196655}
}
</pre></div>
<div id="bHu:2019vf" style="display:none"><pre>This work presents dense stereo reconstruction using high-resolution images for infrastructure inspections. The state-of-the-art stereo reconstruction methods, both learning and non-learning ones, consume too much computational resource on high-resolution data. Recent learning-based methods achieve top ranks on most benchmarks. However, they suffer from the generalization issue due to lack of task-specific training data. We propose to use a less resource demanding non-learning method, guided by a learning-based model, to handle high-resolution images and achieve accurate stereo reconstruction. The deep-learning model produces an initial disparity prediction with uncertainty for each pixel of the down-sampled stereo image pair. The uncertainty serves as a self-measurement of its generalization ability and the per-pixel searching range around the initially predicted disparity. The downstream process performs a modified version of the Semi-Global Block Matching method with the up-sampled per-pixel searching range. The proposed deep-learning assisted method is evaluated on the Middlebury dataset and high-resolution stereo images collected by our customized binocular stereo camera. The combination of learning and non-learning methods achieves better performance on 12 out of 15 cases of the Middlebury dataset. In our infrastructure inspection experiments, the average 3D reconstruction error is less than 0.004m.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Zhen:2019up">[22]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>LiDAR-enhanced Structure-from-Motion</b>.</div><div class="csl-block csl-author">By Zhen, W., Hu, Y., Yu, H. and Scherer, S.</div><div class="csl-block csl-event">In <i>2020 IEEE International Conference on Robotics and Automation (ICRA)</i>, pp. 6773–6779, 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2019up()">bibtex</button>

<script>
    function toggleZhen2019up() {
        var x= document.getElementById('aZhen:2019up');
        // console.log("haha %o",typeof Zhen:2019up);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhen2019up()">abstract</button>


<script>
    function toggle2Zhen2019up() {
        var x= document.getElementById('bZhen:2019up');
        // console.log("haha %o",typeof Zhen:2019up);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA40945.2020.9197030"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aZhen:2019up" style="display:none"><pre>@inproceedings{Zhen:2019up,
  annote = {6 pages plus reference. Work has been submitted to ICRA 2020},
  author = {Zhen, Weikun and Hu, Yaoyu and Yu, Huai and Scherer, Sebastian},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {{LiDAR}-enhanced Structure-from-Motion},
  year = {2020},
  volume = {},
  number = {},
  pages = {6773-6779},
  doi = {10.1109/ICRA40945.2020.9197030}
}
</pre></div>
<div id="bZhen:2019up" style="display:none"><pre>Although Structure-from-Motion (SfM) as a maturing technique has been widely used in many applications, state-of-the-art SfM algorithms are still not robust enough in certain situations. For example, images for inspection purposes are often taken in close distance to obtain detailed textures, which will result in less overlap between images and thus decrease the accuracy of estimated motion. In this paper, we propose a LiDAR-enhanced SfM pipeline that jointly processes data from a rotating LiDAR and a stereo camera pair to estimate sensor motions. We show that incorporating LiDAR helps to effectively reject falsely matched images and significantly improve the model consistency in large-scale environments. Experiments are conducted in different environments to test the performance of the proposed pipeline and comparison results with the state-of-the-art SfM algorithms are reported.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yu:2019ub">[23]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Line-Based 2D–3D Registration and Camera Localization in Structured Environments</b>. </div><div class="csl-block csl-author">By Yu, H., Zhen, W., Yang, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Transactions on Instrumentation and Measurement</i>, vol. 69, no. 11, pp. 8962–8972, Jul. 2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleYu2019ub()">bibtex</button>

<script>
    function toggleYu2019ub() {
        var x= document.getElementById('aYu:2019ub');
        // console.log("haha %o",typeof Yu:2019ub);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yu2019ub()">abstract</button>


<script>
    function toggle2Yu2019ub() {
        var x= document.getElementById('bYu:2019ub');
        // console.log("haha %o",typeof Yu:2019ub);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/TIM.2020.2999137"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aYu:2019ub" style="display:none"><pre>@article{Yu:2019ub,
  author = {Yu, Huai and Zhen, Weikun and Yang, Wen and Scherer, Sebastian},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  month = jul,
  title = {Line-Based {2D–3D} Registration and Camera Localization in Structured Environments},
  year = {2020},
  volume = {69},
  number = {11},
  pages = {8962-8972},
  doi = {10.1109/TIM.2020.2999137}
}
</pre></div>
<div id="bYu:2019ub" style="display:none"><pre>Accurate registration of 2D imagery with point clouds is a key technology for image-LiDAR point cloud fusion, camera to laser scanner calibration and camera localization. Despite continuous improvements, automatic registration of 2D and 3D data without using additional textured information still faces great challenges. In this paper, we propose a new 2D-3D registration method to estimate 2D-3D line feature correspondences and the camera pose in untextured point clouds of structured environments. Specifically, we first use geometric constraints between vanishing points and 3D parallel lines to compute all feasible camera rotations. Then, we utilize a hypothesis testing strategy to estimate the 2D-3D line correspondences and the translation vector. By checking the consistency with computed correspondences, the best rotation matrix can be found. Finally, the camera pose is further refined using non-linear optimization with all the 2D-3D line correspondences. The experimental results demonstrate the effectiveness of the proposed method on the synthetic and real dataset (outdoors and indoors) with repeated structures and rapid depth changes.</pre></div>
</li>
<li><div class="text-justify">
    <span id="yu2020monocular">[24]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences</b>.</div><div class="csl-block csl-author">By Yu, H., Zhen, W., Yang, W., Zhang, J. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="toggleyu2020monocular()">bibtex</button>

<script>
    function toggleyu2020monocular() {
        var x= document.getElementById('ayu2020monocular');
        // console.log("haha %o",typeof yu2020monocular);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2yu2020monocular()">abstract</button>


<script>
    function toggle2yu2020monocular() {
        var x= document.getElementById('byu2020monocular');
        // console.log("haha %o",typeof yu2020monocular);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/2004.00740"><input type="button" class="button4" value="link" /></a>


<a href="https://youtu.be/H80Bnxm8IPE"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="ayu2020monocular" style="display:none"><pre>@inproceedings{yu2020monocular,
  author = {Yu, Huai and Zhen, Weikun and Yang, Wen and Zhang, Ji and Scherer, Sebastian},
  title = {Monocular Camera Localization in Prior {LiDAR} Maps with {2D-3D} Line Correspondences},
  year = {2020},
  month = oct,
  archiveprefix = {arXiv},
  arxivid = {2004.00740},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  eprint = {2004.00740},
  url = {http://arxiv.org/abs/2004.00740},
  video = {https://youtu.be/H80Bnxm8IPE}
}
</pre></div>
<div id="byu2020monocular" style="display:none"><pre>Light-weight camera localization in existing maps is essential for vision-based navigation. Currently, visual and visual-inertial odometry (VO\backslash&amp;VIO) techniques are well-developed for state estimation but with inevitable accumulated drifts and pose jumps upon loop closure. To overcome these problems, we propose an efficient monocular camera localization method in prior LiDAR maps using directly estimated 2D-3D line correspondences. To handle the appearance differences and modality gaps between untextured point clouds and images, geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines are extracted online from video sequences. With the pose prediction from VIO, we can efficiently obtain coarse 2D-3D line correspondences. After that, the camera poses and 2D-3D correspondences are iteratively optimized by minimizing the projection error of correspondences and rejecting outliers. The experiment results on the EurocMav dataset and our collected dataset demonstrate that the proposed method can efficiently estimate camera poses without accumulated drifts or pose jumps in urban environments. The code and our collected data are available at https://github.com/levenberg/2D-3D-pose-tracking.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Pluckter-2018-110919">[25]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Precision UAV Landing in Unstructured Environments</b>.</div><div class="csl-block csl-author">By Pluckter, K. and Scherer, S.</div><div class="csl-block csl-event">In <i>International Symposium on Experimental Robotics</i>, pp. 177–187, 2020.</div></div></span>
    <br />
<button class="button0" onclick="togglePluckter2018110919()">bibtex</button>

<script>
    function togglePluckter2018110919() {
        var x= document.getElementById('aPluckter-2018-110919');
        // console.log("haha %o",typeof Pluckter-2018-110919);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Pluckter2018110919()">abstract</button>


<script>
    function toggle2Pluckter2018110919() {
        var x= document.getElementById('bPluckter-2018-110919');
        // console.log("haha %o",typeof Pluckter-2018-110919);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-030-33950-0_16"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aPluckter-2018-110919" style="display:none"><pre>@inproceedings{Pluckter-2018-110919,
  author = {Pluckter, Kevin and Scherer, Sebastian},
  booktitle = {International Symposium on Experimental Robotics},
  doi = {10.1007/978-3-030-33950-0_16},
  month = nov,
  pages = {177--187},
  title = {Precision {UAV} Landing in Unstructured Environments},
  year = {2020}
}
</pre></div>
<div id="bPluckter-2018-110919" style="display:none"><pre>Autonomous landing of a drone is a necessary part of autonomous flight. One way to have high certainty of safety in landing is to return to the same location the drone took-off from. Implementations of return-to-home functionality fall short when relying solely on GPS or odometry as inaccuracies in the measurements and drift in the state estimate guides the drone to a position with a large offset from the initial position. This can be particularly dangerous if the drone took-off next to something like a body of water. Current work on precision landing relies on localizing to a known landing pattern, which requires the pilot to carry a landing pattern with them. We propose a method using a downward facing fisheye lens camera to accurately land a UAV from where it took off on an unstructured surface, without a landing pattern. Specifically, this approach uses a position estimate relative to the take-off path of the drone to guide the drone back. With the large Field-of-View provided by the fisheye lens, our algorithm can provide visual feedback starting with a large position error at the beginning of the landing, until 25cm above the ground at the end of the landing. This algorithm empirically shows it can correct the drift error in the state estimation and land with an accuracy of 40cm.</pre></div>
</li>
<li><div class="text-justify">
    <span id="wang2020tartanair">[26]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>TartanAir: A Dataset to Push the Limits of Visual SLAM</b>.</div><div class="csl-block csl-author">By Wang, W., Zhu, D., Wang, X., Hu, Y., Qiu, Y., Wang, C., Hu, Y., Kapoor, A. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>2020.</div></div></span>
    <br />
<button class="button0" onclick="togglewang2020tartanair()">bibtex</button>

<script>
    function togglewang2020tartanair() {
        var x= document.getElementById('awang2020tartanair');
        // console.log("haha %o",typeof wang2020tartanair);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2wang2020tartanair()">abstract</button>


<script>
    function toggle2wang2020tartanair() {
        var x= document.getElementById('bwang2020tartanair');
        // console.log("haha %o",typeof wang2020tartanair);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/2003.14338"><input type="button" class="button4" value="link" /></a>


<a href="https://youtu.be/qDwfHvTbJx4"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="awang2020tartanair" style="display:none"><pre>@inproceedings{wang2020tartanair,
  author = {Wang, Wenshan and Zhu, Delong and Wang, Xiangwei and Hu, Yaoyu and Qiu, Yuheng and Wang, Chen and Hu, Yafei and Kapoor, Ashish and Scherer, Sebastian},
  title = {{TartanAir:} A Dataset to Push the Limits of Visual {SLAM}},
  year = {2020},
  month = oct,
  archiveprefix = {arXiv},
  arxivid = {2003.14338},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  eprint = {2003.14338},
  url = {http://arxiv.org/abs/2003.14338},
  video = {https://youtu.be/qDwfHvTbJx4}
}
</pre></div>
<div id="bwang2020tartanair" style="display:none"><pre>We present a challenging dataset, the TartanAir, for robot navigation task and more. The data is collected in photo-realistic simulation environments in the presence of various light conditions, weather and moving objects. By collecting data in simulation, we are able to obtain multi-modal sensor data and precise ground truth labels, including the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud. We set up a large number of environments with various styles and scenes, covering challenging viewpoints and diverse motion patterns, which are difficult to achieve by using physical data collection platforms.</pre></div>
</li></ul.no-bullet>

<h1 id="2019">2019</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Zhen:2019fv">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3-D Reconstructions</b>. </div><div class="csl-block csl-author">By Zhen, W., Hu, Y., Liu, J. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Robotics and Automation Letters</i>, vol. 4, no. 4, pp. 3585–3592, Oct. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2019fv()">bibtex</button>

<script>
    function toggleZhen2019fv() {
        var x= document.getElementById('aZhen:2019fv');
        // console.log("haha %o",typeof Zhen:2019fv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhen2019fv()">abstract</button>


<script>
    function toggle2Zhen2019fv() {
        var x= document.getElementById('bZhen:2019fv');
        // console.log("haha %o",typeof Zhen:2019fv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/LRA.2019.2928261"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aZhen:2019fv" style="display:none"><pre>@article{Zhen:2019fv,
  archiveprefix = {arXiv},
  arxivid = {1907.00930},
  author = {Zhen, Weikun and Hu, Yaoyu and Liu, Jingfeng and Scherer, Sebastian},
  doi = {10.1109/LRA.2019.2928261},
  eprint = {1907.00930},
  issn = {23773766},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {Calibration and Identification,Mapping,Sensor Fusion},
  month = oct,
  number = {4},
  pages = {3585--3592},
  title = {A Joint Optimization Approach of {LiDAR-Camera} Fusion for Accurate Dense {3-D} Reconstructions},
  volume = {4},
  year = {2019}
}
</pre></div>
<div id="bZhen:2019fv" style="display:none"><pre>Fusing data from LiDAR and camera is conceptually attractive because of their complementary properties. For instance, camera images are of higher resolution and have colors, while LiDAR data provide more accurate range measurements and have a wider field of view. However, the sensor fusion problem remains challenging since it is difficult to find reliable correlations between data of very different characteristics (geometry versus texture, sparse versus dense). This letter proposes an offline LiDAR-camera fusion method to build dense, accurate 3-D models. Specifically, our method jointly solves a bundle adjustment problem and a cloud registration problem to compute camera poses and the sensor extrinsic calibration. In experiments, we show that our method can achieve an average accuracy of 2.7 mm and resolution of 70 points/cm2 by comparing to the ground truth data from a survey scanner. Furthermore, the extrinsic calibration result is discussed and shown to outperform the state-of-the-art method.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Keller:2019uu">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A Stereo Algorithm for Thin Obstacles and Reflective Objects</b>. </div><div class="csl-block csl-author">By Keller, J. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Oct. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleKeller2019uu()">bibtex</button>

<script>
    function toggleKeller2019uu() {
        var x= document.getElementById('aKeller:2019uu');
        // console.log("haha %o",typeof Keller:2019uu);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Keller2019uu()">abstract</button>


<script>
    function toggle2Keller2019uu() {
        var x= document.getElementById('bKeller:2019uu');
        // console.log("haha %o",typeof Keller:2019uu);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1910.04874"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aKeller:2019uu" style="display:none"><pre>@article{Keller:2019uu,
  annote = {6 pages, 5 figures},
  archiveprefix = {arXiv},
  arxivid = {1910.04874},
  author = {Keller, John and Scherer, Sebastian},
  eprint = {1910.04874},
  journal = {arXiv.org},
  month = oct,
  title = {A Stereo Algorithm for Thin Obstacles and Reflective Objects},
  url = {http://arxiv.org/abs/1910.04874},
  year = {2019}
}
</pre></div>
<div id="bKeller:2019uu" style="display:none"><pre>Stereo cameras are a popular choice for obstacle avoidance for outdoor lighweight, low-cost robotics applications. However, they are unable to sense thin and reflective objects well. Currently, many algorithms are tuned to perform well on indoor scenes like the Middlebury dataset. When navigating outdoors, reflective objects, like windows and glass, and thin obstacles, like wires, are not well handled by most stereo disparity algorithms. Reflections, repeating patterns and objects parallel to the cameras’ baseline causes mismatches between image pairs which leads to bad disparity estimates. Thin obstacles are difficult for many sliding window based disparity methods to detect because they do not take up large portions of the pixels in the sliding window. We use a trinocular camera setup and micropolarizer camera capable of detecting reflective objects to overcome these issues. We present a hierarchical disparity algorithm that reduces noise, separately identify wires using semantic object triangulation in three images, and use information about the polarization of light to estimate the disparity of reflective objects. We evaluate our approach on outdoor data that we collected. Our method contained an average of 9.27% of bad pixels compared to a typical stereo algorithm’s 18.4% of bad pixels in scenes containing reflective objects. Our trinocular and semantic wire disparity methods detected 53% of wire pixels, whereas a typical two camera stereo algorithm detected 5%.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Keipour2019">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Automatic Real-time Anomaly Detection for Autonomous Aerial Vehicles</b>.</div><div class="csl-block csl-author">By Keipour, A., Mousaei, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>2019 International Conference on Robotics and Automation (ICRA)</i>, Montreal, QC, Canada, Canada, pp. 5679–5685, 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleKeipour2019()">bibtex</button>

<script>
    function toggleKeipour2019() {
        var x= document.getElementById('aKeipour2019');
        // console.log("haha %o",typeof Keipour2019);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Keipour2019()">abstract</button>


<script>
    function toggle2Keipour2019() {
        var x= document.getElementById('bKeipour2019');
        // console.log("haha %o",typeof Keipour2019);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2019.8794286"><input type="button" class="button1" value="doi" /></a>



<a href="https://arxiv.org/abs/1907.00511"><input type="button" class="button4" value="link" /></a>


<a href="https://www.youtube.com/watch?v=HCtGbnqjKj8"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="aKeipour2019" style="display:none"><pre>@inproceedings{Keipour2019,
  address = {Montreal, QC, Canada, Canada},
  author = {Keipour, Azarakhsh and Mousaei, Mohammadreza and Scherer, Sebastian},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2019.8794286},
  isbn = {978-1-5386-6027-0},
  keywords = {Actuators,Aircraft,Atmospheric modeling,Computational modeling,Fault detection,Reliability,Safety,actuators,aerospace components,aerospace simulation,aircraft model,aircraft testing,anomaly detection method,autonomous aerial vehicles,autonomous aircraft,correlated input-output pairs,fault detection open dataset,fault detection research,fault diagnosis,fault tolerant control,fixed-wing flights,ground truth,least squares approximations,mid-flight actuator failures,mobile robots,recursive least squares method},
  month = may,
  pages = {5679--5685},
  title = {Automatic Real-time Anomaly Detection for Autonomous Aerial Vehicles},
  url = {https://arxiv.org/abs/1907.00511},
  video = {https://www.youtube.com/watch?v=HCtGbnqjKj8},
  year = {2019}
}
</pre></div>
<div id="bKeipour2019" style="display:none"><pre>The recent increase in the use of aerial vehicles raises concerns about the safety and reliability of autonomous operations. There is a growing need for methods to monitor the status of these aircraft and report any faults and anomalies to the safety pilot or to the autopilot to deal with the emergency situation. In this paper, we present a real-time approach using the Recursive Least Squares method to detect anomalies in the behavior of an aircraft. The method models the relationship between correlated input-output pairs online and uses the model to detect the anomalies. The result is an easy-to-deploy anomaly detection method that does not assume a specific aircraft model and can detect many types of faults and anomalies in a wide range of autonomous aircraft. The experiments on this method show a precision of 88.23%, recall of 88.23% and 86.36% accuracy for over 22 flight tests. The other contribution is providing a new fault detection open dataset for autonomous aircraft, which contains complete data and the ground truth for 22 fixed-wing flights with eight different types of mid-flight actuator failures to help future fault detection research for aircraft.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang:2018eo">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>CubeSLAM: Monocular 3-D Object SLAM</b>. </div><div class="csl-block csl-author">By Yang, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Transactions on Robotics</i>, vol. 35, no. 4, pp. 925–938, Jun. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleYang2018eo()">bibtex</button>

<script>
    function toggleYang2018eo() {
        var x= document.getElementById('aYang:2018eo');
        // console.log("haha %o",typeof Yang:2018eo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yang2018eo()">abstract</button>


<script>
    function toggle2Yang2018eo() {
        var x= document.getElementById('bYang:2018eo');
        // console.log("haha %o",typeof Yang:2018eo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/TRO.2019.2909168"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aYang:2018eo" style="display:none"><pre>@article{Yang:2018eo,
  annote = {IEEE Transactions on Robotics},
  archiveprefix = {arXiv},
  arxivid = {1806.00557},
  author = {Yang, Shichao and Scherer, Sebastian},
  doi = {10.1109/TRO.2019.2909168},
  eprint = {1806.00557},
  issn = {19410468},
  journal = {IEEE Transactions on Robotics},
  keywords = {Dynamic SLAM,SLAM,object detection,object simultaneous localization and mapping (SLAM)},
  month = jun,
  number = {4},
  pages = {925--938},
  title = {{CubeSLAM: Monocular 3-D Object SLAM}},
  volume = {35},
  year = {2019}
}
</pre></div>
<div id="bYang:2018eo" style="display:none"><pre>In this paper, we present a method for single image three-dimensional (3-D) cuboid object detection and multiview object simultaneous localization and mapping in both static and dynamic environments, and demonstrate that the two parts can improve each other. First, for single image object detection, we generate high-quality cuboid proposals from two-dimensional (2-D) bounding boxes and vanishing points sampling. The proposals are further scored and selected based on the alignment with image edges. Second, multiview bundle adjustment with new object measurements is proposed to jointly optimize poses of cameras, objects, and points. Objects can provide long-range geometric and scale constraints to improve camera pose estimation and reduce monocular drift. Instead of treating dynamic regions as outliers, we utilize object representation and motion model constraints to improve the camera pose estimation. The 3-D detection experiments on SUN RGBD and KITTI show better accuracy and robustness over existing approaches. On the public TUM, KITTI odometry and our own collected datasets, our SLAM method achieves the state-of-the-art monocular camera pose estimation and at the same time, improves the 3-D object detection accuracy.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chandarana-2018-107474">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Decentralized Method for Sub-Swarm Deployment and Rejoining</b>.</div><div class="csl-block csl-author">By Chandarana, M., Luo, W., Lewis, M., Sycara, K. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018</i>, pp. 1209–1214, 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleChandarana2018107474()">bibtex</button>

<script>
    function toggleChandarana2018107474() {
        var x= document.getElementById('aChandarana-2018-107474');
        // console.log("haha %o",typeof Chandarana-2018-107474);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chandarana2018107474()">abstract</button>


<script>
    function toggle2Chandarana2018107474() {
        var x= document.getElementById('bChandarana-2018-107474');
        // console.log("haha %o",typeof Chandarana-2018-107474);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/SMC.2018.00212"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChandarana-2018-107474" style="display:none"><pre>@inproceedings{Chandarana-2018-107474,
  author = {Chandarana, Meghan and Luo, Wenhao and Lewis, Michael and Sycara, Katia and Scherer, Sebastian},
  booktitle = {Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018},
  doi = {10.1109/SMC.2018.00212},
  isbn = {9781538666500},
  month = oct,
  pages = {1209--1214},
  title = {Decentralized Method for Sub-Swarm Deployment and Rejoining},
  year = {2019}
}
</pre></div>
<div id="bChandarana-2018-107474" style="display:none"><pre>As part of swarm search and service (SSS) missions, robots are tasked with servicing jobs as they are sensed. This requires small sub-swarm teams to leave the swarm for a specified amount of time to service the jobs. In doing so, fewer robots are required to change motion than if the whole swarm were diverted, thereby minimizing the job’s overall effect on the swarm’s main goal. We explore the problem of removing the required number of robots from the swarm, while maintaining overall swarm connectivity. By preserving connectivity, robots are able to successfully rejoin the swarm upon completion of their assigned job. These robots are then made available for reallocation. We propose a decentralized and asynchronous method for breaking off sub-swarm groups and rejoining them with the main swarm using the swarm’s communication graph topology. Both single and multiple job site cases are explored. The results are compared against a full swarm movement method. Simulation results show that the proposed method outperforms a full swarm method in the average number of messages sent per robot in each step, as well as, the distance traveled by the swarm.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Zhen:2019jn">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Estimating the localizability in tunnel-like environments using LiDAR and UWB</b>.</div><div class="csl-block csl-author">By Zhen, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, pp. 4903–4908, 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2019jn()">bibtex</button>

<script>
    function toggleZhen2019jn() {
        var x= document.getElementById('aZhen:2019jn');
        // console.log("haha %o",typeof Zhen:2019jn);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhen2019jn()">abstract</button>


<script>
    function toggle2Zhen2019jn() {
        var x= document.getElementById('bZhen:2019jn');
        // console.log("haha %o",typeof Zhen:2019jn);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2019.8794167"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aZhen:2019jn" style="display:none"><pre>@inproceedings{Zhen:2019jn,
  author = {Zhen, Weikun and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2019.8794167},
  isbn = {9781538660263},
  issn = {10504729},
  month = may,
  pages = {4903--4908},
  title = {Estimating the localizability in tunnel-like environments using {LiDAR} and {UWB}},
  year = {2019}
}
</pre></div>
<div id="bZhen:2019jn" style="display:none"><pre>The application of robots in inspection tasks has been growing quickly thanks to the advancements in autonomous navigation technology, especially the robot localization techniques in GPS-denied environments. Although many methods have been proposed to localize a robot using onboard sensors such as cameras and LiDARs, achieving robust localization in geometrically degenerated environments, e.g. tunnels, remains a challenging problem. In this work, we focus on the robust localization problem in such situations. A novel degeneration characterization model is presented to estimate the localizability at a given location in the prior map. And the localizability of a LiDAR and an Ultra-Wideband (UWB) ranging radio is analyzed. Additionally, a probabilistic sensor fusion method is developed to combine IMU, LiDAR and the UWB. Experiment results show that this method allows for robust localization inside a long straight tunnel.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2019hz">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>High performance and safe flight of full-scale helicopters from takeoff to landing with an ensemble of planners</b>. </div><div class="csl-block csl-author">By Choudhury, S., Dugar, V., Maeta, S., MacAllister, B., Arora, S., Althoff, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Journal of Field Robotics</i>, vol. 36, no. 8, pp. 1275–1332, Dec. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2019hz()">bibtex</button>

<script>
    function toggleChoudhury2019hz() {
        var x= document.getElementById('aChoudhury:2019hz');
        // console.log("haha %o",typeof Choudhury:2019hz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2019hz()">abstract</button>


<script>
    function toggle2Choudhury2019hz() {
        var x= document.getElementById('bChoudhury:2019hz');
        // console.log("haha %o",typeof Choudhury:2019hz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1002/rob.21906"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2019hz" style="display:none"><pre>@article{Choudhury:2019hz,
  author = {Choudhury, Sanjiban and Dugar, Vishal and Maeta, Silvio and MacAllister, Brian and Arora, Sankalp and Althoff, Daniel and Scherer, Sebastian},
  doi = {10.1002/rob.21906},
  issn = {15564967},
  journal = {Journal of Field Robotics},
  keywords = {aerial robotics,learning,planning},
  month = dec,
  number = {8},
  pages = {1275--1332},
  title = {High performance and safe flight of full-scale helicopters from takeoff to landing with an ensemble of planners},
  volume = {36},
  year = {2019}
}
</pre></div>
<div id="bChoudhury:2019hz" style="display:none"><pre>Autonomous flight of unmanned full-size rotor-craft has the potential to enable many new applications. However, the dynamics of these aircraft, prevailing wind conditions, the need to operate over a variety of speeds and stringent safety requirements make it difficult to generate safe plans for these systems. Prior work has shown results for only parts of the problem. Here we present the first comprehensive approach to planning safe trajectories for autonomous helicopters from takeoff to landing. Our approach is based on two key insights. First, we compose an approximate solution by cascading various modules that can efficiently solve different relaxations of the planning problem. Our framework invokes a long-term route optimizer, which feeds a receding-horizon planner which in turn feeds a high-fidelity safety executive. Secondly, to deal with the diverse planning scenarios that may arise, we hedge our bets with an ensemble of planners. We use a data-driven approach that maps a planning context to a diverse list of planning algorithms that maximize the likelihood of success. Our approach was extensively evaluated in simulation and in real-world flight tests on three different helicopter systems for duration of more than 109 autonomous hours and 590 pilot-in-the-loop hours. We provide an in-depth analysis and discuss the various tradeoffs of decoupling the problem, using approximations and leveraging statistical techniques. We summarize the insights with the hope that it generalizes to other platforms and applications.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Wang:2019kd">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Improved generalization of heading direction estimation for aerial filming using semi-supervised regression</b>.</div><div class="csl-block csl-author">By Wang, W., Ahuja, A., Zhang, Y., Bonatti, R. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, pp. 5901–5907, 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleWang2019kd()">bibtex</button>

<script>
    function toggleWang2019kd() {
        var x= document.getElementById('aWang:2019kd');
        // console.log("haha %o",typeof Wang:2019kd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Wang2019kd()">abstract</button>


<script>
    function toggle2Wang2019kd() {
        var x= document.getElementById('bWang:2019kd');
        // console.log("haha %o",typeof Wang:2019kd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2019.8793994"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aWang:2019kd" style="display:none"><pre>@inproceedings{Wang:2019kd,
  archiveprefix = {arXiv},
  arxivid = {1903.11174},
  author = {Wang, Wenshan and Ahuja, Aayush and Zhang, Yanfu and Bonatti, Rogerio and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2019.8793994},
  eprint = {1903.11174},
  isbn = {9781538660263},
  issn = {10504729},
  month = may,
  pages = {5901--5907},
  title = {Improved generalization of heading direction estimation for aerial filming using semi-supervised regression},
  year = {2019}
}
</pre></div>
<div id="bWang:2019kd" style="display:none"><pre>In the task of Autonomous aerial filming of a moving actor (e.g. a person or a vehicle), it is crucial to have a good heading direction estimation for the actor from the visual input. However, the models obtained in other similar tasks, such as pedestrian collision risk analysis and human-robot interaction, are very difficult to generalize to the aerial filming task, because of the difference in data distributions. Towards improving generalization with less amount of labeled data, this paper presents a semi-supervised algorithm for heading direction estimation problem. We utilize temporal continuity as the unsupervised signal to regularize the model and achieve better generalization ability. This semi-supervised algorithm is applied to both training and testing phases, which increases the testing performance by a large margin. We show that by leveraging unlabeled sequences, the amount of labeled data required can be significantly reduced. We also discuss several important details on improving the performance by balancing labeled and unlabeled loss, and making good combinations. Experimental results show that our approach robustly outputs the heading direction for different types of actor. The aesthetic value of the video is also improved in the aerial filming task.</pre></div>
</li>
<li><div class="text-justify">
    <span id="chandarana2019hybrid">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Hybrid Model for A Priori Performance Prediction of Multi-Job Type Swarm Search and Service Missions</b>.</div><div class="csl-block csl-author">By Chandarana, M., Hughes, D., Lewis, M., Sycara, K. and Scherer, S.</div><div class="csl-block csl-event">In <i>2019 19th International Conference on Advanced Robotics (ICAR)</i>, pp. 714–719, 2019.</div></div></span>
    <br />
<button class="button0" onclick="togglechandarana2019hybrid()">bibtex</button>

<script>
    function togglechandarana2019hybrid() {
        var x= document.getElementById('achandarana2019hybrid');
        // console.log("haha %o",typeof chandarana2019hybrid);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2chandarana2019hybrid() {
        var x= document.getElementById('bchandarana2019hybrid');
        // console.log("haha %o",typeof chandarana2019hybrid);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="achandarana2019hybrid" style="display:none"><pre>@inproceedings{chandarana2019hybrid,
  title = {Hybrid Model for A Priori Performance Prediction of Multi-Job Type Swarm Search and Service Missions},
  author = {Chandarana, Meghan and Hughes, Dana and Lewis, Michael and Sycara, Katia and Scherer, Sebastian},
  booktitle = {2019 19th International Conference on Advanced Robotics (ICAR)},
  pages = {714--719},
  year = {2019}
}
</pre></div>
<div id="bchandarana2019hybrid" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Bonatti:2019vp">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations</b>. </div><div class="csl-block csl-author">By Bonatti, R., Madaan, R., Vineet, V., Scherer, S. and Kapoor, A.</div><div class="csl-block csl-event">In <i>arXiv:1909.06993v1 [cs.CV]</i>, Sep. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleBonatti2019vp()">bibtex</button>

<script>
    function toggleBonatti2019vp() {
        var x= document.getElementById('aBonatti:2019vp');
        // console.log("haha %o",typeof Bonatti:2019vp);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Bonatti2019vp()">abstract</button>


<script>
    function toggle2Bonatti2019vp() {
        var x= document.getElementById('bBonatti:2019vp');
        // console.log("haha %o",typeof Bonatti:2019vp);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1909.06993"><input type="button" class="button4" value="link" /></a>


<a href="https://www.youtube.com/watch?v=AxE7qGKJWaw"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="aBonatti:2019vp" style="display:none"><pre>@article{Bonatti:2019vp,
  archiveprefix = {arXiv},
  arxivid = {1909.06993},
  author = {Bonatti, Rogerio and Madaan, Ratnesh and Vineet, Vibhav and Scherer, Sebastian and Kapoor, Ashish},
  eprint = {1909.06993},
  journal = {arXiv:1909.06993v1 [cs.CV]},
  month = sep,
  title = {Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations},
  url = {http://arxiv.org/abs/1909.06993},
  video = {https://www.youtube.com/watch?v=AxE7qGKJWaw},
  year = {2019}
}
</pre></div>
<div id="bBonatti:2019vp" style="display:none"><pre>Machines are a long way from robustly solving open-world perception-control tasks, such as first-person view (FPV) aerial navigation. While recent advances in end-to-end Machine Learning, especially Imitation and Reinforcement Learning appear promising, they are constrained by the need of large amounts of difficult-to-collect labeled real-world data. Simulated data, on the other hand, is easy to generate, but generally does not render safe behaviors in diverse real-life scenarios. In this work we propose a novel method for learning robust visuomotor policies for real-world deployment which can be trained purely with simulated data. We develop rich state representations that combine supervised and unsupervised environment data. Our approach takes a cross-modal perspective, where separate modalities correspond to the raw camera data and the system states relevant to the task, such as the relative pose of gates to the drone in the case of drone racing. We feed both data modalities into a novel factored architecture, which learns a joint low-dimensional embedding via Variational Auto Encoders. This compact representation is then fed into a control policy, which we trained using imitation learning with expert trajectories in a simulator. We analyze the rich latent spaces learned with our proposed representations, and show that the use of our cross-modal architecture significantly improves control policy performance as compared to end-to-end learning or purely unsupervised feature extractors. We also present real-world results for drone navigation through gates in different track configurations and environmental conditions. Our proposed method, which runs fully onboard, can successfully generalize the learned representations and policies across simulation and reality, significantly outperforming baseline approaches. Supplementary video: https://youtu.be/VKc3A5HlUU8</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang:2019el">[11]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Monocular object and plane slam in structured environments</b>. </div><div class="csl-block csl-author">By Yang, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Robotics and Automation Letters</i>, vol. 4, no. 4, pp. 3145–3152, 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleYang2019el()">bibtex</button>

<script>
    function toggleYang2019el() {
        var x= document.getElementById('aYang:2019el');
        // console.log("haha %o",typeof Yang:2019el);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yang2019el()">abstract</button>


<script>
    function toggle2Yang2019el() {
        var x= document.getElementById('bYang:2019el');
        // console.log("haha %o",typeof Yang:2019el);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/LRA.2019.2924848"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aYang:2019el" style="display:none"><pre>@article{Yang:2019el,
  author = {Yang, Shichao and Scherer, Sebastian},
  doi = {10.1109/LRA.2019.2924848},
  issn = {23773766},
  journal = {IEEE Robotics and Automation Letters},
  keywords = {Object and plane slam,Semantic scene understanding,Slam},
  number = {4},
  pages = {3145--3152},
  title = {Monocular object and plane slam in structured environments},
  volume = {4},
  year = {2019}
}
</pre></div>
<div id="bYang:2019el" style="display:none"><pre>In this letter, we present a monocular simultaneous localization and mapping (SLAM) algorithm using high-level object and plane landmarks. The built map is denser, more compact and semantic meaningful compared to feature point based SLAM. We first propose a high-order graphical model to jointly infer the three-dimensional object and layout planes from single images considering occlusions and semantic constraints. The extracted objects and planes are further optimized with camera poses in a unified SLAM framework. Objects and planes can provide more semantic constraints such as Manhattan plane and object supporting relationships compared to points. Experiments on various public and collected datasets, including ICL NUIM and TUM Mono show that our algorithm can improve camera localization accuracy compared to state-of-the-art SLAM, especially when there is no loop closure, and also generate dense maps robustly in many structured environments.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Madaan:2019kb">[12]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Multi-view reconstruction of wires using a catenary model</b>.</div><div class="csl-block csl-author">By Madaan, R., Kaess, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, pp. 5657–5664, 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleMadaan2019kb()">bibtex</button>

<script>
    function toggleMadaan2019kb() {
        var x= document.getElementById('aMadaan:2019kb');
        // console.log("haha %o",typeof Madaan:2019kb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Madaan2019kb()">abstract</button>


<script>
    function toggle2Madaan2019kb() {
        var x= document.getElementById('bMadaan:2019kb');
        // console.log("haha %o",typeof Madaan:2019kb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2019.8793852"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMadaan:2019kb" style="display:none"><pre>@inproceedings{Madaan:2019kb,
  author = {Madaan, Ratnesh and Kaess, Michael and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2019.8793852},
  isbn = {9781538660263},
  issn = {10504729},
  month = may,
  pages = {5657--5664},
  title = {Multi-view reconstruction of wires using a catenary model},
  year = {2019}
}
</pre></div>
<div id="bMadaan:2019kb" style="display:none"><pre>Reliable detection and reconstruction of wires is one of the hardest problems in the UAV community, with a wide ranging impact in the industry in terms of wire avoidance capabilities and powerline corridor inspection. In this work, we introduce a real-time, model-based, multi-view algorithm to reconstruct wires from a set of images with known camera poses, while exploiting their natural shape-the catenary curve. Using a model-based approach helps us deal with partial wire detections in images, which may occur due to natural occlusion and false negatives. In addition, using a parsimonious model makes our algorithm efficient as we only need to optimize for 5 model parameters, as opposed to hundreds of 3D points in bundle-adjustment approaches. Our algorithm obviates the need for pixel correspondences by computing the reprojection error via the distance transform of binarized wire segmentation images. Further, we make our algorithm robust to arbitrary initializations by introducing an on-demand, approximate extrapolation of the distance transform based objective. We demonstrate the effectiveness of our algorithm against false negatives and random initializations in simulation, and show qualitative results with real data collected from a small UAV.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Zhen2019">[13]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>LiDAR Enhanced Structure-from-Motion</b>. </div><div class="csl-block csl-author">By Zhen, W., Hu, Y., Yu, H. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Nov. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2019()">bibtex</button>

<script>
    function toggleZhen2019() {
        var x= document.getElementById('aZhen2019');
        // console.log("haha %o",typeof Zhen2019);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Zhen2019() {
        var x= document.getElementById('bZhen2019');
        // console.log("haha %o",typeof Zhen2019);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aZhen2019" style="display:none"><pre>@article{Zhen2019,
  author = {Zhen, Weikun and Hu, Yaoyu and Yu, Huai and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {{LiDAR} Enhanced Structure-from-Motion},
  year = {2019},
  month = nov,
  annote = {6 pages plus reference. Work has been submitted to ICRA 2020},
  archiveprefix = {arxiv},
  eprint = {1911.03369v1},
  primaryclass = {cs.RO}
}
</pre></div>
<div id="bZhen2019" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Bonatti2019">[14]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning Controls Using Cross-Modal Representations: Bridging Simulation and Reality for Drone Racing</b>. </div><div class="csl-block csl-author">By Bonatti, R., Madaan, R., Vineet, V., Scherer, S. and Kapoor, A.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Sep. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleBonatti2019()">bibtex</button>

<script>
    function toggleBonatti2019() {
        var x= document.getElementById('aBonatti2019');
        // console.log("haha %o",typeof Bonatti2019);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Bonatti2019() {
        var x= document.getElementById('bBonatti2019');
        // console.log("haha %o",typeof Bonatti2019);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aBonatti2019" style="display:none"><pre>@article{Bonatti2019,
  author = {Bonatti, Rogerio and Madaan, Ratnesh and Vineet, Vibhav and Scherer, Sebastian and Kapoor, Ashish},
  journal = {arXiv.org},
  title = {Learning Controls Using Cross-Modal Representations: Bridging Simulation and Reality for Drone Racing},
  year = {2019},
  month = sep,
  archiveprefix = {arxiv},
  eprint = {1909.06993v1},
  primaryclass = {cs.CV}
}
</pre></div>
<div id="bBonatti2019" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Bonatti2019a">[15]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous Aerial Cinematography In Unstructured Environments With Learned Artistic Decision-Making</b>. </div><div class="csl-block csl-author">By Bonatti, R., Wang, W., Ho, C., Ahuja, A., Gschwindt, M., Camci, E., Kayacan, E., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Oct. 2019.</div></div></span>
    <br />
<button class="button0" onclick="toggleBonatti2019a()">bibtex</button>

<script>
    function toggleBonatti2019a() {
        var x= document.getElementById('aBonatti2019a');
        // console.log("haha %o",typeof Bonatti2019a);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Bonatti2019a() {
        var x= document.getElementById('bBonatti2019a');
        // console.log("haha %o",typeof Bonatti2019a);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aBonatti2019a" style="display:none"><pre>@article{Bonatti2019a,
  author = {Bonatti, Rogerio and Wang, Wenshan and Ho, Cherie and Ahuja, Aayush and Gschwindt, Mirko and Camci, Efe and Kayacan, Erdal and Choudhury, Sanjiban and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {Autonomous Aerial Cinematography In Unstructured Environments With Learned Artistic Decision-Making},
  year = {2019},
  month = oct,
  archiveprefix = {arxiv},
  eprint = {1910.06988v1},
  primaryclass = {cs.RO}
}
</pre></div>
<div id="bBonatti2019a" style="display:none"><pre></pre></div>
</li></ul.no-bullet>

<h1 id="2018">2018</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Zhen-2018-109998">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A Unified 3D Mapping Framework Using a 3D or 2D LiDAR</b>.</div><div class="csl-block csl-author">By Zhen, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>International Symposium on Experimental Robotics</i>, pp. 702–711, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2018109998()">bibtex</button>

<script>
    function toggleZhen2018109998() {
        var x= document.getElementById('aZhen-2018-109998');
        // console.log("haha %o",typeof Zhen-2018-109998);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhen2018109998()">abstract</button>


<script>
    function toggle2Zhen2018109998() {
        var x= document.getElementById('bZhen-2018-109998');
        // console.log("haha %o",typeof Zhen-2018-109998);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-030-33950-0_60"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aZhen-2018-109998" style="display:none"><pre>@inproceedings{Zhen-2018-109998,
  archiveprefix = {arXiv},
  arxivid = {1810.12515},
  author = {Zhen, Weikun and Scherer, Sebastian},
  booktitle = {International Symposium on Experimental Robotics},
  doi = {10.1007/978-3-030-33950-0_60},
  eprint = {1810.12515},
  month = nov,
  pages = {702--711},
  title = {A Unified {3D} Mapping Framework Using a {3D} or {2D} {LiDAR}},
  year = {2018}
}
</pre></div>
<div id="bZhen-2018-109998" style="display:none"><pre>Simultaneous Localization and Mapping (SLAM) has been considered as a solved problem thanks to the progress made in the past few years. However, the great majority of LiDAR-based SLAM algorithms are designed for a specific type of payload and therefore don’t generalize across different platforms. In practice, this drawback causes the development, deployment and maintenance of an algorithm difficult. Consequently, our work focuses on improving the compatibility across different sensing payloads. Specifically, we extend the Cartographer SLAM library to handle different types of LiDAR including fixed or rotating, 2D or 3D LiDARs. By replacing the localization module of Cartographer and maintaining the sparse pose graph (SPG), the proposed framework can create high-quality 3D maps in real-time on different sensing payloads. Additionally, it brings the benefit of simplicity with only a few parameters need to be adjusted for each sensor type.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Zhen:2018uf">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Achieving Robust Localization in Geometrically Degenerated Tunnels</b>.</div><div class="csl-block csl-author">By Zhen, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>Workshop on Challenges and Opportunities for Resilient Collective Intelligence in Subterranean Environments</i>, Pittsburgh, Pa2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2018uf()">bibtex</button>

<script>
    function toggleZhen2018uf() {
        var x= document.getElementById('aZhen:2018uf');
        // console.log("haha %o",typeof Zhen:2018uf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhen2018uf()">abstract</button>


<script>
    function toggle2Zhen2018uf() {
        var x= document.getElementById('bZhen:2018uf');
        // console.log("haha %o",typeof Zhen:2018uf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aZhen:2018uf" style="display:none"><pre>@inproceedings{Zhen:2018uf,
  address = {Pittsburgh, Pa},
  author = {Zhen, Weikun and Scherer, Sebastian},
  booktitle = {Workshop on Challenges and Opportunities for Resilient Collective Intelligence in Subterranean Environments},
  month = jun,
  title = {Achieving Robust Localization in Geometrically Degenerated Tunnels},
  year = {2018}
}
</pre></div>
<div id="bZhen:2018uf" style="display:none"><pre>Although many methods have been proposed to localize a robot using onboard sensors in GPS-denied environments , achieving robust localization in geometrically degenerated tunnels remains a challenging problem in robot-based inspection tasks. In this work, we first present a novel model to analyze the localizability of the prior map at a given location. Then we propose the utilization of a single Ultra-Wideband (UWB) ranging device to compensate for the degeneration of LiDAR based localization inside tunnels. A probabilistic sensor fusion method is developed and demonstrated to achieve real-time robust localization inside a geometrically degenerated tunnel.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dong:2018cg">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>An efficient global energy optimization approach for robust 3D plane segmentation of point clouds</b>. </div><div class="csl-block csl-author">By Dong, Z., Yang, B., Hu, P. and Scherer, S.</div><div class="csl-block csl-event">In <i>ISPRS Journal of Photogrammetry and Remote Sensing</i>, vol. 137, pp. 112–133, Mar. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleDong2018cg()">bibtex</button>

<script>
    function toggleDong2018cg() {
        var x= document.getElementById('aDong:2018cg');
        // console.log("haha %o",typeof Dong:2018cg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dong2018cg()">abstract</button>


<script>
    function toggle2Dong2018cg() {
        var x= document.getElementById('bDong:2018cg');
        // console.log("haha %o",typeof Dong:2018cg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1016/j.isprsjprs.2018.01.013"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDong:2018cg" style="display:none"><pre>@article{Dong:2018cg,
  author = {Dong, Zhen and Yang, Bisheng and Hu, Pingbo and Scherer, Sebastian},
  doi = {10.1016/j.isprsjprs.2018.01.013},
  issn = {09242716},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords = {Energy optimization,Guided sampling,Hybrid region growing,Multiscale supervoxel,Plane segmentation,Simulated annealing},
  month = mar,
  pages = {112--133},
  title = {An efficient global energy optimization approach for robust {3D} plane segmentation of point clouds},
  volume = {137},
  year = {2018}
}
</pre></div>
<div id="bDong:2018cg" style="display:none"><pre>Automatic 3D plane segmentation is necessary for many applications including point cloud registration, building information model (BIM) reconstruction, simultaneous localization and mapping (SLAM), and point cloud compression. However, most of the existing 3D plane segmentation methods still suffer from low precision and recall, and inaccurate and incomplete boundaries, especially for low-quality point clouds collected by RGB-D sensors. To overcome these challenges, this paper formulates the plane segmentation problem as a global energy optimization because it is robust to high levels of noise and clutter. First, the proposed method divides the raw point cloud into multiscale supervoxels, and considers planar supervoxels and individual points corresponding to nonplanar supervoxels as basic units. Then, an efficient hybrid region growing algorithm is utilized to generate initial plane set by incrementally merging adjacent basic units with similar features. Next, the initial plane set is further enriched and refined in a mutually reinforcing manner under the framework of global energy optimization. Finally, the performances of the proposed method are evaluated with respect to six metrics (i.e., plane precision, plane recall, under-segmentation rate, over-segmentation rate, boundary precision, and boundary recall) on two benchmark datasets. Comprehensive experiments demonstrate that the proposed method obtained good performances both in high-quality TLS point clouds (i.e., SEMANTIC3D.NET dataset) and low-quality RGB-D point clouds (i.e., S3DIS dataset) with six metrics of (94.2%, 95.1%, 2.9%, 3.8%, 93.6%, 94.1%) and (90.4%, 91.4%, 8.2%, 7.6%, 90.8%, 91.7%) respectively.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2017wf">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Bayesian active edge evaluation on expensive graphs</b>. </div><div class="csl-block csl-author">By Choudhury, S., Srinivasa, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>IJCAI International Joint Conference on Artificial Intelligence</i>, pp. 4890–4897, Nov. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2017wf()">bibtex</button>

<script>
    function toggleChoudhury2017wf() {
        var x= document.getElementById('aChoudhury:2017wf');
        // console.log("haha %o",typeof Choudhury:2017wf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2017wf()">abstract</button>


<script>
    function toggle2Choudhury2017wf() {
        var x= document.getElementById('bChoudhury:2017wf');
        // console.log("haha %o",typeof Choudhury:2017wf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.24963/ijcai.2018/679"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2017wf" style="display:none"><pre>@article{Choudhury:2017wf,
  archiveprefix = {arXiv},
  arxivid = {1711.07329},
  author = {Choudhury, Sanjiban and Srinivasa, Siddhartha and Scherer, Sebastian},
  doi = {10.24963/ijcai.2018/679},
  eprint = {1711.07329},
  isbn = {9780999241127},
  issn = {10450823},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  month = nov,
  pages = {4890--4897},
  title = {Bayesian active edge evaluation on expensive graphs},
  year = {2018}
}
</pre></div>
<div id="bChoudhury:2017wf" style="display:none"><pre>We consider the problem of real-time motion planning that requires evaluating a minimal number of edges on a graph to quickly discover collision-free paths. Evaluating edges is expensive, both for robots with complex geometries like robot arms, and for robots sensing the world online like UAVs. Until now, this challenge has been addressed via laziness i.e. deferring edge evaluation until absolutely necessary, with the hope that edges turn out to be valid. However, all edges are not alike in value - some have a lot of potentially good paths flowing through them, and some others encode the likelihood of neighbouring edges being valid. This leads to our key insight - instead of passive laziness, we can actively choose edges that reduce the uncertainty about the validity of paths. We show that this is equivalent to the Bayesian active learning paradigm of decision region determination (DRD). However, the DRD problem is not only combina-torially hard, but also requires explicit enumeration of all possible worlds. We propose a novel framework that combines two DRD algorithms, DIRECT and BISECT, to overcome both issues. We show that our approach outperforms several state-of-the-art algorithms on a spectrum of planning problems for mobile robots, manipulators and autonomous helicopters.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2017vh">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Data-driven planning via imitation learning</b>. </div><div class="csl-block csl-author">By Choudhury, S., Bhardwaj, M., Arora, S., Kapoor, A., Ranade, G., Scherer, S. and Dey, D.</div><div class="csl-block csl-event">In <i>International Journal of Robotics Research</i>, vol. 37, no. 13-14, pp. 1632–1672, Dec. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2017vh()">bibtex</button>

<script>
    function toggleChoudhury2017vh() {
        var x= document.getElementById('aChoudhury:2017vh');
        // console.log("haha %o",typeof Choudhury:2017vh);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2017vh()">abstract</button>


<script>
    function toggle2Choudhury2017vh() {
        var x= document.getElementById('bChoudhury:2017vh');
        // console.log("haha %o",typeof Choudhury:2017vh);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1177/0278364918781001"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2017vh" style="display:none"><pre>@article{Choudhury:2017vh,
  archiveprefix = {arXiv},
  arxivid = {1711.06391},
  author = {Choudhury, Sanjiban and Bhardwaj, Mohak and Arora, Sankalp and Kapoor, Ashish and Ranade, Gireeja and Scherer, Sebastian and Dey, Debadeepta},
  doi = {10.1177/0278364918781001},
  eprint = {1711.06391},
  issn = {17413176},
  journal = {International Journal of Robotics Research},
  keywords = {,Imitation learning,POMDPs,QMDPs,heuristic search,sequential decision making},
  month = dec,
  number = {13-14},
  pages = {1632--1672},
  title = {Data-driven planning via imitation learning},
  volume = {37},
  year = {2018}
}
</pre></div>
<div id="bChoudhury:2017vh" style="display:none"><pre>Robot planning is the process of selecting a sequence of actions that optimize for a task=specific objective. For instance, the objective for a navigation task would be to find collision-free paths, whereas the objective for an exploration task would be to map unknown areas. The optimal solutions to such tasks are heavily influenced by the implicit structure in the environment, i.e. the configuration of objects in the world. State-of-the-art planning approaches, however, do not exploit this structure, thereby expending valuable effort searching the action space instead of focusing on potentially good actions. In this paper, we address the problem of enabling planners to adapt their search strategies by inferring such good actions in an efficient manner using only the information uncovered by the search up until that time. We formulate this as a problem of sequential decision making under uncertainty where at a given iteration a planning policy must map the state of the search to a planning action. Unfortunately, the training process for such partial-information-based policies is slow to converge and susceptible to poor local minima. Our key insight is that if we could fully observe the underlying world map, we would easily be able to disambiguate between good and bad actions. We hence present a novel data-driven imitation learning framework to efficiently train planning policies by imitating a clairvoyant oracle: an oracle that at train time has full knowledge about the world map and can compute optimal decisions. We leverage the fact that for planning problems, such oracles can be efficiently computed and derive performance guarantees for the learnt policy. We examine two important domains that rely on partial-information-based policies: informative path planning and search-based motion planning. We validate the approach on a spectrum of environments for both problem domains, including experiments on a real UAV, and show that the learnt policy consistently outperforms state-of-the-art algorithms. Our framework is able to train policies that achieve up to 39% more reward than state-of-the art information-gathering heuristics and a 70 x speedup as compared with A* on search-based planning problems. Our approach paves the way forward for applying data-driven techniques to other such problem domains under the umbrella of robot planning.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chandarana-2018-107676">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Determining Effective Swarm Sizes for Multi-Job Type Missions</b>.</div><div class="csl-block csl-author">By Chandarana, M., Lewis, M., Sycara, K. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, pp. 4848–4853, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleChandarana2018107676()">bibtex</button>

<script>
    function toggleChandarana2018107676() {
        var x= document.getElementById('aChandarana-2018-107676');
        // console.log("haha %o",typeof Chandarana-2018-107676);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chandarana2018107676()">abstract</button>


<script>
    function toggle2Chandarana2018107676() {
        var x= document.getElementById('bChandarana-2018-107676');
        // console.log("haha %o",typeof Chandarana-2018-107676);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2018.8593919"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChandarana-2018-107676" style="display:none"><pre>@inproceedings{Chandarana-2018-107676,
  author = {Chandarana, Meghan and Lewis, Michael and Sycara, Katia and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2018.8593919},
  isbn = {9781538680940},
  issn = {21530866},
  month = sep,
  pages = {4848--4853},
  title = {Determining Effective Swarm Sizes for Multi-Job Type Missions},
  year = {2018}
}
</pre></div>
<div id="bChandarana-2018-107676" style="display:none"><pre>Swarm search and service (SSS) missions require large swarms to simultaneously search an area while servicing jobs as they are encountered. Jobs must be immediately serviced and can be one of several different job types - each requiring a different service time and number of vehicles to complete its service successfully. After jobs are serviced, vehicles are returned to the swarm and become available for reallocation. As part of SSS mission planning, human operators must determine the number of vehicles needed to achieve this balance. The complexities associated with balancing vehicle allocation to multiple as yet unknown tasks with returning vehicles makes this extremely difficult for humans. Previous work assumes that all system jobs are known ahead of time or that vehicles move independently of each other in a multi-agent framework. We present a dynamic vehicle routing (DVR) framework whose policies optimally allocate vehicles as jobs arrive. By incorporating time constraints into the DVR framework, an M/M/k/k queuing model can be used to evaluate overall steady state system performance for a given swarm size. Using these estimates, operators can rapidly compare system performance across different configurations, leading to more effective choices for swarm size. A sensitivity analysis is performed and its results are compared with the model, illustrating the appropriateness of our method to problems of plausible scale and complexity.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dubey-2018-107515">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>DROAN - Disparity-Space Representation for Obstacle Avoidance: Enabling Wire Mapping Avoidance</b>.</div><div class="csl-block csl-author">By Dubey, G., Madaan, R. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, pp. 6311–6318, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleDubey2018107515()">bibtex</button>

<script>
    function toggleDubey2018107515() {
        var x= document.getElementById('aDubey-2018-107515');
        // console.log("haha %o",typeof Dubey-2018-107515);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dubey2018107515()">abstract</button>


<script>
    function toggle2Dubey2018107515() {
        var x= document.getElementById('bDubey-2018-107515');
        // console.log("haha %o",typeof Dubey-2018-107515);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2018.8593499"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDubey-2018-107515" style="display:none"><pre>@inproceedings{Dubey-2018-107515,
  author = {Dubey, Geetesh and Madaan, Ratnesh and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2018.8593499},
  isbn = {9781538680940},
  issn = {21530866},
  month = oct,
  pages = {6311--6318},
  title = {{DROAN - D}isparity-Space Representation for Obstacle Avoidance: Enabling Wire Mapping Avoidance},
  year = {2018}
}
</pre></div>
<div id="bDubey-2018-107515" style="display:none"><pre>Wire detection, depth estimation and avoidance is one of the hardest challenges towards the ubiquitous presence of robust autonomous aerial vehicles. We present an approach and a system which tackles these three challenges along with generic obstacle avoidance as well. First, we perform monocular wire detection using a convolutional neural network under the semantic segmentation paradigm, and obtain a confidence map of wire pixels. Along with this, we also use a binocular stereo pair to detect other generic obstacles. We represent wires and generic obstacles using a disparity space representation and do a C-space expansion by using a non-linear sensor model we develop. Occupancy inference for collision checking is performed by maintaining a pose graph over multiple disparity images. For avoidance of wire and generic obstacles, we use a precomputed trajectory library, which is evaluated in an online fashion in accordance to a cost function over proximity to the goal. We follow this trajectory with a path tracking controller. Finally, we demonstrate the effectiveness of our proposed method in simulation for wire mapping, and on hardware by multiple runs for both wire and generic obstacle avoidance.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dong-2018-106594">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Hierarchical registration of unordered TLS point clouds based on binary shape context descriptor</b>. </div><div class="csl-block csl-author">By Dong, Z., Yang, B., Liang, F., Huang, R. and Scherer, S.</div><div class="csl-block csl-event">In <i>ISPRS Journal of Photogrammetry and Remote Sensing</i>, vol. 144, pp. 61–79, Oct. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleDong2018106594()">bibtex</button>

<script>
    function toggleDong2018106594() {
        var x= document.getElementById('aDong-2018-106594');
        // console.log("haha %o",typeof Dong-2018-106594);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dong2018106594()">abstract</button>


<script>
    function toggle2Dong2018106594() {
        var x= document.getElementById('bDong-2018-106594');
        // console.log("haha %o",typeof Dong-2018-106594);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1016/j.isprsjprs.2018.06.018"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDong-2018-106594" style="display:none"><pre>@article{Dong-2018-106594,
  author = {Dong, Zhen and Yang, Bisheng and Liang, Fuxun and Huang, Ronggang and Scherer, Sebastian},
  doi = {10.1016/j.isprsjprs.2018.06.018},
  issn = {09242716},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords = {Binary shape context,Hierarchical registration,Multiple overlaps,Point cloud registration,Point cloud similarity,Vector of locally aggregated descriptors},
  month = oct,
  pages = {61--79},
  title = {Hierarchical registration of unordered {TLS} point clouds based on binary shape context descriptor},
  volume = {144},
  year = {2018}
}
</pre></div>
<div id="bDong-2018-106594" style="display:none"><pre>Automatic registration of unordered point clouds collected by the terrestrial laser scanner (TLS) is the prerequisite for many applications including 3D model reconstruction, cultural heritage management, forest structure assessment, landslide monitoring, and solar energy analysis. However, most of the existing point cloud registration methods still suffer from some limitations. On one hand, most of them are considerable time-consuming and high computational complexity due to the exhaustive pairwise search for recovering the underlying overlaps, which makes them infeasible for the registration of large-scale point clouds. On the other hand, most of them only leverage pairwise overlaps and rarely use the overlaps between multiple point clouds, resulting in difficulty dealing with point clouds with limited overlaps. To overcome these limitations, this paper presents a Hierarchical Merging based Multiview Registration (HMMR) algorithm to align unordered point clouds from various scenes. First, the multi-level descriptors (i.e., local descriptor: Binary Shape Context (BSC) and global descriptor: Vector of Locally Aggregated Descriptor (VLAD)) are calculated. Second, the point clouds overlapping (adjacent) graph is efficiently constructed by leveraging the similarity between their corresponding VLAD vectors. Finally, the proposed method hierarchically registers multiple point clouds by iteratively performing optimal registration point clouds calculation, BSC descriptor based pairwise registration and point cloud groups overlapping (adjacent) graph update, until all the point clouds are aligned into a common coordinate reference. Comprehensive experiments demonstrate that the proposed algorithm obtains good performance in terms of successful registration rate, rotation error, translation error, and runtime, and outperformed the state-of-the-art approaches.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Arora:2018vy">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Hindsight is Only 50/50: Unsuitability of MDP based Approximate POMDP Solvers for Multi-resolution Information Gathering</b>. </div><div class="csl-block csl-author">By Arora, S., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Apr. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2018vy()">bibtex</button>

<script>
    function toggleArora2018vy() {
        var x= document.getElementById('aArora:2018vy');
        // console.log("haha %o",typeof Arora:2018vy);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Arora2018vy()">abstract</button>


<script>
    function toggle2Arora2018vy() {
        var x= document.getElementById('bArora:2018vy');
        // console.log("haha %o",typeof Arora:2018vy);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1804.02573"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aArora:2018vy" style="display:none"><pre>@article{Arora:2018vy,
  annote = {6 pages, 1 figure},
  archiveprefix = {arXiv},
  arxivid = {1804.02573},
  author = {Arora, Sankalp and Choudhury, Sanjiban and Scherer, Sebastian},
  eprint = {1804.02573},
  journal = {arXiv.org},
  month = apr,
  title = {Hindsight is Only 50/50: {Unsuitability} of {MDP} based Approximate {POMDP} Solvers for Multi-resolution Information Gathering},
  url = {http://arxiv.org/abs/1804.02573},
  year = {2018}
}
</pre></div>
<div id="bArora:2018vy" style="display:none"><pre>Partially Observable Markov Decision Processes (POMDPs) offer an elegant framework to model sequential decision making in uncertain environments. Solving POMDPs online is an active area of research and given the size of real-world problems approximate solvers are used. Recently, a few approaches have been suggested for solving POMDPs by using MDP solvers in conjunction with imitation learning. MDP based POMDP solvers work well for some cases, while catastrophically failing for others. The main failure point of such solvers is the lack of motivation for MDP solvers to gain information, since under their assumption the environment is either already known as much as it can be or the uncertainty will disappear after the next step. However for solving POMDP problems gaining information can lead to efficient solutions. In this paper we derive a set of conditions where MDP based POMDP solvers are provably sub-optimal. We then use the well-known tiger problem to demonstrate such sub-optimality. We show that multi-resolution, budgeted information gathering cannot be addressed using MDP based POMDP solvers. The contribution of the paper helps identify the properties of a POMDP problem for which the use of MDP based POMDP solvers is inappropriate, enabling better design choices.</pre></div>
</li>
<li><div class="text-justify">
    <span id="yanfu-2018-111053">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Integrating kinematics and environment context into deep inverse reinforcement learning for predicting off-road vehicle trajectories</b>.</div><div class="csl-block csl-author">By Zhang, Y., Wang, W., Bonatti, R., Maturana, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Conference on Robot Learning</i>2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleyanfu2018111053()">bibtex</button>

<script>
    function toggleyanfu2018111053() {
        var x= document.getElementById('ayanfu-2018-111053');
        // console.log("haha %o",typeof yanfu-2018-111053);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2yanfu2018111053()">abstract</button>


<script>
    function toggle2yanfu2018111053() {
        var x= document.getElementById('byanfu-2018-111053');
        // console.log("haha %o",typeof yanfu-2018-111053);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1810.07225"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="ayanfu-2018-111053" style="display:none"><pre>@inproceedings{yanfu-2018-111053,
  archiveprefix = {arXiv},
  arxivid = {1810.07225},
  author = {Zhang, Yanfu and Wang, Wenshan and Bonatti, Rogerio and Maturana, Daniel and Scherer, Sebastian},
  booktitle = {Conference on Robot Learning},
  eprint = {1810.07225},
  month = oct,
  publisher = {Journal of Machine Learning Research},
  title = {Integrating kinematics and environment context into deep inverse reinforcement learning for predicting off-road vehicle trajectories},
  url = {http://arxiv.org/abs/1810.07225},
  year = {2018}
}
</pre></div>
<div id="byanfu-2018-111053" style="display:none"><pre>Predicting the motion of a mobile agent from a third-person perspective is an important component for many robotics applications, such as autonomous navigation and tracking. With accurate motion prediction of other agents, robots can plan for more intelligent behaviors to achieve specified objectives, instead of acting in a purely reactive way. Previous work addresses motion prediction by either only filtering kinematics, or using hand-designed and learned representations of the environment. Instead of separating kinematic and environmental context, we propose a novel approach to integrate both into an inverse reinforcement learning (IRL) framework for trajectory prediction. Instead of exponentially increasing the state-space complexity with kinematics, we propose a two-stage neural network architecture that considers motion and environment together to recover the reward function. The first-stage network learns feature representations of the environment using low-level LiDAR statistics and the second-stage network combines those learned features with kinematics data. We collected over 30 km of off-road driving data and validated experimentally that our method can effectively extract useful environmental and kinematic features. We generate accurate predictions of the distribution of future trajectories of the vehicle, encoding complex behaviors such as multi-modal distributions at road intersections, and even show different predictions at the same intersection depending on the vehicle’s speed.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Das:2018es">[11]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality</b>. </div><div class="csl-block csl-author">By Das, M.P., Dong, Z. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, pp. 6357–6363, Nov. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleDas2018es()">bibtex</button>

<script>
    function toggleDas2018es() {
        var x= document.getElementById('aDas:2018es');
        // console.log("haha %o",typeof Das:2018es);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Das2018es()">abstract</button>


<script>
    function toggle2Das2018es() {
        var x= document.getElementById('bDas:2018es');
        // console.log("haha %o",typeof Das:2018es);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2018.8594318"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDas:2018es" style="display:none"><pre>@article{Das:2018es,
  archiveprefix = {arXiv},
  arxivid = {1811.02563},
  author = {Das, Manash Pratim and Dong, Zhen and Scherer, Sebastian},
  doi = {10.1109/IROS.2018.8594318},
  eprint = {1811.02563},
  isbn = {9781538680940},
  issn = {21530866},
  journal = {IEEE International Conference on Intelligent Robots and Systems},
  month = nov,
  pages = {6357--6363},
  title = {Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality},
  year = {2018}
}
</pre></div>
<div id="bDas:2018es" style="display:none"><pre>This paper introduces a method of structure inspection using mixed-reality headsets to reduce the human effort in reporting accurate inspection information such as fault locations in 3D coordinates. Prior to every inspection, the headset needs to be localized. While external pose estimation and fiducial marker based localization would require setup, maintenance, and manual calibration; marker-free self-localization can be achieved using the onboard depth sensor and camera. However, due to limited depth sensor range of portable mixed-reality headsets like Microsoft HoloLens, localization based on simple point cloud registration (sPCR) would require extensive mapping of the environment. Also, localization based on camera image would face same issues as stereo ambiguities and hence depends on viewpoint. We thus introduce a novel approach to Joint Point Cloud and Image-based Localization (JPIL) for mixed-reality headsets that uses visual cues and headset orientation to register small, partially overlapped point clouds and save significant manual labor and time in environment mapping. Our empirical results compared to sPCR show average 10 fold reduction of required overlap surface area that could potentially save on average 20 minutes per inspection. JPIL is not only restricted to inspection tasks but also can be essential in enabling intuitive human-robot interaction for spatial mapping and scene understanding in conjunction with other agents like autonomous robotic systems that are increasingly being deployed in outdoor environments for applications like structural inspection.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Garg:2018tc">[12]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Monocular and Stereo Cues for Landing Zone Evaluation for Micro UAVs</b>. </div><div class="csl-block csl-author">By Garg, R., Yang, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Dec. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleGarg2018tc()">bibtex</button>

<script>
    function toggleGarg2018tc() {
        var x= document.getElementById('aGarg:2018tc');
        // console.log("haha %o",typeof Garg:2018tc);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Garg2018tc()">abstract</button>


<script>
    function toggle2Garg2018tc() {
        var x= document.getElementById('bGarg:2018tc');
        // console.log("haha %o",typeof Garg:2018tc);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1812.03539"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aGarg:2018tc" style="display:none"><pre>@article{Garg:2018tc,
  archiveprefix = {arXiv},
  arxivid = {1812.03539},
  author = {Garg, Rohit and Yang, Shichao and Scherer, Sebastian},
  eprint = {1812.03539},
  journal = {arXiv.org},
  month = dec,
  title = {Monocular and Stereo Cues for Landing Zone Evaluation for Micro {UAVs}},
  url = {http://arxiv.org/abs/1812.03539},
  year = {2018}
}
</pre></div>
<div id="bGarg:2018tc" style="display:none"><pre>Autonomous and safe landing is important for unmanned aerial vehicles. We present a monocular and stereo image based method for fast and accurate landing zone evaluation for UAVs in various scenarios. Many existing methods rely on Lidar or depth sensor to provide accurate and dense surface reconstruction. We utilize stereo images to evaluate the slope and monocular images to compute homography error. By combining them together, our approach works for both rigid and non-rigid dynamic surfaces. Experiments on many outdoor scenes such as water, grass and roofs, demonstrate the robustness and effectiveness of our approach.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Gupta:2018tg">[13]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Open Problems in Robotic Anomaly Detection</b>. </div><div class="csl-block csl-author">By Gupta, R., Kurtz, Z.T., Scherer, S. and Smereka, J.M.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Sep. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleGupta2018tg()">bibtex</button>

<script>
    function toggleGupta2018tg() {
        var x= document.getElementById('aGupta:2018tg');
        // console.log("haha %o",typeof Gupta:2018tg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Gupta2018tg()">abstract</button>


<script>
    function toggle2Gupta2018tg() {
        var x= document.getElementById('bGupta:2018tg');
        // console.log("haha %o",typeof Gupta:2018tg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1809.03565"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aGupta:2018tg" style="display:none"><pre>@article{Gupta:2018tg,
  archiveprefix = {arXiv},
  arxivid = {1809.03565},
  author = {Gupta, Ritwik and Kurtz, Zachary T. and Scherer, Sebastian and Smereka, Jonathon M.},
  eprint = {1809.03565},
  journal = {arXiv.org},
  month = sep,
  title = {Open Problems in Robotic Anomaly Detection},
  url = {http://arxiv.org/abs/1809.03565},
  year = {2018}
}
</pre></div>
<div id="bGupta:2018tg" style="display:none"><pre>Failures in robotics can have disastrous consequences that worsen rapidly over time. This, the ability to rely on robotic systems, depends on our ability to monitor them and intercede when necessary, manually or autonomously. Prior work in this area surveys intrusion detection and security challenges in robotics, but a discussion of the more general anomaly detection problems is lacking. As such, we provide a brief insight-focused discussion and frameworks of thought on some compelling open problems with anomaly detection in robotic systems. Namely, we discuss non-malicious faults, invalid data, intentional anomalous behavior, hierarchical anomaly detection, distribution of computation, and anomaly correction on the fly. We demonstrate the need for additional work in these areas by providing a case study which examines the limitations of implementing a basic anomaly detection (AD) system in the Robot Operating System (ROS) 2 middleware. Showing that if even supporting a basic system is a significant hurdle, the path to more complex and advanced AD systems is even more problematic. We discuss these ROS 2 platform limitations to support solutions in robotic anomaly detection and provide recommendations to address the issues discovered.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Schopferer:2018wk">[14]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Path Planning for Unmanned Fixed-Wing Aircraft in Uncertain Wind Conditions Using Trochoids</b>.</div><div class="csl-block csl-author">By Schopferer, S., Lorenz, J.S., Keipour, A. and Scherer, S.</div><div class="csl-block csl-event">In <i>2018 International Conference on Unmanned Aircraft Systems, ICUAS 2018</i>, Dallas, TX, pp. 503–512, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleSchopferer2018wk()">bibtex</button>

<script>
    function toggleSchopferer2018wk() {
        var x= document.getElementById('aSchopferer:2018wk');
        // console.log("haha %o",typeof Schopferer:2018wk);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Schopferer2018wk()">abstract</button>


<script>
    function toggle2Schopferer2018wk() {
        var x= document.getElementById('bSchopferer:2018wk');
        // console.log("haha %o",typeof Schopferer:2018wk);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICUAS.2018.8453391"><input type="button" class="button1" value="doi" /></a>




<a href="https://www.youtube.com/watch?v=cltd0eY2dcM"><input type="button" class="button5" value="video" /></a>

<!--  -->
</div>

<div id="aSchopferer:2018wk" style="display:none"><pre>@inproceedings{Schopferer:2018wk,
  address = {Dallas, TX},
  author = {Schopferer, Simon and Lorenz, Julian Soren and Keipour, Azarakhsh and Scherer, Sebastian},
  booktitle = {2018 International Conference on Unmanned Aircraft Systems, ICUAS 2018},
  doi = {10.1109/ICUAS.2018.8453391},
  isbn = {9781538613535},
  keywords = {Fixed-Wing UAV,Path Planning,Trochoids,Wind},
  month = jun,
  pages = {503--512},
  title = {Path Planning for Unmanned Fixed-Wing Aircraft in Uncertain Wind Conditions Using Trochoids},
  video = {https://www.youtube.com/watch?v=cltd0eY2dcM},
  year = {2018}
}
</pre></div>
<div id="bSchopferer:2018wk" style="display:none"><pre>On-board path planning is a key capability for safe autonomous unmanned flight. Recently, it has been shown that using trochoids for turn segments allows for run-time efficient path planning under consideration of the prevailing wind. However, with varying wind conditions and uncertainty in the estimation of wind speed and direction, paths optimized for a reference wind condition may become infeasible to track for the aircraft. In this work, we discuss how to calculate conservative turn rate limits and we present a novel approach to calculate safety distances along trochoidal turn segments in order to account for an unknown wind speed and airspeed component of bounded magnitude. This allows to plan flight paths that are optimized for the currently expected wind condition but are still safe in case the aircraft experiences different wind conditions. We present results from simulation and flight tests which demonstrate the impact of uncertain and varying wind conditions on the tracking performance of paths with circular and trochoidal turn segments. The results show that trochoids can be used to reduce path tracking errors even if the prevailing wind changes significantly. Furthermore, the proposed method to calculate safety distances conservatively over-approximates path deviations in all considered cases. Thus, it can be used to plan safe paths in the presence of uncertain wind conditions without solely relying on conservative performance limits.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Tian-2018-110482">[15]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Positioning error analysis of least squares method for wireless sensor networks</b>.</div><div class="csl-block csl-author">By Tian, X., Zhen, W., Scherer, S. and Lu, X.</div><div class="csl-block csl-event">In <i>50th International Symposium on Robotics, ISR 2018</i>, pp. 143–146, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleTian2018110482()">bibtex</button>

<script>
    function toggleTian2018110482() {
        var x= document.getElementById('aTian-2018-110482');
        // console.log("haha %o",typeof Tian-2018-110482);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Tian2018110482()">abstract</button>


<script>
    function toggle2Tian2018110482() {
        var x= document.getElementById('bTian-2018-110482');
        // console.log("haha %o",typeof Tian-2018-110482);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aTian-2018-110482" style="display:none"><pre>@inproceedings{Tian-2018-110482,
  author = {Tian, Xiangrui and Zhen, Weikun and Scherer, Sebastian and Lu, Xiong},
  booktitle = {50th International Symposium on Robotics, ISR 2018},
  isbn = {9781510870314},
  month = jun,
  pages = {143--146},
  publisher = {VDE},
  title = {Positioning error analysis of least squares method for wireless sensor networks},
  year = {2018}
}
</pre></div>
<div id="bTian-2018-110482" style="display:none"><pre>Wireless sensor networks (WSN) is widely used for indoor positioning and navigation of mobile robots. Least squares method (LSM) is the most common and simple method for position calculation, and various optimization algorithms were designed elaborately for reducing localization error. Unlike other localization papers which focus on designing elaborate localization algorithms, this paper takes a different perspective, focusing on the error propagation problem, addressing questions such as where the localization error comes from and how it propagates. Based on the theory of variance and covariance, a novel simplified error propagation algorithm is proposed to analyse the localization error for triangulation method. This algorithm exactly shows influence of ranging errors and network structure on positioning. Finally, a simulation test in Matlab is carried out to verify the validity of the proposed algorithm, and it is shown that the algorithm significantly simplifies the calulation of the positioning error. The work of this paper can be used for mul-ti-sensor fusion where an accurate error model is required. Besides, error estimation is also useful for error control by optimizing the structure of WSN.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Maturana:2018jq">[16]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Real-Time Semantic Mapping for Autonomous Off-Road Navigation</b>.</div><div class="csl-block csl-author">By Maturana, D., Chou, P.-W., Uenoyama, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>Field and Service Robotics</i>, </div><div class="csl-block csl-editor">Springer, Cham, 2018, pp. pp. 335–350</div></div></span>
    <br />
<button class="button0" onclick="toggleMaturana2018jq()">bibtex</button>

<script>
    function toggleMaturana2018jq() {
        var x= document.getElementById('aMaturana:2018jq');
        // console.log("haha %o",typeof Maturana:2018jq);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Maturana2018jq()">abstract</button>


<script>
    function toggle2Maturana2018jq() {
        var x= document.getElementById('bMaturana:2018jq');
        // console.log("haha %o",typeof Maturana:2018jq);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-319-67361-5_22"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMaturana:2018jq" style="display:none"><pre>@incollection{Maturana:2018jq,
  author = {Maturana, Daniel and Chou, Po-Wei and Uenoyama, Masashi and Scherer, Sebastian},
  booktitle = {Field and Service Robotics},
  doi = {10.1007/978-3-319-67361-5_22},
  pages = {335--350},
  publisher = {Springer, Cham},
  title = {Real-Time Semantic Mapping for Autonomous Off-Road Navigation},
  year = {2018}
}
</pre></div>
<div id="bMaturana:2018jq" style="display:none"><pre>In this paper we describe a semantic mapping system for autonomous off-road driving with an All-Terrain Vehicle (ATVs). The system’s goal is to provide a richer representation of the environment than a purely geometric map, allowing it to distinguish, e.g., tall grass from obstacles. The system builds a 2.5D grid map encoding both geometric (terrain height) and semantic information (navigation-relevant classes such as trail, grass, etc.). The geometric and semantic information are estimated online and in real-time from LiDAR and image sensor data, respectively. Using this semantic map, motion planners can create semantically aware trajectories. To achieve robust and efficient semantic segmentation, we design a custom Convolutional Neural Network (CNN) and train it with a novel dataset of labelled off-road imagery built for this purpose. We evaluate our semantic segmentation offline, showing comparable performance to the state of the art with slightly lower latency. We also show closed-loop field results with an autonomous ATV driving over challenging off-road terrain by using the semantic map in conjunction with a simple path planner. Our models and labelled dataset will be publicly available at http://dimatura.net/offroad.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Liu:2017wv">[17]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Robust image-based crack detection in concrete structure using multi-scale enhancement and visual features</b>.</div><div class="csl-block csl-author">By Liu, X., Ai, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - International Conference on Image Processing, ICIP</i>, pp. 2304–2308, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleLiu2017wv()">bibtex</button>

<script>
    function toggleLiu2017wv() {
        var x= document.getElementById('aLiu:2017wv');
        // console.log("haha %o",typeof Liu:2017wv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Liu2017wv()">abstract</button>


<script>
    function toggle2Liu2017wv() {
        var x= document.getElementById('bLiu:2017wv');
        // console.log("haha %o",typeof Liu:2017wv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICIP.2017.8296693"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aLiu:2017wv" style="display:none"><pre>@inproceedings{Liu:2017wv,
  author = {Liu, Xiangzeng and Ai, Yunfeng and Scherer, Sebastian},
  booktitle = {Proceedings - International Conference on Image Processing, ICIP},
  doi = {10.1109/ICIP.2017.8296693},
  isbn = {9781509021758},
  issn = {15224880},
  keywords = {Concrete structure,Crack detection,Guided filter,Image enhancement},
  month = sep,
  pages = {2304--2308},
  title = {Robust image-based crack detection in concrete structure using multi-scale enhancement and visual features},
  year = {2018}
}
</pre></div>
<div id="bLiu:2017wv" style="display:none"><pre>Crack detection is an important technique to evaluate the safety and predict the life of a concrete asset. In order to improve the robustness of the crack detection in complex background, a new crack detection framework based on multi-scale enhancement and visual features is developed. Firstly, to deal with the effect of low contrast, a multi-scale enhancement method using guided filter and gradient information is proposed. Then, the adaptive threshold algorithm is used to obtain the binary image. Finally, the combination of morphological processing and visual features are adopted to purify the cracks. The experimental results with different images of real concrete surface demonstrate the high robustness and validity of the developed technique, in which the average TPR can reach 94.22%.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Kim:2018ew">[18]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Season-Invariant Semantic Segmentation with a Deep Multimodal Network</b>.</div><div class="csl-block csl-author">By Kim, D.-K., Maturana, D., Uenoyama, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>Field and Service Robotics</i>, </div><div class="csl-block csl-editor">Springer, Cham, 2018, pp. pp. 255–270</div></div></span>
    <br />
<button class="button0" onclick="toggleKim2018ew()">bibtex</button>

<script>
    function toggleKim2018ew() {
        var x= document.getElementById('aKim:2018ew');
        // console.log("haha %o",typeof Kim:2018ew);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Kim2018ew()">abstract</button>


<script>
    function toggle2Kim2018ew() {
        var x= document.getElementById('bKim:2018ew');
        // console.log("haha %o",typeof Kim:2018ew);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-319-67361-5_17"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aKim:2018ew" style="display:none"><pre>@incollection{Kim:2018ew,
  author = {Kim, Dong-Ki and Maturana, Daniel and Uenoyama, Masashi and Scherer, Sebastian},
  booktitle = {Field and Service Robotics},
  doi = {10.1007/978-3-319-67361-5_17},
  pages = {255--270},
  publisher = {Springer, Cham},
  title = {Season-Invariant Semantic Segmentation with a Deep Multimodal Network},
  year = {2018}
}
</pre></div>
<div id="bKim:2018ew" style="display:none"><pre>Semantic scene understanding is a useful capability for autonomous vehicles operating in off-roads. While cameras are the most common sensor used for semantic classification, the performance of methods using camera imagery may suffer when there is significant variation between the train and testing sets caused by illumination, weather, and seasonal variations. On the other hand, 3D information from active sensors such as LiDAR is comparatively invariant to these factors, which motivates us to investigate whether it can be used to improve performance in this scenario. In this paper, we propose a novel multimodal Convolutional Neural Network (CNN) architecture consisting of two streams, 2D and 3D, which are fused by projecting 3D features to image space to achieve a robust pixelwise semantic segmentation. We evaluate our proposed method in a novel off-road terrain classification benchmark, and show a 25% improvement in mean Intersection over Union (IoU) of navigation-related semantic classes, relative to an image-only baseline.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chandarana-2018-107674">[19]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Swarm size planning tool for multi-job type missions</b>.</div><div class="csl-block csl-author">By Chandarana, M., Lewis, M., Allen, B.D., Sycara, K. and Scherer, S.</div><div class="csl-block csl-event">In <i>2018 Aviation Technology, Integration, and Operations Conference</i>2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleChandarana2018107674()">bibtex</button>

<script>
    function toggleChandarana2018107674() {
        var x= document.getElementById('aChandarana-2018-107674');
        // console.log("haha %o",typeof Chandarana-2018-107674);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chandarana2018107674()">abstract</button>


<script>
    function toggle2Chandarana2018107674() {
        var x= document.getElementById('bChandarana-2018-107674');
        // console.log("haha %o",typeof Chandarana-2018-107674);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.2514/6.2018-3846"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChandarana-2018-107674" style="display:none"><pre>@inproceedings{Chandarana-2018-107674,
  author = {Chandarana, Meghan and Lewis, Michael and Allen, Bonnie Danette and Sycara, Katia and Scherer, Sebastian},
  booktitle = {2018 Aviation Technology, Integration, and Operations Conference},
  doi = {10.2514/6.2018-3846},
  isbn = {9781624105562},
  month = sep,
  title = {Swarm size planning tool for multi-job type missions},
  year = {2018}
}
</pre></div>
<div id="bChandarana-2018-107674" style="display:none"><pre>As part of swarm search and service (SSS) missions, swarms are tasked with searching an area while simultaneously servicing jobs as they are encountered. Jobs must be immediately serviced and can be one of multiple types. Each type requires that vehicle(s) break off from the swarm and travel to the job site for a specified amount of time. The number of vehicles needed and the service time for each job type are known. Once a job has been successfully serviced, vehicles return to the swarm and are available for reallocation. When planning SSS missions, human operators are tasked with determining the required number of vehicles needed to handle the expected job demand. The complex relationship between job type parameters makes this choice challenging. This work presents a prediction model used to estimate the swarm size necessary to achieve a given performance. User studies were conducted to determine the usefulness and ease of use of such a prediction model as an aid during mission planning. Results show that using the planning tool leads to 7x less missed area and a 50% cost reduction.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Khosiawan:2018kf">[20]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Toward delay-tolerant multiple-unmanned aerial vehicle scheduling system using Multi-strategy Coevolution algorithm</b>. </div><div class="csl-block csl-author">By Khosiawan, Y., Scherer, S. and Nielsen, I.</div><div class="csl-block csl-event">In <i>Advances in Mechanical Engineering</i>, vol. 10, no. 12, p. 168781401881523, Dec. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleKhosiawan2018kf()">bibtex</button>

<script>
    function toggleKhosiawan2018kf() {
        var x= document.getElementById('aKhosiawan:2018kf');
        // console.log("haha %o",typeof Khosiawan:2018kf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Khosiawan2018kf()">abstract</button>


<script>
    function toggle2Khosiawan2018kf() {
        var x= document.getElementById('bKhosiawan:2018kf');
        // console.log("haha %o",typeof Khosiawan:2018kf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1177/1687814018815235"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aKhosiawan:2018kf" style="display:none"><pre>@article{Khosiawan:2018kf,
  author = {Khosiawan, Yohanes and Scherer, Sebastian and Nielsen, Izabela},
  doi = {10.1177/1687814018815235},
  issn = {16878140},
  journal = {Advances in Mechanical Engineering},
  keywords = {Unmanned aerial vehicle,delay-tolerant,metaheuristic,optimization,scheduling},
  month = dec,
  number = {12},
  pages = {168781401881523},
  title = {Toward delay-tolerant multiple-unmanned aerial vehicle scheduling system using Multi-strategy Coevolution algorithm},
  volume = {10},
  year = {2018}
}
</pre></div>
<div id="bKhosiawan:2018kf" style="display:none"><pre>Autonomous bridge inspection operations using unmanned aerial vehicles take multiple task assignments and constraints into account. To efficiently execute the operations, a schedule is required. Generating a cost optimum schedule of multiple-unmanned aerial vehicle operations is known to be Non-deterministic Polynomial-time (NP)-hard. This study approaches such a problem with heuristic-based algorithms to get a high-quality feasible solution in a short computation time. A constructive heuristic called Retractable Chain Task Assignment algorithm is presented to build an evaluable schedule from a task sequence. The task sequence representation is used during the search to perform seamless operations. Retractable Chain Task Assignment algorithm calculates and incorporates slack time to the schedule according to the properties of the task. The slack time acts as a cushion which makes the schedule delay-tolerant. This algorithm is incorporated with a metaheuristic algorithm called Multi-strategy Coevolution to search the solution space. The proposed algorithm is verified through numerical simulations, which take inputs from real flight test data. The obtained solutions are evaluated based on the makespan, battery consumption, computation time, and the robustness level of the schedules. The performance of Multi-strategy Coevolution is compared to Differential Evolution, Particle Swarm Optimization, and Differential Evolution–Fused Particle Swarm Optimization. The simulation results show that Multi-strategy Coevolution gives better objective values than the other algorithms.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Zhu:2018bb">[21]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Visual Place Recognition in Long-term and Large-scale Environment based on CNN Feature</b>.</div><div class="csl-block csl-author">By Zhu, J., Ai, Y., Tian, B., Cao, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Intelligent Vehicles Symposium, Proceedings</i>, pp. 1679–1685, 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhu2018bb()">bibtex</button>

<script>
    function toggleZhu2018bb() {
        var x= document.getElementById('aZhu:2018bb');
        // console.log("haha %o",typeof Zhu:2018bb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhu2018bb()">abstract</button>


<script>
    function toggle2Zhu2018bb() {
        var x= document.getElementById('bZhu:2018bb');
        // console.log("haha %o",typeof Zhu:2018bb);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IVS.2018.8500686"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aZhu:2018bb" style="display:none"><pre>@inproceedings{Zhu:2018bb,
  author = {Zhu, Jianliang and Ai, Yunfeng and Tian, Bin and Cao, Dongpu and Scherer, Sebastian},
  booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
  doi = {10.1109/IVS.2018.8500686},
  isbn = {9781538644522},
  month = oct,
  pages = {1679--1685},
  title = {Visual Place Recognition in Long-term and Large-scale Environment based on {CNN} Feature},
  year = {2018}
}
</pre></div>
<div id="bZhu:2018bb" style="display:none"><pre>With the universal application of camera in intelligent vehicles, visual place recognition has become a major problem in intelligent vehicle localization. The traditional solution is to make visual description of place images using hand-crafted feature for matching places, but this description method is not very good for extreme variability, especially for seasonal transformation. In this paper, we propose a new method based on convolutional neural network (CNN), by putting images into the pre-trained network model to get automatically learned image descriptors, and through some operations of pooling, fusion and binarization to optimize them, then the similarity result of place recognition is presented with the Hamming distance of the place sequence. In the experimental part, we compare our method with some state-of-the-art algorithms, FABMAP, ABLE-M and SeqSLAM, to illustrate its advantages. The experimental results show that our method based on CNN achieves better performance than other methods on the representative public datasets.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Bonatti2018a">[22]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous drone cinematographer: Using artistic principles to create smooth, safe, occlusion-free trajectories for aerial filming</b>. </div><div class="csl-block csl-author">By Bonatti, R., Zhang, Y., Choudhury, S., Wang, W. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Aug. 2018.</div></div></span>
    <br />
<button class="button0" onclick="toggleBonatti2018a()">bibtex</button>

<script>
    function toggleBonatti2018a() {
        var x= document.getElementById('aBonatti2018a');
        // console.log("haha %o",typeof Bonatti2018a);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Bonatti2018a() {
        var x= document.getElementById('bBonatti2018a');
        // console.log("haha %o",typeof Bonatti2018a);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aBonatti2018a" style="display:none"><pre>@article{Bonatti2018a,
  author = {Bonatti, Rogerio and Zhang, Yanfu and Choudhury, Sanjiban and Wang, Wenshan and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {Autonomous drone cinematographer: {Using} artistic principles to create smooth, safe, occlusion-free trajectories for aerial filming},
  year = {2018},
  month = aug,
  archiveprefix = {arxiv},
  eprint = {1808.09563v1},
  primaryclass = {cs.RO}
}
</pre></div>
<div id="bBonatti2018a" style="display:none"><pre></pre></div>
</li></ul.no-bullet>

<h1 id="2017">2017</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Song:2016gi">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A multi-sensor fusion MAV state estimation from long-range stereo, IMU, GPS and barometric sensors</b>. </div><div class="csl-block csl-author">By Song, Y., Nuske, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Sensors</i>, vol. 17, no. 1, p. 11, Dec. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleSong2016gi()">bibtex</button>

<script>
    function toggleSong2016gi() {
        var x= document.getElementById('aSong:2016gi');
        // console.log("haha %o",typeof Song:2016gi);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Song2016gi()">abstract</button>


<script>
    function toggle2Song2016gi() {
        var x= document.getElementById('bSong:2016gi');
        // console.log("haha %o",typeof Song:2016gi);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.3390/s17010011"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aSong:2016gi" style="display:none"><pre>@article{Song:2016gi,
  author = {Song, Yu and Nuske, Stephen and Scherer, Sebastian},
  doi = {10.3390/s17010011},
  issn = {14248220},
  journal = {Sensors},
  keywords = {Absolute and relative state measurements,GPS-denied state estimation,Long-range stereo visual odometry,Multi-sensor fusion,Stochastic cloning EKF},
  month = dec,
  number = {1},
  pages = {11},
  title = {A multi-sensor fusion {MAV} state estimation from long-range stereo, {IMU}, {GPS} and barometric sensors},
  volume = {17},
  year = {2017}
}
</pre></div>
<div id="bSong:2016gi" style="display:none"><pre>State estimation is the most critical capability for MAV (Micro-Aerial Vehicle) localization, autonomous obstacle avoidance, robust flight control and 3D environmental mapping. There are three main challenges for MAV state estimation: (1) it can deal with aggressive 6 DOF (Degree Of Freedom) motion; (2) it should be robust to intermittent GPS (Global Positioning System) (even GPS-denied) situations; (3) it should work well both for low- and high-altitude flight. In this paper, we present a state estimation technique by fusing long-range stereo visual odometry, GPS, barometric and IMU (Inertial Measurement Unit) measurements. The new estimation system has two main parts, a stochastic cloning EKF (Extended Kalman Filter) estimator that loosely fuses both absolute state measurements (GPS, barometer) and the relative state measurements (IMU, visual odometry), and is derived and discussed in detail. A long-range stereo visual odometry is proposed for high-altitude MAV odometry calculation by using both multi-view stereo triangulation and a multi-view stereo inverse depth filter. The odometry takes the EKF information (IMU integral) for robust camera pose tracking and image feature matching, and the stereo odometry output serves as the relative measurements for the update of the state estimation. Experimental results on a benchmark dataset and our real flight dataset show the effectiveness of the proposed state estimation system, especially for the aggressive, intermittent GPS and high-altitude MAV flight.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dugar:2017vo">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A KITE in the wind: Smooth trajectory optimization in a moving reference frame</b>.</div><div class="csl-block csl-author">By Dugar, V., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Singapore, Singapore, pp. 109–116, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleDugar2017vo()">bibtex</button>

<script>
    function toggleDugar2017vo() {
        var x= document.getElementById('aDugar:2017vo');
        // console.log("haha %o",typeof Dugar:2017vo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dugar2017vo()">abstract</button>


<script>
    function toggle2Dugar2017vo() {
        var x= document.getElementById('bDugar:2017vo');
        // console.log("haha %o",typeof Dugar:2017vo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2017.7989017"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDugar:2017vo" style="display:none"><pre>@inproceedings{Dugar:2017vo,
  address = {Singapore, Singapore},
  author = {Dugar, Vishal and Choudhury, Sanjiban and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2017.7989017},
  isbn = {9781509046331},
  issn = {10504729},
  month = may,
  pages = {109--116},
  title = {A {KITE} in the wind: Smooth trajectory optimization in a moving reference frame},
  year = {2017}
}
</pre></div>
<div id="bDugar:2017vo" style="display:none"><pre>A significant challenge for unmanned aerial vehicles capable of flying long distances is planning in a wind field. Although there has been a plethora of work on the individual topics of planning long routes, smooth trajectory optimization and planning in a wind field, it is difficult for these methods to scale to solve the combined problem. In this paper, we address the problem of planning long, dynamically feasible, time-optimal trajectories in the presence of wind (which creates a moving reference frame). We present an algorithm, \kappaITE, that elegantly decouples the joint trajectory optimization problem into individual path optimization in a fixed ground frame and a velocity profile optimization in a moving reference frame. The key idea is to derive a decoupling framework that guarantees feasibility of the final fused trajectory. Our results show that \kappaITE is able to produce high-quality solutions for planning with a helicopter flying at speeds of 50 m/s, handling winds up to 20 m/s and missions over 200 km. We validate our approach with real-world experiments on a full-scale helicopter with a pilot in the loop. Our approach paves the way forward for autonomous systems to exhibit pilot-like behavior when flying missions in winds aloft.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2017tr">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Adaptive information gathering via imitation learning</b>. </div><div class="csl-block csl-author">By Choudhury, S., Kapoor, A., Ranade, G., Scherer, S. and Dey, D.</div><div class="csl-block csl-event">In <i>Robotics: Science and Systems</i>, vol. 13, May 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2017tr()">bibtex</button>

<script>
    function toggleChoudhury2017tr() {
        var x= document.getElementById('aChoudhury:2017tr');
        // console.log("haha %o",typeof Choudhury:2017tr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2017tr()">abstract</button>


<script>
    function toggle2Choudhury2017tr() {
        var x= document.getElementById('bChoudhury:2017tr');
        // console.log("haha %o",typeof Choudhury:2017tr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.15607/rss.2017.xiii.041"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2017tr" style="display:none"><pre>@article{Choudhury:2017tr,
  annote = {Robotics Science and Systems, 2017},
  archiveprefix = {arXiv},
  arxivid = {1705.07834},
  author = {Choudhury, Sanjiban and Kapoor, Ashish and Ranade, Gireeja and Scherer, Sebastian and Dey, Debadeepta},
  doi = {10.15607/rss.2017.xiii.041},
  eprint = {1705.07834},
  isbn = {9780992374730},
  issn = {2330765X},
  journal = {Robotics: Science and Systems},
  month = may,
  title = {Adaptive information gathering via imitation learning},
  volume = {13},
  year = {2017}
}
</pre></div>
<div id="bChoudhury:2017tr" style="display:none"><pre>In the adaptive information gathering problem, a policy is required to select an informative sensing location using the history of measurements acquired thus far. While there is an extensive amount of prior work investigating effective practical approximations using variants of Shannon’s entropy, the efficacy of such policies heavily depends on the geometric distribution of objects in the world. On the other hand, the principled approach of employing online POMDP solvers is rendered impractical by the need to explicitly sample online from a posterior distribution of world maps. We present a novel data-driven imitation learning framework to efficiently train information gathering policies. The policy imitates a clairvoyant oracle - an oracle that at train time has full knowledge about the world map and can compute maximally informative sensing locations. We analyze the learnt policy by showing that offline imitation of a clairvoyant oracle is implicitly equivalent to online oracle execution in conjunction with posterior sampling. This observation allows us to obtain powerful near-optimality guarantees for information gathering problems possessing an adaptive sub-modularity property. As demonstrated on a spectrum of 2D and 3D exploration problems, the trained policies enjoy the best of both worlds - they adapt to different world map distributions while being computationally inexpensive to evaluate.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang:2017uf">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Direct monocular odometry using points and lines</b>. </div><div class="csl-block csl-author">By Yang, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, pp. 3871–3877, Mar. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleYang2017uf()">bibtex</button>

<script>
    function toggleYang2017uf() {
        var x= document.getElementById('aYang:2017uf');
        // console.log("haha %o",typeof Yang:2017uf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yang2017uf()">abstract</button>


<script>
    function toggle2Yang2017uf() {
        var x= document.getElementById('bYang:2017uf');
        // console.log("haha %o",typeof Yang:2017uf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2017.7989446"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aYang:2017uf" style="display:none"><pre>@article{Yang:2017uf,
  annote = {ICRA 2017},
  author = {Yang, Shichao and Scherer, Sebastian},
  doi = {10.1109/ICRA.2017.7989446},
  isbn = {9781509046331},
  issn = {10504729},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  month = mar,
  pages = {3871--3877},
  title = {Direct monocular odometry using points and lines},
  year = {2017}
}
</pre></div>
<div id="bYang:2017uf" style="display:none"><pre>Most visual odometry algorithm for a monocular camera focuses on points, either by feature matching, or direct alignment of pixel intensity, while ignoring a common but important geometry entity: edges. In this paper, we propose an odometry algorithm that combines points and edges to benefit from the advantages of both direct and feature based methods. It works better in texture-less environments and is also more robust to lighting changes and fast motion by increasing the convergence basin. We maintain a depth map for the keyframe then in the tracking part, the camera pose is recovered by minimizing both the photometric error and geometric error to the matched edge in a probabilistic framework. In the mapping part, edge is used to speed up and increase stereo matching accuracy. On various public datasets, our algorithm achieves better or comparable performance than state-of-the-art monocular odometry methods. In some challenging texture-less environments, our algorithm reduces the state estimation error over 50%.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dubey:2017wo">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>DROAN - Disparity-space representation for obstacle Avoidance</b>.</div><div class="csl-block csl-author">By Dubey, G., Arora, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Vancouver, pp. 1324–1330, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleDubey2017wo()">bibtex</button>

<script>
    function toggleDubey2017wo() {
        var x= document.getElementById('aDubey:2017wo');
        // console.log("haha %o",typeof Dubey:2017wo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dubey2017wo()">abstract</button>


<script>
    function toggle2Dubey2017wo() {
        var x= document.getElementById('bDubey:2017wo');
        // console.log("haha %o",typeof Dubey:2017wo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2017.8202309"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDubey:2017wo" style="display:none"><pre>@inproceedings{Dubey:2017wo,
  address = {Vancouver},
  author = {Dubey, Geetesh and Arora, Sankalp and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2017.8202309},
  isbn = {9781538626825},
  issn = {21530866},
  month = sep,
  pages = {1324--1330},
  title = {{DROAN} - {Disparity-space} representation for obstacle Avoidance},
  year = {2017}
}
</pre></div>
<div id="bDubey:2017wo" style="display:none"><pre>Agile MAVs are required to operate in cluttered, unstructured environments at high speeds and low altitudes for efficient data gathering. Given the payload constraints and long range sensing requirements, cameras are the preferred sensing modality for MAVs. The computation burden of using cameras for obstacle sensing has forced the state of the art methods to construct world representations on a per frame basis, leading to myopic decision making. In this paper we propose a long range perception and planning approach using cameras. By utilizing FPGA hardware for disparity calculation and image space to represent obstacles, our approach and system design allows for construction of long term world representation whilst accounting for highly non-linear noise models in real time. We demonstrate these obstacle avoidance capabilities on a quadrotor flying through dense foliage at speeds of up to 4 m/s for a total of 1.6 hours of autonomous flights. The presented approach enables high speed navigation at low altitudes for MAVs for terrestrial scouting.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chou:2017vd">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution</b>.</div><div class="csl-block csl-author">By Chou, P.W., Maturana, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>34th International Conference on Machine Learning, ICML 2017</i>, Sydneyvol. 2, , pp. 1386–1396, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleChou2017vd()">bibtex</button>

<script>
    function toggleChou2017vd() {
        var x= document.getElementById('aChou:2017vd');
        // console.log("haha %o",typeof Chou:2017vd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chou2017vd()">abstract</button>


<script>
    function toggle2Chou2017vd() {
        var x= document.getElementById('bChou:2017vd');
        // console.log("haha %o",typeof Chou:2017vd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChou:2017vd" style="display:none"><pre>@inproceedings{Chou:2017vd,
  address = {Sydney},
  author = {Chou, Po Wei and Maturana, Daniel and Scherer, Sebastian},
  booktitle = {34th International Conference on Machine Learning, ICML 2017},
  isbn = {9781510855144},
  month = aug,
  pages = {1386--1396},
  title = {Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution},
  volume = {2},
  year = {2017}
}
</pre></div>
<div id="bChou:2017vd" style="display:none"><pre>Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic policy. In this work, we propose to use the Beta distribution as an alternative and analyze the bias and variance of the policy gradients of both policies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on-and off-policy stochastic methods respectively, on Ope-nAI Gym’s and MuJoCo’s continuous control environments.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Bhardwaj:2017tz">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning Heuristic Search via Imitation</b>.</div><div class="csl-block csl-author">By Bhardwaj, M., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>CoRL</i>2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleBhardwaj2017tz()">bibtex</button>

<script>
    function toggleBhardwaj2017tz() {
        var x= document.getElementById('aBhardwaj:2017tz');
        // console.log("haha %o",typeof Bhardwaj:2017tz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Bhardwaj2017tz()">abstract</button>


<script>
    function toggle2Bhardwaj2017tz() {
        var x= document.getElementById('bBhardwaj:2017tz');
        // console.log("haha %o",typeof Bhardwaj:2017tz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1707.03034"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aBhardwaj:2017tz" style="display:none"><pre>@inproceedings{Bhardwaj:2017tz,
  archiveprefix = {arXiv},
  arxivid = {1707.03034},
  author = {Bhardwaj, Mohak and Choudhury, Sanjiban and Scherer, Sebastian},
  eprint = {1707.03034},
  booktitle = {CoRL},
  title = {Learning Heuristic Search via Imitation},
  url = {http://arxiv.org/abs/1707.03034},
  year = {2017}
}
</pre></div>
<div id="bBhardwaj:2017tz" style="display:none"><pre>Robotic motion planning problems are typically solved by constructing a search tree of valid maneuvers from a start to a goal configuration. Limited onboard computation and real-time planning constraints impose a limit on how large this search tree can grow. Heuristics play a crucial role in such situations by guiding the search towards potentially good directions and consequently minimizing search effort. Moreover, it must infer such directions in an efficient manner using only the information uncovered by the search up until that time. However, state of the art methods do not address the problem of computing a heuristic that explicitly minimizes search effort. In this paper, we do so by training a heuristic policy that maps the partial information from the search to decide which node of the search tree to expand. Unfortunately, naively training such policies leads to slow convergence and poor local minima. We present SaIL, an efficient algorithm that trains heuristic policies by imitating "clairvoyant oracles" - oracles that have full information about the world and demonstrate decisions that minimize search effort. We leverage the fact that such oracles can be efficiently computed using dynamic programming and derive performance guarantees for the learnt heuristic. We validate the approach on a spectrum of environments which show that SaIL consistently outperforms state of the art algorithms. Our approach paves the way forward for learning heuristics that demonstrate an anytime nature - finding feasible solutions quickly and incrementally refining it over time.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Maturana:2017kd">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Looking forward: A semantic mapping system for scouting with micro-aerial vehicles</b>.</div><div class="csl-block csl-author">By Maturana, D., Arora, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, pp. 6691–6698, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleMaturana2017kd()">bibtex</button>

<script>
    function toggleMaturana2017kd() {
        var x= document.getElementById('aMaturana:2017kd');
        // console.log("haha %o",typeof Maturana:2017kd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Maturana2017kd()">abstract</button>


<script>
    function toggle2Maturana2017kd() {
        var x= document.getElementById('bMaturana:2017kd');
        // console.log("haha %o",typeof Maturana:2017kd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2017.8206585"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMaturana:2017kd" style="display:none"><pre>@inproceedings{Maturana:2017kd,
  author = {Maturana, Daniel and Arora, Sankalp and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2017.8206585},
  isbn = {9781538626825},
  issn = {21530866},
  pages = {6691--6698},
  title = {Looking forward: A semantic mapping system for scouting with micro-aerial vehicles},
  month = sep,
  year = {2017}
}
</pre></div>
<div id="bMaturana:2017kd" style="display:none"><pre>The last decade has seen a massive growth in applications for Micro-Aerial Vehicles (MAVs), due in large part to their versatility for data gathering with cameras, LiDAR and various other sensors. Their ability to quickly go from assessing large spaces from a high vantage points to flying in close to capture high-resolution data makes them invaluable for applications where we are interested in a specific target with an a priori unknown location, e.g. survivors in disaster response scenarios, vehicles in surveillance, animals in wildlife monitoring, etc., a task we will refer to scouting. Our ultimate goal is to enable MAVs to perform autonomous scouting. In this paper, we describe a semantic mapping system designed to support this goal. The system maintains a 2.5D map describing its belief about the location of semantic classes of interest, using forward-looking cameras and state estimation. The map is continuously updated on the fly, using only onboard processing. The system couples a deep learning 2D semantic segmentation algorithm with a novel mapping method to project and aggregate the 2D semantic measurements into a global 2.5D grid map. We train and evaluate our segmentation method on a novel dataset of cars labelled in oblique aerial imagery. We also study the performance of the mapping system in isolation. Finally, we show the integrated system performing a fully autonomous car scouting mission in the field.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2017tka">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Near-optimal edge evaluation in explicit generalized binomial graphs</b>. </div><div class="csl-block csl-author">By Choudhury, S., Javdani, S., Srinivasa, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Advances in Neural Information Processing Systems</i>, pp. 4632–4642, Jun. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2017tka()">bibtex</button>

<script>
    function toggleChoudhury2017tka() {
        var x= document.getElementById('aChoudhury:2017tka');
        // console.log("haha %o",typeof Choudhury:2017tka);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2017tka()">abstract</button>


<script>
    function toggle2Choudhury2017tka() {
        var x= document.getElementById('bChoudhury:2017tka');
        // console.log("haha %o",typeof Choudhury:2017tka);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChoudhury:2017tka" style="display:none"><pre>@article{Choudhury:2017tka,
  author = {Choudhury, Sanjiban and Javdani, Shervin and Srinivasa, Siddhartha and Scherer, Sebastian},
  issn = {10495258},
  journal = {Advances in Neural Information Processing Systems},
  month = jun,
  pages = {4632--4642},
  title = {Near-optimal edge evaluation in explicit generalized binomial graphs},
  year = {2017}
}
</pre></div>
<div id="bChoudhury:2017tka" style="display:none"><pre>Robotic motion-planning problems, such as a UAV flying fast in a partially-known environment or a robot arm moving around cluttered objects, require finding collision-free paths quickly. Typically, this is solved by constructing a graph, where vertices represent robot configurations and edges represent potentially valid movements of the robot between these configurations. The main computational bottlenecks are expensive edge evaluations to check for collisions. State of the art planning methods do not reason about the optimal sequence of edges to evaluate in order to find a collision free path quickly. In this paper, we do so by drawing a novel equivalence between motion planning and the Bayesian active learning paradigm of decision region determination (DRD). Unfortunately, a straight application of existing methods requires computation exponential in the number of edges in a graph. We present BISECT, an efficient and near-optimal algorithm to solve the DRD problem when edges are independent Bernoulli random variables. By leveraging this property, we are able to significantly reduce computational complexity from exponential to linear in the number of edges. We show that BISECT outperforms several state of the art algorithms on a spectrum of planning problems for mobile robots, manipulators, and real flight data collected from a full scale helicopter.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang:2017we">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Obstacle Avoidance through Deep Networks based Intermediate Perception</b>. </div><div class="csl-block csl-author">By Yang, S., Konam, S., Ma, C., Rosenthal, S., Veloso, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Apr. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleYang2017we()">bibtex</button>

<script>
    function toggleYang2017we() {
        var x= document.getElementById('aYang:2017we');
        // console.log("haha %o",typeof Yang:2017we);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yang2017we()">abstract</button>


<script>
    function toggle2Yang2017we() {
        var x= document.getElementById('bYang:2017we');
        // console.log("haha %o",typeof Yang:2017we);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1704.08759"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aYang:2017we" style="display:none"><pre>@article{Yang:2017we,
  archiveprefix = {arXiv},
  arxivid = {1704.08759},
  author = {Yang, Shichao and Konam, Sandeep and Ma, Chen and Rosenthal, Stephanie and Veloso, Manuela and Scherer, Sebastian},
  eprint = {1704.08759},
  journal = {arXiv.org},
  month = apr,
  title = {Obstacle Avoidance through Deep Networks based Intermediate Perception},
  url = {http://arxiv.org/abs/1704.08759},
  year = {2017}
}
</pre></div>
<div id="bYang:2017we" style="display:none"><pre>Obstacle avoidance from monocular images is a challenging problem for robots. Though multi-view structure-from-motion could build 3D maps, it is not robust in textureless environments. Some learning based methods exploit human demonstration to predict a steering command directly from a single image. However, this method is usually biased towards certain tasks or demonstration scenarios and also biased by human understanding. In this paper, we propose a new method to predict a trajectory from images. We train our system on more diverse NYUv2 dataset. The ground truth trajectory is computed from the designed cost functions automatically. The Convolutional Neural Network perception is divided into two stages: first, predict depth map and surface normal from RGB images, which are two important geometric properties related to 3D obstacle representation. Second, predict the trajectory from the depth and normal. Results show that our intermediate perception increases the accuracy by 20% than the direct prediction. Our model generalizes well to other public indoor datasets and is also demonstrated for robot flights in simulation and experiments.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Arora:2017tz">[11]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Randomized algorithm for informative path planning with budget constraints</b>.</div><div class="csl-block csl-author">By Arora, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Singapore, Singapore, pp. 4997–5004, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2017tz()">bibtex</button>

<script>
    function toggleArora2017tz() {
        var x= document.getElementById('aArora:2017tz');
        // console.log("haha %o",typeof Arora:2017tz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Arora2017tz()">abstract</button>


<script>
    function toggle2Arora2017tz() {
        var x= document.getElementById('bArora:2017tz');
        // console.log("haha %o",typeof Arora:2017tz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2017.7989582"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aArora:2017tz" style="display:none"><pre>@inproceedings{Arora:2017tz,
  address = {Singapore, Singapore},
  author = {Arora, Sankalp and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2017.7989582},
  isbn = {9781509046331},
  issn = {10504729},
  month = may,
  pages = {4997--5004},
  title = {Randomized algorithm for informative path planning with budget constraints},
  year = {2017}
}
</pre></div>
<div id="bArora:2017tz" style="display:none"><pre>Maximizing information gathered within a budget is a relevant problem for information gathering tasks for robots with cost or operating time constraints. This problem is also known as the informative path planning (IPP) problem or correlated orienteering. It can be formalized as that of finding budgeted routes in a graph such that the reward collected by the route is maximized, where the reward at nodes can be dependent. Unfortunately, the problem is NP-Hard and the state of the art methods are too slow to even present an approximate solution online. Here we present Randomized Anytime Orienteering (RAOr) algorithm that provides near optimal solutions while demonstrably converging to an efficient solution in runtimes that allows the solver to be run online. The key idea of our approach is to pose orienteering as a combination of a Constraint Satisfaction Problem and a Traveling Salesman Problem. This formulation allows us to restrict the search space to routes that incur minimum distance to visit a set of selected nodes, and rapidly search this space using random sampling. The paper provides the analysis of asymptotic near-optimality, convergence rates for RAOr algorithms, and present strategies to improve anytime performance of the algorithm. Our experimental results suggest an improvement by an order of magnitude over the state of the art methods in relevant simulation and in real world scenarios.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Fang:2017ho">[12]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Robust Autonomous Flight in Constrained and Visually Degraded Shipboard Environments</b>. </div><div class="csl-block csl-author">By Fang, Z., Yang, S., Jain, S., Dubey, G., Roth, S., Maeta, S., Nuske, S., Zhang, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>Journal of Field Robotics</i>, vol. 34, no. 1, pp. 25–52, Jan. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleFang2017ho()">bibtex</button>

<script>
    function toggleFang2017ho() {
        var x= document.getElementById('aFang:2017ho');
        // console.log("haha %o",typeof Fang:2017ho);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Fang2017ho()">abstract</button>


<script>
    function toggle2Fang2017ho() {
        var x= document.getElementById('bFang:2017ho');
        // console.log("haha %o",typeof Fang:2017ho);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1002/rob.21670"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aFang:2017ho" style="display:none"><pre>@article{Fang:2017ho,
  author = {Fang, Zheng and Yang, Shichao and Jain, Sezal and Dubey, Geetesh and Roth, Stephan and Maeta, Silvio and Nuske, Stephen and Zhang, Yu and Scherer, Sebastian},
  doi = {10.1002/rob.21670},
  issn = {15564967},
  journal = {Journal of Field Robotics},
  month = jan,
  number = {1},
  pages = {25--52},
  title = {Robust Autonomous Flight in Constrained and Visually Degraded Shipboard Environments},
  volume = {34},
  year = {2017}
}
</pre></div>
<div id="bFang:2017ho" style="display:none"><pre>This paper addresses the problem of autonomous navigation of a micro aerial vehicle (MAV) for inspection and damage assessment inside a constrained shipboard environment, which might be perilous or inaccessible for humans, especially in emergency scenarios. The environment is GPS-denied and visually degraded, containing narrow passageways, doorways, and small objects protruding from the wall. This causes existing two-dimensional LIDAR, vision, or mechanical bumper-based autonomous navigation solutions to fail. To realize autonomous navigation in such challenging environments, we first propose a robust state estimation method that fuses estimates from a real-time odometry estimation algorithm and a particle filtering localization algorithm with other sensor information in a two-layer fusion framework. Then, an online motion-planning algorithm that combines trajectory optimization with a receding horizon control framework is proposed for fast obstacle avoidance. All the computations are done in real time on the onboard computer. We validate the system by running experiments under different environmental conditions in both laboratory and practical shipboard environments. The field experiment results of over 10 runs show that our vehicle can robustly navigate 20-m-long and only 1-m-wide corridors and go through a very narrow doorway (66-cm width, only 4-cm clearance on each side) autonomously even when it is completely dark or full of light smoke. These experiments show that despite the challenges associated with flying robustly in challenging shipboard environments, it is possible to use a MAV to autonomously fly into a confined shipboard environment to rapidly gather situational information to guide firefighting and rescue efforts.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Zhen:2017uf">[13]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Robust localization and localizability estimation with a rotating laser scanner</b>.</div><div class="csl-block csl-author">By Zhen, W., Zeng, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Singapore, Singapore, pp. 6240–6245, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleZhen2017uf()">bibtex</button>

<script>
    function toggleZhen2017uf() {
        var x= document.getElementById('aZhen:2017uf');
        // console.log("haha %o",typeof Zhen:2017uf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Zhen2017uf()">abstract</button>


<script>
    function toggle2Zhen2017uf() {
        var x= document.getElementById('bZhen:2017uf');
        // console.log("haha %o",typeof Zhen:2017uf);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2017.7989739"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aZhen:2017uf" style="display:none"><pre>@inproceedings{Zhen:2017uf,
  address = {Singapore, Singapore},
  author = {Zhen, Weikun and Zeng, Sam and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2017.7989739},
  isbn = {9781509046331},
  issn = {10504729},
  month = may,
  pages = {6240--6245},
  title = {Robust localization and localizability estimation with a rotating laser scanner},
  year = {2017}
}
</pre></div>
<div id="bZhen:2017uf" style="display:none"><pre>This paper presents a robust localization approach that fuses measurements from inertial measurement unit (IMU) and a rotating laser scanner. An Error State Kalman Filter (ESKF) is used for sensor fusion and is combined with a Gaussian Particle Filter (GPF) for measurements update. We experimentally demonstrated the robustness of this implementation in various challenging situations such as kidnapped robot situation, laser range reduction and various environment scales and characteristics. Additionally, we propose a new method to evaluate localizability of a given 3D map and show that the computed localizability can precisely predict localization errors, thus helps to find safe routes during flight.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Shah:2017ut">[14]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Robust Localization of an Arbitrary Distribution of Radioactive Sources for Aerial Inspection</b>. </div><div class="csl-block csl-author">By Shah, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Oct. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleShah2017ut()">bibtex</button>

<script>
    function toggleShah2017ut() {
        var x= document.getElementById('aShah:2017ut');
        // console.log("haha %o",typeof Shah:2017ut);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Shah2017ut()">abstract</button>


<script>
    function toggle2Shah2017ut() {
        var x= document.getElementById('bShah:2017ut');
        // console.log("haha %o",typeof Shah:2017ut);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://arxiv.org/abs/1710.01701"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aShah:2017ut" style="display:none"><pre>@article{Shah:2017ut,
  annote = {15 pages, 10 figures. Accepted for presentation in Waste Management Symposium 2018},
  archiveprefix = {arXiv},
  arxivid = {1710.01701},
  author = {Shah, Dhruv and Scherer, Sebastian},
  eprint = {1710.01701},
  journal = {arXiv.org},
  month = oct,
  title = {Robust Localization of an Arbitrary Distribution of Radioactive Sources for Aerial Inspection},
  url = {http://arxiv.org/abs/1710.01701},
  year = {2017}
}
</pre></div>
<div id="bShah:2017ut" style="display:none"><pre>Radiation source detection has seen various applications in the past decade, ranging from the detection of dirty bombs in public places to scanning critical nuclear facilities for leakage or flaws, and in the autonomous inspection of nuclear sites. Despite the success in detecting single point sources or a small number of spatially separated point sources, most of the existing algorithms fail to localize sources in complex scenarios with a large number of point sources or non-trivial distributions &amp; bulk sources. Even in simpler environments, most existing algorithms are not scalable to larger regions and/or higher dimensional spaces. For effective autonomous inspection, we not only need to estimate the positions of the sources, but also the number, distribution, and intensities of each of them. In this paper, we present a novel algorithm for the robust localization of an arbitrary distribution of radiation sources using multi-layer sequential Monte Carlo methods coupled with suitable clustering algorithms. We achieve near-perfect accuracy, in terms of F1-scores (\textgreater 0.95), while allowing the algorithm to scale, both to large regions in space and to higher dimensional spaces (5 tested).</pre></div>
</li>
<li><div class="text-justify">
    <span id="ShichaoYang:2017ufa">[15]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Semantic 3D occupancy mapping through efficient high order CRFs</b>.</div><div class="csl-block csl-author">By Yang, S., Huang, Y. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Vancouver, pp. 590–597, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleShichaoYang2017ufa()">bibtex</button>

<script>
    function toggleShichaoYang2017ufa() {
        var x= document.getElementById('aShichaoYang:2017ufa');
        // console.log("haha %o",typeof ShichaoYang:2017ufa);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2ShichaoYang2017ufa()">abstract</button>


<script>
    function toggle2ShichaoYang2017ufa() {
        var x= document.getElementById('bShichaoYang:2017ufa');
        // console.log("haha %o",typeof ShichaoYang:2017ufa);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2017.8202212"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aShichaoYang:2017ufa" style="display:none"><pre>@inproceedings{ShichaoYang:2017ufa,
  address = {Vancouver},
  archiveprefix = {arXiv},
  arxivid = {1707.07388},
  author = {Yang, Shichao and Huang, Yulan and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2017.8202212},
  eprint = {1707.07388},
  isbn = {9781538626825},
  issn = {21530866},
  month = sep,
  pages = {590--597},
  title = {Semantic {3D} occupancy mapping through efficient high order {CRFs}},
  year = {2017}
}
</pre></div>
<div id="bShichaoYang:2017ufa" style="display:none"><pre>Semantic 3D mapping can be used for many applications such as robot navigation and virtual interaction. In recent years, there has been great progress in semantic segmentation and geometric 3D mapping. However, it is still challenging to combine these two tasks for accurate and large-scale semantic mapping from images. In the paper, we propose an incremental and (near) real-time semantic mapping system. A 3D scrolling occupancy grid map is built to represent the world, which is memory and computationally efficient and bounded for large scale environments. We utilize the CNN segmentation as prior prediction and further optimize 3D grid labels through a novel CRF model. Superpixels are utilized to enforce smoothness and form robust PN high order potential. An efficient mean field inference is developed for the graph optimization. We evaluate our system on the KITTI dataset and improve the segmentation accuracy by 10% over existing systems.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dugar:2017un">[16]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Smooth trajectory optimization in Wind: First results on a full-scale helicopter</b>.</div><div class="csl-block csl-author">By Dugar, V., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Annual Forum Proceedings - AHS International</i>, Fort Worth, TX, pp. 2924–2932, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleDugar2017un()">bibtex</button>

<script>
    function toggleDugar2017un() {
        var x= document.getElementById('aDugar:2017un');
        // console.log("haha %o",typeof Dugar:2017un);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dugar2017un()">abstract</button>


<script>
    function toggle2Dugar2017un() {
        var x= document.getElementById('bDugar:2017un');
        // console.log("haha %o",typeof Dugar:2017un);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aDugar:2017un" style="display:none"><pre>@inproceedings{Dugar:2017un,
  address = {Fort Worth, TX},
  author = {Dugar, Vishal and Choudhury, Sanjiban and Scherer, Sebastian},
  booktitle = {Annual Forum Proceedings - AHS International},
  issn = {15522938},
  month = may,
  pages = {2924--2932},
  title = {Smooth trajectory optimization in Wind: {First} results on a full-scale helicopter},
  year = {2017}
}
</pre></div>
<div id="bDugar:2017un" style="display:none"><pre>A significant challenge for unmanned aerial vehicles is flying long distances in the presence of wind. The presence of wind, which acts like a forcing function on the system dynamics, significantly affects control authority and flight times. While there is a large body of work on the individual topics of planning long missions and path planning in wind fields, these methods do not scale to solve the combined problem under real-time constraints. In this paper, we address the problem of planning long, dynamically feasible, time-optimal trajectories in the presence of wind for a full-scale helicopter. We build on our existing algorithm, kITE, which accounts for wind in a principled and elegant way, and produces dynamically-feasible trajectories that are guaranteed to be safe in near real-time. It uses a novel framework to decouple path optimization in a fixed ground frame from velocity optimization in a moving air frame. We present extensive experimental evaluation of kITE on an autonomous helicopter platform (with a human safety pilot in the loop) with data from over 23 missions in winds up to 20m=s and airspeeds up to 50m=s. Our results not only shows the efficacy of the algorithm and its implementation, but also provide insights into failure cases that we encountered. This paves the way forward for autonomous systems to exhibit pilot-like behavior when flying missions in winds aloft.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Madaan:2017wl">[17]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Wire detection using synthetic data and dilated convolutional networks for unmanned aerial vehicles</b>.</div><div class="csl-block csl-author">By Madaan, R., Maturana, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Vancouver, pp. 3487–3494, 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleMadaan2017wl()">bibtex</button>

<script>
    function toggleMadaan2017wl() {
        var x= document.getElementById('aMadaan:2017wl');
        // console.log("haha %o",typeof Madaan:2017wl);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Madaan2017wl()">abstract</button>


<script>
    function toggle2Madaan2017wl() {
        var x= document.getElementById('bMadaan:2017wl');
        // console.log("haha %o",typeof Madaan:2017wl);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2017.8206190"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMadaan:2017wl" style="display:none"><pre>@inproceedings{Madaan:2017wl,
  address = {Vancouver},
  author = {Madaan, Ratnesh and Maturana, Daniel and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2017.8206190},
  isbn = {9781538626825},
  issn = {21530866},
  month = sep,
  pages = {3487--3494},
  title = {Wire detection using synthetic data and dilated convolutional networks for unmanned aerial vehicles},
  year = {2017}
}
</pre></div>
<div id="bMadaan:2017wl" style="display:none"><pre>Wire detection is a key capability for safe navigation of autonomous aerial vehicles and is a challenging problem as wires are generally only a few pixels wide, can appear at any orientation and location, and are hard to distinguish from other similar looking lines and edges. We leverage the recent advances in deep learning by treating wire detection as a semantic segmentation task, and investigate the effectiveness of convolutional neural networks for the same. To find an optimal model in terms of detection accuracy and real time performance on a portable GPU, we perform a grid search over a finite space of architectures. Further, to combat the issue of unavailability of a large public dataset with annotations, we render synthetic wires using a ray tracing engine, and overlay them on 67K images from flight videos available on the internet. We use this synthetic dataset for pretraining our models before finetuning on real data, and show that synthetic data alone can lead to pretty accurate detections qualitatively as well. We also verify if providing explicit information about local evidence of wiry-ness in the form of edge and line detection results from a traditional computer vision method, as additional channels to the network input, makes the task easier or not. We evaluate our best models from the grid search on a publicly available dataset and show that they outperform previous work using traditional computer vision and various deep net baselines of FCNs, SegNet and E-Net, on both standard edge detection metrics and inference speed. Our top models run at more than 3Hz on the NVIDIA Jetson TX2 with input resolution of 480×640, with an Average Precision score of 0.73 on our test split of the USF dataset.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang2017">[18]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Direct Monocular Odometry Using Points and Lines</b>. </div><div class="csl-block csl-author">By Yang, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Mar. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleYang2017()">bibtex</button>

<script>
    function toggleYang2017() {
        var x= document.getElementById('aYang2017');
        // console.log("haha %o",typeof Yang2017);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Yang2017() {
        var x= document.getElementById('bYang2017');
        // console.log("haha %o",typeof Yang2017);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aYang2017" style="display:none"><pre>@article{Yang2017,
  author = {Yang, Shichao and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {Direct Monocular Odometry Using Points and Lines},
  year = {2017},
  month = mar,
  annote = {ICRA 2017},
  archiveprefix = {arxiv},
  eprint = {1703.06380v1},
  primaryclass = {cs.CV}
}
</pre></div>
<div id="bYang2017" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury2017a">[19]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Bayesian Active Edge Evaluation on Expensive Graphs</b>. </div><div class="csl-block csl-author">By Choudhury, S., Srinivasa, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Nov. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2017a()">bibtex</button>

<script>
    function toggleChoudhury2017a() {
        var x= document.getElementById('aChoudhury2017a');
        // console.log("haha %o",typeof Choudhury2017a);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Choudhury2017a() {
        var x= document.getElementById('bChoudhury2017a');
        // console.log("haha %o",typeof Choudhury2017a);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChoudhury2017a" style="display:none"><pre>@article{Choudhury2017a,
  author = {Choudhury, Sanjiban and Srinivasa, Siddhartha and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {Bayesian Active Edge Evaluation on Expensive Graphs},
  year = {2017},
  month = nov,
  archiveprefix = {arxiv},
  eprint = {1711.07329v1},
  primaryclass = {cs.RO}
}
</pre></div>
<div id="bChoudhury2017a" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury2017b">[20]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs</b>. </div><div class="csl-block csl-author">By Choudhury, S., Javdani, S., Srinivasa, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Jun. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2017b()">bibtex</button>

<script>
    function toggleChoudhury2017b() {
        var x= document.getElementById('aChoudhury2017b');
        // console.log("haha %o",typeof Choudhury2017b);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Choudhury2017b() {
        var x= document.getElementById('bChoudhury2017b');
        // console.log("haha %o",typeof Choudhury2017b);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChoudhury2017b" style="display:none"><pre>@article{Choudhury2017b,
  author = {Choudhury, Sanjiban and Javdani, Shervin and Srinivasa, Siddhartha and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs},
  year = {2017},
  month = jun,
  archiveprefix = {arxiv},
  eprint = {1706.09351v1},
  primaryclass = {cs.RO}
}
</pre></div>
<div id="bChoudhury2017b" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Bhardwaj2017">[21]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning Heuristic Search via Imitation</b>. </div><div class="csl-block csl-author">By Bhardwaj, M., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>arXiv.org</i>, Jul. 2017.</div></div></span>
    <br />
<button class="button0" onclick="toggleBhardwaj2017()">bibtex</button>

<script>
    function toggleBhardwaj2017() {
        var x= document.getElementById('aBhardwaj2017');
        // console.log("haha %o",typeof Bhardwaj2017);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Bhardwaj2017() {
        var x= document.getElementById('bBhardwaj2017');
        // console.log("haha %o",typeof Bhardwaj2017);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aBhardwaj2017" style="display:none"><pre>@article{Bhardwaj2017,
  author = {Bhardwaj, Mohak and Choudhury, Sanjiban and Scherer, Sebastian},
  journal = {arXiv.org},
  title = {Learning Heuristic Search via Imitation},
  year = {2017},
  month = jul,
  annote = {14 pages},
  archiveprefix = {arxiv},
  eprint = {1707.03034v1},
  primaryclass = {cs.RO}
}
</pre></div>
<div id="bBhardwaj2017" style="display:none"><pre></pre></div>
</li></ul.no-bullet>

<h1 id="2016">2016</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Pereira:2016jd">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A framework for optimal repairing of vector field-based motion plans</b>.</div><div class="csl-block csl-author">By Pereira, G.A.S., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>2016 International Conference on Unmanned Aircraft Systems, ICUAS 2016</i>, Washington, D.C., pp. 261–266, 2016.</div></div></span>
    <br />
<button class="button0" onclick="togglePereira2016jd()">bibtex</button>

<script>
    function togglePereira2016jd() {
        var x= document.getElementById('aPereira:2016jd');
        // console.log("haha %o",typeof Pereira:2016jd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Pereira2016jd()">abstract</button>


<script>
    function toggle2Pereira2016jd() {
        var x= document.getElementById('bPereira:2016jd');
        // console.log("haha %o",typeof Pereira:2016jd);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICUAS.2016.7502525"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aPereira:2016jd" style="display:none"><pre>@inproceedings{Pereira:2016jd,
  address = {Washington, D.C.},
  author = {Pereira, Guilherme A.S. and Choudhury, Sanjiban and Scherer, Sebastian},
  booktitle = {2016 International Conference on Unmanned Aircraft Systems, ICUAS 2016},
  doi = {10.1109/ICUAS.2016.7502525},
  isbn = {9781467393331},
  pages = {261--266},
  title = {A framework for optimal repairing of vector field-based motion plans},
  year = {2016}
}
</pre></div>
<div id="bPereira:2016jd" style="display:none"><pre>This paper presents a framework that integrates vector field based motion planning techniques with an optimal path planner. The main motivation for this integration is the solution of UAVs’ motion planning problems that are easily and intuitively solved using vector fields, but are very difficult to be even posed as optimal motion planning problems, mainly due to the lack of clear cost functions. Examples of such problems include the ones where a goal configuration is not defined, such as circulation of curves, loitering and road following. While several vector field methodologies were proposed to solve these tasks, they are susceptible to failures in the presence of previously unmodeled obstacles, including no-fly zones specified during the flight. Our framework uses a vector field as a high level specification of a task and an optimal motion planner (in our case RRT - ) as a local, on-line planner that generates paths that follow the vector field, but also consider the new obstacles encountered by the vehicle during the flight. A series of simulations illustrate and validate the proposed methodology. One of these simulations considers a rotorcraft UAV equipped with a spinning laser patrolling an urban area in the presence of unmodeled obstacles and no-fly zones.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2016uz">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Constrained CHOMP using Dual Projected Newton Method</b>.</div><div class="csl-block csl-author">By Choudhury, S. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-16-17, Apr-2016</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2016uz()">bibtex</button>

<script>
    function toggleChoudhury2016uz() {
        var x= document.getElementById('aChoudhury:2016uz');
        // console.log("haha %o",typeof Choudhury:2016uz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Choudhury2016uz() {
        var x= document.getElementById('bChoudhury:2016uz');
        // console.log("haha %o",typeof Choudhury:2016uz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.ri.cmu.edu/pub_files/2016/5/main-choudhury.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aChoudhury:2016uz" style="display:none"><pre>@techreport{Choudhury:2016uz,
  abstraxt = {CHOMP is a popular trajectory optimization algorithm that uses covari- ant gradient techniques to produce high quality solutions. In its original formulation, it solves an unconstrained sequentially quadratic problem with extensions for handling equality constraints. In this paper we present an approach to solve sequentially quadratic problem with linear inequality con- straints. We present a dual projected newton method to efficiently solve this problem. The proposed method alternates between primal and dual up- dates thus leading to faster convergence than solving a constrained quadratic program at each iteration.},
  address = {Pittsburgh, PA},
  author = {Choudhury, Sanjiban and Scherer, Sebastian},
  institution = {Carnegie Mellon University},
  title = {Constrained {CHOMP} using Dual Projected {Newton} Method},
  number = {CMU-RI-TR-16-17},
  month = apr,
  year = {2016},
  url = {https://www.ri.cmu.edu/pub_files/2016/5/main-choudhury.pdf}
}
</pre></div>
<div id="bChoudhury:2016uz" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Chakraborty:2016tx">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Detecting cars in aerial photographs with a hierarchy of deconvolution nets</b>.</div><div class="csl-block csl-author">By Chakraborty, S., Maturana, D. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-16-60, 2016</div></div></span>
    <br />
<button class="button0" onclick="toggleChakraborty2016tx()">bibtex</button>

<script>
    function toggleChakraborty2016tx() {
        var x= document.getElementById('aChakraborty:2016tx');
        // console.log("haha %o",typeof Chakraborty:2016tx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chakraborty2016tx()">abstract</button>


<script>
    function toggle2Chakraborty2016tx() {
        var x= document.getElementById('bChakraborty:2016tx');
        // console.log("haha %o",typeof Chakraborty:2016tx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChakraborty:2016tx" style="display:none"><pre>@techreport{Chakraborty:2016tx,
  address = {Pittsburgh, PA},
  author = {Chakraborty, Satyaki and Maturana, Daniel and Scherer, Sebastian},
  institution = {Carnegie Mellon University},
  keywords = {Deconvolution nets,Neural networks,—Object detection},
  number = {CMU-RI-TR-16-60},
  title = {Detecting cars in aerial photographs with a hierarchy of deconvolution nets},
  year = {2016}
}
</pre></div>
<div id="bChakraborty:2016tx" style="display:none"><pre>—Detecting cars in large aerial photographs can be quite a challenging task, given that cars in such datasets are often barely visible to the naked human eye. Traditional object detection algorithms fail to perform well when it comes to detecting cars under such circumstances. One would rather use context or exploit spatial relationship between different entities in the scene to narrow down the search space. We aim to do so by looking at different resolutions of the image to process context and focus on promising areas. This is done using a hierarchy of deconvolution networks with each level of the hierarchy trying to predict a heatmap of a certain resolution. We show that our architecture is able to model context implicitly and use it for finer prediction and faster search.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Pereira:2016vx">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Kinodynamic Motion Planning on Vector Fields using RRT *</b>.</div><div class="csl-block csl-author">By Pereira, G.A.S., Choudhury, S. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-16-17, 2016</div></div></span>
    <br />
<button class="button0" onclick="togglePereira2016vx()">bibtex</button>

<script>
    function togglePereira2016vx() {
        var x= document.getElementById('aPereira:2016vx');
        // console.log("haha %o",typeof Pereira:2016vx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Pereira2016vx()">abstract</button>


<script>
    function toggle2Pereira2016vx() {
        var x= document.getElementById('bPereira:2016vx');
        // console.log("haha %o",typeof Pereira:2016vx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aPereira:2016vx" style="display:none"><pre>@techreport{Pereira:2016vx,
  address = {Pittsburgh, PA},
  author = {Pereira, Guilherme A S and Choudhury, Sanjiban and Scherer, Sebastian},
  institution = {Carnegie Mellon University},
  keywords = {motion planning,navigation,optimal planners,rrt,vector fields},
  number = {CMU-RI-TR-16-17},
  title = {Kinodynamic Motion Planning on Vector Fields using {RRT *}},
  year = {2016}
}
</pre></div>
<div id="bPereira:2016vx" style="display:none"><pre>This report presents a methodology to integrate vector field based motion planning tech-niques with optimal, differential constrained trajectory planners. The main motivation for this integration is the solution of robot motion planning problems that are easily and intuitively solved using vector fields, but are very difficult to be even posed as an optimal motion planning problem, mainly due to the lack of a clear cost function. Examples of such problems include the ones where a goal configuration is not defined, such as circulation of curves, loitering, road following, etc. While several vector field methodologies were proposed to solve these tasks, they do not explicitly consider the robot’s differential constraints and are susceptible to failures in the presence of previously unmodeled obstacles. To add together the good characteristics of each approach, our methodology uses a vector field as a high level specification of a task and an optimal motion planner (in our case RRT*) as a local planner that generates trajectories that follow the vector field, but also consider the kinematics and the dynamics of the robot, as well as the new obstacles encountered by the robot in its workspace.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Tallavajhula:2016bs">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>List prediction applied to motion planning</b>.</div><div class="csl-block csl-author">By Tallavajhula, A., Choudhury, S., Scherer, S. and Kelly, A.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Stockholm, Sweden, pp. 213–220, 2016.</div></div></span>
    <br />
<button class="button0" onclick="toggleTallavajhula2016bs()">bibtex</button>

<script>
    function toggleTallavajhula2016bs() {
        var x= document.getElementById('aTallavajhula:2016bs');
        // console.log("haha %o",typeof Tallavajhula:2016bs);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Tallavajhula2016bs()">abstract</button>


<script>
    function toggle2Tallavajhula2016bs() {
        var x= document.getElementById('bTallavajhula:2016bs');
        // console.log("haha %o",typeof Tallavajhula:2016bs);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2016.7487136"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aTallavajhula:2016bs" style="display:none"><pre>@inproceedings{Tallavajhula:2016bs,
  address = {Stockholm, Sweden},
  author = {Tallavajhula, Abhijeet and Choudhury, Sanjiban and Scherer, Sebastian and Kelly, Alonzo},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2016.7487136},
  isbn = {9781467380263},
  issn = {10504729},
  pages = {213--220},
  month = jun,
  title = {List prediction applied to motion planning},
  year = {2016}
}
</pre></div>
<div id="bTallavajhula:2016bs" style="display:none"><pre>There is growing interest in applying machine learning to motion planning. Potential applications are predicting an initial seed for trajectory optimization, predicting an effective heuristic for search based planning, and even predicting a planning algorithm for adaptive motion planning systems. In these situations, providing only a single prediction is unsatisfactory. It leads to many scenarios where the prediction suffers a high loss. In this paper, we advocate list prediction. Each predictor in a list focusses on different regions in the space of environments. This overcomes the shortcoming of a single predictor, and improves overall performance. A framework for list prediction, ConseqOpt, already exists. Our contribution is an extensive domain-specific treatment. We provide a rigorous and clear exposition of the procedure for training a list of predictors. We provide experimental results on a spectrum of motion planning applications. Each application contributes to understanding the behavior of list prediction. We observe that the benefit of list prediction over a single prediction is significant, irrespective of the application.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Lee:2016tl">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Modeling and Control of Coaxial UAV with Swashplate Controlled Lower Propeller</b>.</div><div class="csl-block csl-author">By Lee, R., Sreenath, K. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-16-33, Jun-2016</div></div></span>
    <br />
<button class="button0" onclick="toggleLee2016tl()">bibtex</button>

<script>
    function toggleLee2016tl() {
        var x= document.getElementById('aLee:2016tl');
        // console.log("haha %o",typeof Lee:2016tl);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Lee2016tl()">abstract</button>


<script>
    function toggle2Lee2016tl() {
        var x= document.getElementById('bLee:2016tl');
        // console.log("haha %o",typeof Lee:2016tl);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://www.andrew.cmu.edu/user/rl1/lee-coax-technical-report.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aLee:2016tl" style="display:none"><pre>@techreport{Lee:2016tl,
  address = {Pittsburgh, PA},
  author = {Lee, Richard and Sreenath, Koushil and Scherer, Sebastian},
  institution = {Carnegie Mellon University},
  number = {CMU-RI-TR-16-33},
  title = {Modeling and Control of Coaxial {UAV} with Swashplate Controlled Lower Propeller},
  month = jun,
  year = {2016},
  url = {http://www.andrew.cmu.edu/user/rl1/lee-coax-technical-report.pdf}
}
</pre></div>
<div id="bLee:2016tl" style="display:none"><pre>There is a growing interest in the design and control of coaxial vehicles for the purposes of autonomous flight. These vehicles utilize two, contra-rotating propellers for generating thrust and swashplates for generating pitch and roll. In this report, we present a novel coaxial design in which both upper and lower rotors are contained within a ducted fan, the speeds of both rotors are inde-pendently controlled, and the lower rotor’s cyclic pitch is controlled through a swashplate. Based on this design, a simple dynamic model was developed with unique force and moment generation equations. Given this model, we are able to map desired force and moment values to the control inputs capable of producing them. Afterwards, position and attitude control were implemented over this nonlinear dynamic model in simulation, such that the vehicle was able to recover from poor initial conditions and follow desired trajectories. As demonstrated by the examples presented in this report, position control results in simulations with low max percent overshoot and reasonable settling times. These results prove promising for the implementation of position and attitude control on our physical system.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Pereira:wi">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Nonholonomic Motion Planning in Partially Unknown Environments Using Vector Fields and Optimal Planners</b>.</div><div class="csl-block csl-author">By Pereira, G.A.S., Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Congresso Brasileiro de Automatica (CBA)</i>, Vitoria, Brazil2016.</div></div></span>
    <br />
<button class="button0" onclick="togglePereirawi()">bibtex</button>

<script>
    function togglePereirawi() {
        var x= document.getElementById('aPereira:wi');
        // console.log("haha %o",typeof Pereira:wi);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Pereirawi()">abstract</button>


<script>
    function toggle2Pereirawi() {
        var x= document.getElementById('bPereira:wi');
        // console.log("haha %o",typeof Pereira:wi);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://pdfs.semanticscholar.org/28ac/1260c50c9812711914a58ce687e00803cd13.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aPereira:wi" style="display:none"><pre>@inproceedings{Pereira:wi,
  address = {Vitoria, Brazil},
  author = {Pereira, Guilherme A S and Choudhury, Sanjiban and Scherer, Sebastian},
  booktitle = {Congresso Brasileiro de Automatica (CBA)},
  keywords = {este artigo apresenta uma,metodologia para integra,motion planning,optimal planning,resumo,robotics,rrt,vector fields},
  month = oct,
  title = {Nonholonomic Motion Planning in Partially Unknown Environments Using Vector Fields and Optimal Planners},
  url = {https://pdfs.semanticscholar.org/28ac/1260c50c9812711914a58ce687e00803cd13.pdf},
  year = {2016}
}
</pre></div>
<div id="bPereira:wi" style="display:none"><pre>— This paper presents a methodology to integrate vector field-based robot motion planning tech-niques with optimal trajectory planners. The main motivation for this integration is the solution of planning problems that are intuitively solved using vector fields, but are very difficult to be even posed as an optimal mo-tion planning problem, mainly due to the lack of a clear cost function. Among such problems are the ones where a goal configuration is not defined, such as circulation of curves and road following. While several vector field based methodologies were proposed to solve these tasks, they do not explicitly consider the robot’s differential constraints and are susceptible to failures in the presence of previously unmodeled obstacles. Our methodology uses a vector field as a high level specification of a task and an optimal motion planner (in our case RRT*) as a local planner that generates trajectories that follow the vector field and also consider the kinematic and dynamic constraints of the robot, as well as the new obstacles encountered in the workspace. To illustrate the approach, we show simulations with a Dubins like vehicle moving in partially unknown planar environments. Keywords— Robotics, motion planning, vector fields, optimal planning, RRT*. Resumo— Este artigo apresenta uma metodologia para integração de técnicas de planejamento de movimento para robôs baseadas em campos vetorias e planejadore otimos. A principal motivação para essa integraçãó e a solução de problemas de planejamento que são intuitivamente solucionados usando campos vetoriais, mas são muito difícies de serem modelados como um problema de planejamentó otimo, principalmente devidò a falta de uma função de custo. Entre esses problemas estão aqueles em que um alvo nãó e definido, como circulação de curvas e seguimento de rodovias. Enquanto várias metodologias baseadas em campos vetoriais foram propostas para solucionar essas tarefas, elas não consideram explicitamente as restrições diferenciais dos robôs e estão sujeita a falhas na presença de obstáculos que não foram previamente modelados. A metodologia proposta nesse artigo usa um campo vetorial como uma especificação de alto nível para uma tarefa e um planejado otimo (nesse caso o RRT*) como um planejador local que gera trajetórias que seguem o campo vetorial e também consideram as restrições cinemáticas e dinâmicas do robô, bem como os novos obstáculos encontrados em seu espaço de trabalho. Para ilustrar a metodologia, o artigo apresenta simulações utilizando um robô com cinemática de Dubins se locovendo em um ambiente planar parcialmente conhecido. Palavras-chave— Robótica, planejamento de movimento, campos vetorias, planejamentó otimo, RRT*.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang:2016hr">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Pop-up SLAM: Semantic monocular plane SLAM for low-texture environments</b>.</div><div class="csl-block csl-author">By Yang, S., Song, Y., Kaess, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Daejeon, Korea, pp. 1222–1229, 2016.</div></div></span>
    <br />
<button class="button0" onclick="toggleYang2016hr()">bibtex</button>

<script>
    function toggleYang2016hr() {
        var x= document.getElementById('aYang:2016hr');
        // console.log("haha %o",typeof Yang:2016hr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yang2016hr()">abstract</button>


<script>
    function toggle2Yang2016hr() {
        var x= document.getElementById('bYang:2016hr');
        // console.log("haha %o",typeof Yang:2016hr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2016.7759204"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aYang:2016hr" style="display:none"><pre>@inproceedings{Yang:2016hr,
  address = {Daejeon, Korea},
  archiveprefix = {arXiv},
  arxivid = {1703.07334},
  author = {Yang, Shichao and Song, Yu and Kaess, Michael and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2016.7759204},
  eprint = {1703.07334},
  isbn = {9781509037629},
  issn = {21530866},
  pages = {1222--1229},
  title = {Pop-up {SLAM}: Semantic monocular plane {SLAM} for low-texture environments},
  month = nov,
  year = {2016}
}
</pre></div>
<div id="bYang:2016hr" style="display:none"><pre>Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.</pre></div>
</li>
<li><div class="text-justify">
    <span id="ShichaoYang:2016bg">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Real-time 3D scene layout from a single image using Convolutional Neural Networks</b>.</div><div class="csl-block csl-author">By Yang, S., Maturana, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Stockholm, Sweden, pp. 2183–2189, 2016.</div></div></span>
    <br />
<button class="button0" onclick="toggleShichaoYang2016bg()">bibtex</button>

<script>
    function toggleShichaoYang2016bg() {
        var x= document.getElementById('aShichaoYang:2016bg');
        // console.log("haha %o",typeof ShichaoYang:2016bg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2ShichaoYang2016bg()">abstract</button>


<script>
    function toggle2ShichaoYang2016bg() {
        var x= document.getElementById('bShichaoYang:2016bg');
        // console.log("haha %o",typeof ShichaoYang:2016bg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2016.7487368"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aShichaoYang:2016bg" style="display:none"><pre>@inproceedings{ShichaoYang:2016bg,
  address = {Stockholm, Sweden},
  author = {Yang, Shichao and Maturana, Daniel and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2016.7487368},
  isbn = {9781467380263},
  issn = {10504729},
  pages = {2183--2189},
  title = {Real-time {3D} scene layout from a single image using Convolutional Neural Networks},
  month = jun,
  year = {2016}
}
</pre></div>
<div id="bShichaoYang:2016bg" style="display:none"><pre>We consider the problem of understanding the 3D layout of indoor corridor scenes from a single image in real time. Identifying obstacles such as walls is essential for robot navigation, but also challenging due to the diversity in structure, appearance and illumination of real-world corridor scenes. Many current single-image methods make Manhattan-world assumptions, and break down in environments that do not meet this mold. They also may require complicated hand-designed features for image segmentation or clear boundaries to form certain building models. In addition, most cannot run in real time In this paper, we propose to combine machine learning with geometric modelling to build a simplified 3D model from a single image.We first employ a supervised Convolutional Neural Network (CNN) to provide a dense, but coarse, geometric class labelling of the scene. We then refine this labelling with a fully connected Conditional Random Field (CRF). Finally, we fit line segments along wall-ground boundaries and ?pop up? a 3D model using geometric constraints. We assemble a dataset of 967 labelled corridor images. Our experiments on this dataset and another publicly available dataset show our method outperforms other single image scene understanding methods in pixelwise accuracy while labelling images at over 15 Hz..</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2016ti">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Regionally accelerated batch informed trees (RABIT∗): A framework to integrate local information into optimal path planning</b>.</div><div class="csl-block csl-author">By Choudhury, S., Gammell, J.D., Barfoot, T.D., Srinivasa, S.S.D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Stockholm, Sweden, pp. 4207–4214, 2016.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2016ti()">bibtex</button>

<script>
    function toggleChoudhury2016ti() {
        var x= document.getElementById('aChoudhury:2016ti');
        // console.log("haha %o",typeof Choudhury:2016ti);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2016ti()">abstract</button>


<script>
    function toggle2Choudhury2016ti() {
        var x= document.getElementById('bChoudhury:2016ti');
        // console.log("haha %o",typeof Choudhury:2016ti);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2016.7487615"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2016ti" style="display:none"><pre>@inproceedings{Choudhury:2016ti,
  address = {Stockholm, Sweden},
  author = {Choudhury, Sanjiban and Gammell, Jonathan D. and Barfoot, Timothy D. and Srinivasa, Siddhartha S.D. and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2016.7487615},
  isbn = {9781467380263},
  issn = {10504729},
  pages = {4207--4214},
  title = {Regionally accelerated batch informed trees {(RABIT∗)}: A framework to integrate local information into optimal path planning},
  month = jun,
  year = {2016}
}
</pre></div>
<div id="bChoudhury:2016ti" style="display:none"><pre>Sampling-based optimal planners, such as RRT∗, almost-surely converge asymptotically to the optimal solution, but have provably slow convergence rates in high dimensions. This is because their commitment to finding the global optimum compels them to prioritize exploration of the entire problem domain even as its size grows exponentially. Optimization techniques, such as CHOMP, have fast convergence on these problems but only to local optima. This is because they are exploitative, prioritizing the immediate improvement of a path even though this may not find the global optimum of nonconvex cost functions.</pre></div>
</li></ul.no-bullet>

<h1 id="2015">2015</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Maturana:2015fs">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>3D Convolutional Neural Networks for landing zone detection from LiDAR</b>.</div><div class="csl-block csl-author">By Maturana, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WA, USA, pp. 3471–3478, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleMaturana2015fs()">bibtex</button>

<script>
    function toggleMaturana2015fs() {
        var x= document.getElementById('aMaturana:2015fs');
        // console.log("haha %o",typeof Maturana:2015fs);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Maturana2015fs()">abstract</button>


<script>
    function toggle2Maturana2015fs() {
        var x= document.getElementById('bMaturana:2015fs');
        // console.log("haha %o",typeof Maturana:2015fs);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7139679"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMaturana:2015fs" style="display:none"><pre>@inproceedings{Maturana:2015fs,
  address = {Seattle, WA, USA},
  author = {Maturana, Daniel and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7139679},
  issn = {10504729},
  month = jun,
  pages = {3471--3478},
  title = {{3D} Convolutional Neural Networks for landing zone detection from {LiDAR}},
  year = {2015}
}
</pre></div>
<div id="bMaturana:2015fs" style="display:none"><pre>We present a system for the detection of small and potentially obscured obstacles in vegetated terrain. The key novelty of this system is the coupling of a volumetric occupancy map with a 3D Convolutional Neural Network (CNN), which to the best of our knowledge has not been previously done. This architecture allows us to train an extremely efficient and highly accurate system for detection tasks from raw occupancy data. We apply this method to the problem of detecting safe landing zones for autonomous helicopters from LiDAR point clouds. Current methods for this problem rely on heuristic rules and use simple geometric features. These heuristics break down in the presence of low vegetation, as they do not distinguish between vegetation that may be landed on and solid objects that should be avoided. We evaluate the system with a combination of real and synthetic range data. We show our system outperforms various benchmarks, including a system integrating various hand-crafted point cloud features from the literature.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Nuske:2015dj">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous exploration and motion planning for an unmanned aerial vehicle navigating rivers</b>. </div><div class="csl-block csl-author">By Nuske, S., Choudhury, S., Jain, S., Chambers, A., Yoder, L., Scherer, S., Chamberlain, L., Cover, H. and Singh, S.</div><div class="csl-block csl-event">In <i>Journal of Field Robotics</i>, vol. 32, no. 8, pp. 1141–1162, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleNuske2015dj()">bibtex</button>

<script>
    function toggleNuske2015dj() {
        var x= document.getElementById('aNuske:2015dj');
        // console.log("haha %o",typeof Nuske:2015dj);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Nuske2015dj()">abstract</button>


<script>
    function toggle2Nuske2015dj() {
        var x= document.getElementById('bNuske:2015dj');
        // console.log("haha %o",typeof Nuske:2015dj);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1002/rob.21596"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aNuske:2015dj" style="display:none"><pre>@article{Nuske:2015dj,
  author = {Nuske, Stephen and Choudhury, Sanjiban and Jain, Sezal and Chambers, Andrew and Yoder, Luke and Scherer, Sebastian and Chamberlain, Lyle and Cover, Hugh and Singh, Sanjiv},
  doi = {10.1002/rob.21596},
  issn = {15564967},
  journal = {Journal of Field Robotics},
  number = {8},
  pages = {1141--1162},
  title = {Autonomous exploration and motion planning for an unmanned aerial vehicle navigating rivers},
  volume = {32},
  year = {2015}
}
</pre></div>
<div id="bNuske:2015dj" style="display:none"><pre>Mapping a river’s geometry provides valuable information to help understand the topology and health of an environment and deduce other attributes such as which types of surface vessels could traverse the river. While many rivers can be mapped from satellite imagery, smaller rivers that pass through dense vegetation are occluded. We develop a micro air vehicle (MAV) that operates beneath the tree line, detects and maps the river, and plans paths around three-dimensional (3D) obstacles (such as overhanging tree branches) to navigate rivers purely with onboard sensing, with no GPS and no prior map. We present the two enabling algorithms for exploration and for 3D motion planning. We extract high-level goal-points using a novel exploration algorithm that uses multiple layers of information to maximize the length of the river that is explored during a mission. We also present an efficient modification to the SPARTAN (Sparse Tangential Network) algorithm called SPARTAN-lite, which exploits geodesic properties on smooth manifolds of a tangential surface around obstacles to plan rapidly through free space. Using limited onboard resources, the exploration and planning algorithms together compute trajectories through complex unstructured and unknown terrain, a capability rarely demonstrated by flying vehicles operating over rivers or over ground. We evaluate our approach against commonly employed algorithms and compare guidance decisions made by our system to those made by a human piloting a boat carrying our system over multiple kilometers. We also present fully autonomous flights on riverine environments generating 3D maps over several hundred-meter stretches of tight winding rivers.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Jain:wt">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous river exploration</b>.</div><div class="csl-block csl-author">By Jain, S., Nuske, S., Chambers, A., Yoder, L., Cover, H., Chamberlain, L., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Springer Tracts in Advanced Robotics</i>, Brisbanne, Australiavol. 105, , pp. 93–106, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleJainwt()">bibtex</button>

<script>
    function toggleJainwt() {
        var x= document.getElementById('aJain:wt');
        // console.log("haha %o",typeof Jain:wt);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Jainwt()">abstract</button>


<script>
    function toggle2Jainwt() {
        var x= document.getElementById('bJain:wt');
        // console.log("haha %o",typeof Jain:wt);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-319-07488-7_7"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aJain:wt" style="display:none"><pre>@inproceedings{Jain:wt,
  address = {Brisbanne, Australia},
  author = {Jain, Sezal and Nuske, Stephen and Chambers, Andrew and Yoder, Luke and Cover, Hugh and Chamberlain, Lyle and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {Springer Tracts in Advanced Robotics},
  doi = {10.1007/978-3-319-07488-7_7},
  isbn = {9783319074870},
  issn = {1610742X},
  month = dec,
  pages = {93--106},
  title = {Autonomous river exploration},
  volume = {105},
  year = {2015}
}
</pre></div>
<div id="bJain:wt" style="display:none"><pre>Mapping a rivers course and width provides valuable information to help understand the ecology, topology and health of a particular environment. Such maps can also be useful to determine whether specific surface vessels can traverse the rivers. While rivers can be mapped from satellite imagery, the presence of vegetation, sometimes so thick that the canopy completely occludes the river, complicates the process of mapping. Here we propose the use of a micro air vehicle flying under the canopy to create accurate maps of the environment.We study and present a systemthat can autonomously explore riverswithout any prior information, and demonstrate an algorithm that can guide the vehicle based upon local sensors mounted on board the flying vehicle that can perceive the river, bank and obstacles. Our field experiments demonstrate what we believe is the first autonomous exploration of rivers by an autonomous vehicle. We show the 3D maps produced by our system over runs of 100-450 meters in length and compare guidance decisions made by our system to those made by a human piloting a boat carrying our system over multiple kilometers.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Arora:2015vr">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous Semantic Exploration Using Unmanned Aerial Vehicles</b>.</div><div class="csl-block csl-author">By Arora, S., Dubey, G., Jain, S., Maturana, D., Song, Y., Nuske, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Workshop on Vision-based Control and Navigation of Small Lightweight UAVs, IROS 2015</i>2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2015vr()">bibtex</button>

<script>
    function toggleArora2015vr() {
        var x= document.getElementById('aArora:2015vr');
        // console.log("haha %o",typeof Arora:2015vr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Arora2015vr() {
        var x= document.getElementById('bArora:2015vr');
        // console.log("haha %o",typeof Arora:2015vr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aArora:2015vr" style="display:none"><pre>@inproceedings{Arora:2015vr,
  author = {Arora, S and Dubey, G and Jain, S and Maturana, D and Song, Y and Nuske, S and Scherer, Sebastian},
  booktitle = {Workshop on Vision-based Control and Navigation of Small Lightweight UAVs, IROS 2015},
  month = oct,
  title = {Autonomous Semantic Exploration Using Unmanned Aerial Vehicles},
  year = {2015}
}
</pre></div>
<div id="bArora:2015vr" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Althoff:2015ge">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Connected invariant sets for high-speed motion planning in partially-known environments</b>.</div><div class="csl-block csl-author">By Althoff, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WA, USA, pp. 3279–3285, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleAlthoff2015ge()">bibtex</button>

<script>
    function toggleAlthoff2015ge() {
        var x= document.getElementById('aAlthoff:2015ge');
        // console.log("haha %o",typeof Althoff:2015ge);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Althoff2015ge()">abstract</button>


<script>
    function toggle2Althoff2015ge() {
        var x= document.getElementById('bAlthoff:2015ge');
        // console.log("haha %o",typeof Althoff:2015ge);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7139651"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aAlthoff:2015ge" style="display:none"><pre>@inproceedings{Althoff:2015ge,
  address = {Seattle, WA, USA},
  author = {Althoff, Daniel and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7139651},
  issn = {10504729},
  month = jun,
  pages = {3279--3285},
  title = {Connected invariant sets for high-speed motion planning in partially-known environments},
  year = {2015}
}
</pre></div>
<div id="bAlthoff:2015ge" style="display:none"><pre>Ensuring safety in partially-known environments is a critical problem in robotics since the environment is perceived through sensors and the environment cannot be completely known ahead of time. Prior work has considered the problem of finding positive control invariant sets (PCIS). However, this approach limits the planning horizon of the motion planner since the PCIS must lie completely in the limited known part of the environment. Here we consider the problem of guaranteeing safety by ensuring the existence of at least one PCIS in partially-known environments leading to an extension of the PCIS concept. It is shown, that this novel method is less conservative than the common PCIS approach and robust to unknown small obstacles which might appear in the close vicinity of the robot. An example implementation for loiter circles and power line obstacles is presented. Simulation scenarios are used for validating the proposed concept.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Arora:2015fo">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Emergency maneuver library - Ensuring safe navigation in partially known environments</b>.</div><div class="csl-block csl-author">By Arora, S., Choudhury, S., Althoff, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WA, USA, pp. 6431–6438, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2015fo()">bibtex</button>

<script>
    function toggleArora2015fo() {
        var x= document.getElementById('aArora:2015fo');
        // console.log("haha %o",typeof Arora:2015fo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Arora2015fo()">abstract</button>


<script>
    function toggle2Arora2015fo() {
        var x= document.getElementById('bArora:2015fo');
        // console.log("haha %o",typeof Arora:2015fo);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7140102"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aArora:2015fo" style="display:none"><pre>@inproceedings{Arora:2015fo,
  address = {Seattle, WA, USA},
  author = {Arora, Sankalp and Choudhury, Sanjiban and Althoff, Daniel and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7140102},
  issn = {10504729},
  month = jun,
  pages = {6431--6438},
  title = {Emergency maneuver library - {Ensuring} safe navigation in partially known environments},
  year = {2015}
}
</pre></div>
<div id="bArora:2015fo" style="display:none"><pre>Autonomous mobile robots are required to operate in partially known and unstructured environments. It is imperative to guarantee safety of such systems for their successful deployment. Current state of the art does not fully exploit the sensor and dynamic capabilities of a robot. Also, given the non-holonomic systems with non-linear dynamic constraints, it becomes computationally infeasible to find an optimal solution if the full dynamics are to be exploited online. In this paper we present an online algorithm to guarantee the safety of the robot through an emergency maneuver library. The maneuvers in the emergency maneuver library are optimized such that the probability of finding an emergency maneuver that lies in the known obstacle free space is maximized. We prove that the related trajectory set diversity problem is monotonic and sub-modular which enables one to develop an efficient trajectory set generation algorithm with bounded sub-optimality. We generate an off-line computed trajectory set that exploits the full dynamics of the robot and the known obstacle-free region. We test and validate the algorithm on a full-size autonomous helicopter flying up to speeds of 56\textlesssup\textgreaterm/s\textless/sup\textgreater in partially-known environments. We present results from 4 months of flight testing where the helicopter has been avoiding trees, performing autonomous landing, avoiding mountains while being guaranteed safe.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Yang_2015_7839">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>High-precision Autonomous Flight in Constrained Shipboard Environments</b>.</div><div class="csl-block csl-author">By Yang, S., Fang, Z., Jain, S., Dubey, G., Maeta, S., Roth, S., Scherer, S., Zhang, Y. and Nuske, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-16-17, Feb-2015</div></div></span>
    <br />
<button class="button0" onclick="toggleYang_2015_7839()">bibtex</button>

<script>
    function toggleYang_2015_7839() {
        var x= document.getElementById('aYang_2015_7839');
        // console.log("haha %o",typeof Yang_2015_7839);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Yang_2015_7839()">abstract</button>


<script>
    function toggle2Yang_2015_7839() {
        var x= document.getElementById('bYang_2015_7839');
        // console.log("haha %o",typeof Yang_2015_7839);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.ri.cmu.edu/pub_files/2015/2/shipboard_final_report_20151.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aYang_2015_7839" style="display:none"><pre>@techreport{Yang_2015_7839,
  address = {Pittsburgh, PA},
  author = {Yang, Shichao and Fang, Zheng and Jain, Sezal and Dubey, Geetesh and Maeta, Silvio and Roth, Stephan and Scherer, Sebastian and Zhang, Yu and Nuske, Stephen},
  institution = {Carnegie Mellon University},
  month = feb,
  number = {CMU-RI-TR-16-17},
  pages = {CMU--RI--TR--15--06},
  title = {High-precision Autonomous Flight in Constrained Shipboard Environments},
  url = {https://www.ri.cmu.edu/pub{\_}files/2015/2/shipboard{\_}final{\_}report{\_}20151.pdf},
  year = {2015}
}
</pre></div>
<div id="bYang_2015_7839" style="display:none"><pre>This paper addresses the problem of autonomous navigation of a micro aerial vehicle (MAV) inside of a constrained shipboard environment to aid in fire con-trol, which might be perilous or inaccessible for humans. The environment is GPS-denied and visually degraded, containing narrow passageways, doorways and small objects protruding from the wall, which makes existing 2D LIDAR, vision or mechanical bumper-based autonomous navigation solutions fail. To re-alize autonomous navigation in such challenging environments, we first propose a fast and robust state estimation algorithm that fuses estimates from a direct depth odometry method and a Monte Carlo localization algorithm with other sensor in-formation in a two-level fusion framework. Then, an online motion planning al-gorithm that combines trajectory optimization with receding horizon control is proposed for fast obstacle avoidance. All the computations are done in real-time onboard our customized MAV platform. We validate the system by running ex-periments in different environmental conditions. The results of over 10 runs show that our vehicle robustly navigates 20m long corridors only 1m wide and goes through a very narrow doorway (only 4cm clearance on each side) completely autonomously even when it is completely dark or full of light smoke.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Dorneich:2015kj">[8]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Mixed-Initiative Control of a Roadable Air Vehicle for Non-Pilots</b>. </div><div class="csl-block csl-author">By Dorneich, M.C., Letsu-Dake, E., Singh, S., Scherer, S., Chamberlain, L. and Bergerman, M.</div><div class="csl-block csl-event">In <i>Journal of Human-Robot Interaction</i>, vol. 4, no. 3, p. 38, Jan. 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleDorneich2015kj()">bibtex</button>

<script>
    function toggleDorneich2015kj() {
        var x= document.getElementById('aDorneich:2015kj');
        // console.log("haha %o",typeof Dorneich:2015kj);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Dorneich2015kj()">abstract</button>


<script>
    function toggle2Dorneich2015kj() {
        var x= document.getElementById('bDorneich:2015kj');
        // console.log("haha %o",typeof Dorneich:2015kj);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.5898/jhri.4.3.dorneich"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aDorneich:2015kj" style="display:none"><pre>@article{Dorneich:2015kj,
  author = {Dorneich, Michael Christian and Letsu-Dake, Emmanuel and Singh, Sanjiv and Scherer, Sebastian and Chamberlain, Lyle and Bergerman, Marcel},
  doi = {10.5898/jhri.4.3.dorneich},
  issn = {2163-0364},
  journal = {Journal of Human-Robot Interaction},
  month = jan,
  number = {3},
  pages = {38},
  title = {Mixed-Initiative Control of a Roadable Air Vehicle for Non-Pilots},
  volume = {4},
  year = {2015}
}
</pre></div>
<div id="bDorneich:2015kj" style="display:none"><pre>This work developed and evaluated a human-machine interface for the control of a roadable air vehicle (RAV), capable of surface driving, vertical takeoff, sustained flight, and landing. Military applications seek to combine the benefits of ground and air vehicles to maximize flexibility of movement but require that the operator have minimal pilot training. This makes the operator vulnerable to automation complexity issues; however, the operator will expect to be able to interact extensively and control the vehicle during flight. A mixed-initiative control approach mitigates these vulnerabilities by integrating the operator into many complex control domains in the way that they often expect—flexibly in charge, aware, but not required to issue every command. Intrinsic safety aspects were evaluated by comparing performance, decision making, precision, and workload for three RAV control paradigms: human-only, fully automated, and mixed-initiative control. The results suggest that the mixed-initiative paradigm leverages the benefits of human and automated control while also avoiding the drawbacks associated with each.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Raj__2015_8025">[9]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Multi-Scale Convolutional Architecture for Semantic Segmentation</b>.</div><div class="csl-block csl-author">By Raj, A., Maturana, D. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-15-21, Oct-2015</div></div></span>
    <br />
<button class="button0" onclick="toggleRaj__2015_8025()">bibtex</button>

<script>
    function toggleRaj__2015_8025() {
        var x= document.getElementById('aRaj__2015_8025');
        // console.log("haha %o",typeof Raj__2015_8025);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Raj__2015_8025()">abstract</button>


<script>
    function toggle2Raj__2015_8025() {
        var x= document.getElementById('bRaj__2015_8025');
        // console.log("haha %o",typeof Raj__2015_8025);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.ri.cmu.edu/pub_files/2015/10/CMU-RI-TR_AmanRaj_revision2.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aRaj__2015_8025" style="display:none"><pre>@techreport{Raj__2015_8025,
  address = {Pittsburgh, PA},
  author = {Raj, Aman and Maturana, Daniel and Scherer, Sebastian},
  booktitle = {Cmu},
  institution = {Carnegie Mellon University},
  month = oct,
  number = {CMU-RI-TR-15-21},
  title = {Multi-Scale Convolutional Architecture for Semantic Segmentation},
  url = {https://www.ri.cmu.edu/pub{\_}files/2015/10/CMU-RI-TR{\_}AmanRaj{\_}revision2.pdf},
  year = {2015}
}
</pre></div>
<div id="bRaj__2015_8025" style="display:none"><pre>Advances in 3D sensing technologies have made the availability of RGB and Depth information easier than earlier which can greatly assist in the semantic segmentation of 2D scenes. There are many works in literature that perform semantic segmentation in such scenes, but few relates to the environment that possesses a high degree of clutter in general e.g. indoor scenes. In this paper, we explore the use of depth information along with RGB and deep convolutional network for indoor scene understanding through semantic labeling. Our work exploits the geocentric encoding of a depth image and uses a multi-scale deep convolutional neural network architecture that captures high and low-level features of a scene to generate rich semantic labels. We apply our method on indoor RGBD images from NYUD2 dataset [1] and achieve a competitive performance of 70.45 % accuracy in labeling four object classes compared with some prior approaches. The results show our system is capable of generating a pixel-map directly from an input image where each pixel-value corresponds to a particular class of object.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Althoff_2015_8013">[10]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Online safety verification of trajectories for unmanned flight with offline computed robust invariant sets</b>.</div><div class="csl-block csl-author">By Althoff, D., Althoff, M. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Hamburg, Germany, pp. 3470–3477, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleAlthoff_2015_8013()">bibtex</button>

<script>
    function toggleAlthoff_2015_8013() {
        var x= document.getElementById('aAlthoff_2015_8013');
        // console.log("haha %o",typeof Althoff_2015_8013);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Althoff_2015_8013()">abstract</button>


<script>
    function toggle2Althoff_2015_8013() {
        var x= document.getElementById('bAlthoff_2015_8013');
        // console.log("haha %o",typeof Althoff_2015_8013);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2015.7353861"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aAlthoff_2015_8013" style="display:none"><pre>@inproceedings{Althoff_2015_8013,
  address = {Hamburg, Germany},
  author = {Althoff, Daniel and Althoff, Matthias and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2015.7353861},
  isbn = {9781479999941},
  issn = {21530866},
  month = sep,
  pages = {3470--3477},
  title = {Online safety verification of trajectories for unmanned flight with offline computed robust invariant sets},
  year = {2015}
}
</pre></div>
<div id="bAlthoff_2015_8013" style="display:none"><pre>We address the problem of verifying motion plans for aerial robots in uncertain and partially-known environments. Thereby, the initial state of the robot is uncertain due to errors from the state estimation and the motion is uncertain due to wind disturbances and control errors caused by sensor noise. Since the environment is perceived at runtime, the verification of partial motion plans must be performed online (i.e. during operation) to ensure safety within the planning horizon and beyond. This is achieved by efficiently generating robust control invariant sets based on so-called loiter circles, where the position of the aerial robot follows a circular pattern. Verification of aerial robots is challenging due to the nonlinearity of their dynamics, the high dimensionality of their state space, and their potentially high velocities. We use novel techniques from reachability analysis to overcome those challenges. In order to ensure that the robot never finds itself in a situation for which no safe maneuver exists, we provide a technique that ensures safety of aerial robots beyond the planning horizon. Our method is applicable to all kinds of robotic systems that follow reference trajectories, such as bipedal robotic walking, robotic manipulators, automated vehicles, and the like. We evaluate our method by simulations of high speed helicopter flights.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Arora:2015wx">[11]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>PASP: Policy based approach for sensor planning</b>.</div><div class="csl-block csl-author">By Arora, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WAno. June, , pp. 3479–3486, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2015wx()">bibtex</button>

<script>
    function toggleArora2015wx() {
        var x= document.getElementById('aArora:2015wx');
        // console.log("haha %o",typeof Arora:2015wx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Arora2015wx()">abstract</button>


<script>
    function toggle2Arora2015wx() {
        var x= document.getElementById('bArora:2015wx');
        // console.log("haha %o",typeof Arora:2015wx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7139680"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aArora:2015wx" style="display:none"><pre>@inproceedings{Arora:2015wx,
  address = {Seattle, WA},
  author = {Arora, Sankalp and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7139680},
  issn = {10504729},
  month = may,
  number = {June},
  pages = {3479--3486},
  title = {{PASP: P}olicy based approach for sensor planning},
  year = {2015}
}
</pre></div>
<div id="bArora:2015wx" style="display:none"><pre>Capabilities of mobile autonomous systems is often limited by the sensory constraints. Range sensors moving in a fixed pattern are commonly used as sensing modalities on mobile robots. The performance of these sensors can be augmented by actively controlling their configuration for minimizing the expected cost of the mission. The related information gain problem in NP hard. Current methodologies are either computationally too expensive to run online or make simplifying assumptions that fail in complex environments. We present a method to create and learn a policy that maps features calculated online to sensory actions. The policy developed in this work actively controls a nodding lidar to keep the vehicle safe at high velocities and focuses the sensor bandwidth on gaining information relevant for the mission once safety is ensured. It is validated and evaluated on an autonomous full-scale helicopter (Boeing Unmanned Little Bird) equipped with an actively controlled nodding laser. It is able to keep the vehicle safe at its maximum operating velocity, 56 m=s, and reduce the landing zone evaluation time by 500% as compared to passive nodding. The structure of the policy and efficient learning algorithm should generalize to provide a solution for actively controlling a sensor for keeping a mobile robots safe while exploring regions of interest to the robot.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Fang:2015ed">[12]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Real-time onboard 6DoF localization of an indoor MAV in degraded visual environments using a RGB-D camera</b>.</div><div class="csl-block csl-author">By Fang, Z. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WA, USA, pp. 5253–5259, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleFang2015ed()">bibtex</button>

<script>
    function toggleFang2015ed() {
        var x= document.getElementById('aFang:2015ed');
        // console.log("haha %o",typeof Fang:2015ed);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Fang2015ed()">abstract</button>


<script>
    function toggle2Fang2015ed() {
        var x= document.getElementById('bFang:2015ed');
        // console.log("haha %o",typeof Fang:2015ed);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7139931"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aFang:2015ed" style="display:none"><pre>@inproceedings{Fang:2015ed,
  address = {Seattle, WA, USA},
  author = {Fang, Zheng and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7139931},
  issn = {10504729},
  month = jun,
  pages = {5253--5259},
  title = {Real-time onboard {6DoF} localization of an indoor {MAV} in degraded visual environments using a {RGB-D} camera},
  year = {2015}
}
</pre></div>
<div id="bFang:2015ed" style="display:none"><pre>Real-time and reliable localization is a prerequisite for autonomously performing high-level tasks with micro aerial vehicles(MAVs). Nowadays, most existing methods use vision system for 6DoF pose estimation, which can not work in degraded visual environments. This paper presents an onboard 6DoF pose estimation method for an indoor MAV in challenging GPS-denied degraded visual environments by using a RGB-D camera. In our system, depth images are mainly used for odometry estimation and localization. First, a fast and robust relative pose estimation (6DoF Odometry) method is proposed, which uses the range rate constraint equation and photometric error metric to get the frame-to-frame transform. Then, an absolute pose estimation (6DoF Localization) method is proposed to locate the MAV in a given 3D global map by using a particle filter. The whole localization system can run in real-time on an embedded computer with low CPU usage. We demonstrate the effectiveness of our system in extensive real environments on a customized MAV platform. The experimental results show that our localization system can robustly and accurately locate the robot in various practical challenging environments.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Ho:2015id">[13]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Recognition of human group activity for video analytics</b>.</div><div class="csl-block csl-author">By Ju, J., Yang, C., Scherer, S. and Ko, H.</div><div class="csl-block csl-event">In <i>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</i>, vol. 9315</div>Y.-S. Ho, J. Sang, Y. M. Ro, J. Kim, and F. Wu, Eds. <div class="csl-block csl-editor">Cham: , 2015, pp. pp. 161–169</div></div></span>
    <br />
<button class="button0" onclick="toggleHo2015id()">bibtex</button>

<script>
    function toggleHo2015id() {
        var x= document.getElementById('aHo:2015id');
        // console.log("haha %o",typeof Ho:2015id);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Ho2015id()">abstract</button>


<script>
    function toggle2Ho2015id() {
        var x= document.getElementById('bHo:2015id');
        // console.log("haha %o",typeof Ho:2015id);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/978-3-319-24078-7_16"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aHo:2015id" style="display:none"><pre>@incollection{Ho:2015id,
  address = {Cham},
  author = {Ju, Jaeyong and Yang, Cheoljong and Scherer, Sebastian and Ko, Hanseok},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi = {10.1007/978-3-319-24078-7_16},
  editor = {Ho, Yo-Sung and Sang, Jitao and Ro, Yong Man and Kim, Junmo and Wu, Fei},
  isbn = {9783319240770},
  issn = {16113349},
  keywords = {Activity recognition,Human group activity,Video analytics},
  month = sep,
  pages = {161--169},
  title = {Recognition of human group activity for video analytics},
  volume = {9315},
  year = {2015}
}
</pre></div>
<div id="bHo:2015id" style="display:none"><pre>Human activity recognition is an important and challenging task for video content analysis and understanding. Individual activity recognition has been well studied recently. However, recognizing the activities of human group with more than three people having complex interactions is still a formidable challenge. In this paper, a novel human group activity recognition method is proposed to deal with complex situation where there are multiple sub-groups. To characterize the inherent interactions of intra-subgroups and inter-subgroups with the varying number of participants, this paper proposes three types of group-activity descriptor using motion trajectory and appearance information of people. Experimental results on a public human group activity dataset demonstrate effectiveness of the proposed method.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2015dw">[14]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>The Dynamics Projection Filter (DPF) - Real-time nonlinear trajectory optimization using projection operators</b>.</div><div class="csl-block csl-author">By Choudhury, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WA, USA, pp. 644–649, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2015dw()">bibtex</button>

<script>
    function toggleChoudhury2015dw() {
        var x= document.getElementById('aChoudhury:2015dw');
        // console.log("haha %o",typeof Choudhury:2015dw);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2015dw()">abstract</button>


<script>
    function toggle2Choudhury2015dw() {
        var x= document.getElementById('bChoudhury:2015dw');
        // console.log("haha %o",typeof Choudhury:2015dw);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7139247"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2015dw" style="display:none"><pre>@inproceedings{Choudhury:2015dw,
  address = {Seattle, WA, USA},
  author = {Choudhury, Sanjiban and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7139247},
  issn = {10504729},
  month = jun,
  pages = {644--649},
  title = {The Dynamics Projection Filter {(DPF)} - Real-time nonlinear trajectory optimization using projection operators},
  year = {2015}
}
</pre></div>
<div id="bChoudhury:2015dw" style="display:none"><pre>Robotic navigation applications often require on-line generation of trajectories that respect underactuated non-linear dynamics, while optimizing a cost function that depends only on a low-dimensional workspace (collision avoidance). Approaches to non-linear optimization, such as differential dynamic programming (DDP), suffer from the drawbacks of slow convergence by being limited to stay within the trust-region of the linearized dynamics and having to integrate the dynamics with fine granularity at each iteration. We address the problem of decoupling the workspace optimization from the enforcement of non-linear constraints. In this paper, we introduce the Dynamics Projection Filter, a nonlinear projection operator based approach that first optimizes a workspace trajectory with reduced constraints and then projects (filters) it to a feasible configuration space trajectory that has a bounded sub-optimality guarantee. We show simulation results for various curvature and curvature-derivatives constrained systems, where the dynamics projection filter is able to, on average, produce similar quality solution 50 times faster than DDP. We also show results from flight tests on an autonomous helicopter that solved these problems on-line while avoiding mountains at high speed as well as trees and buildings as it came in to land.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury_2015_7887">[15]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>The planner ensemble: Motion planning by executing diverse algorithms</b>.</div><div class="csl-block csl-author">By Choudhury, S., Arora, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Seattle, WA, USA, pp. 2389–2395, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury_2015_7887()">bibtex</button>

<script>
    function toggleChoudhury_2015_7887() {
        var x= document.getElementById('aChoudhury_2015_7887');
        // console.log("haha %o",typeof Choudhury_2015_7887);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury_2015_7887()">abstract</button>


<script>
    function toggle2Choudhury_2015_7887() {
        var x= document.getElementById('bChoudhury_2015_7887');
        // console.log("haha %o",typeof Choudhury_2015_7887);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2015.7139517"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury_2015_7887" style="display:none"><pre>@inproceedings{Choudhury_2015_7887,
  address = {Seattle, WA, USA},
  author = {Choudhury, Sanjiban and Arora, Sankalp and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2015.7139517},
  issn = {10504729},
  month = jun,
  pages = {2389--2395},
  title = {The planner ensemble: {Motion} planning by executing diverse algorithms},
  year = {2015}
}
</pre></div>
<div id="bChoudhury_2015_7887" style="display:none"><pre>Autonomous systems that navigate in unknown environments encounter a variety of planning problems. The success of any one particular planning strategy depends on the validity of assumptions it leverages about the structure of the problem, e.g., Is the cost map locally convex? Does the feasible state space have good connectivity? We address the problem of determining suitable motion planning strategies that can work on a diverse set of applications. We have developed a planning system that does this by running competing planners in parallel. In this paper, we present an approach that constructs a planner ensemble - a set of complementary planners that lever-age a diverse set of assumptions. Our approach optimizes the submodular selection criteria with a greedy approach and lazy evaluation. We seed our selection with learnt priors on planner performance, thus allowing us to solve new applications without evaluating every planner on that application. We present results in simulation where the selected ensemble outperforms the best single planner and does almost as well as an off-line planner. We also present results from an autonomous helicopter that has flown missions several kilometers long at speeds of up to 56m/s which involved avoiding unmapped mountains, no-fly zones and landing in cluttered areas with trees and buildings. This work opens the door on the more general problem of adaptive motion planning.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury_2015_7965">[16]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Theoretical limits of speed and resolution for kinodynamic planning in a poisson forest</b>. </div><div class="csl-block csl-author">By Choudhury, S., Scherer, S. and Bagnell, J.A.</div><div class="csl-block csl-event">In <i>Robotics: Science and Systems</i>, vol. 11, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury_2015_7965()">bibtex</button>

<script>
    function toggleChoudhury_2015_7965() {
        var x= document.getElementById('aChoudhury_2015_7965');
        // console.log("haha %o",typeof Choudhury_2015_7965);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury_2015_7965()">abstract</button>


<script>
    function toggle2Choudhury_2015_7965() {
        var x= document.getElementById('bChoudhury_2015_7965');
        // console.log("haha %o",typeof Choudhury_2015_7965);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.15607/RSS.2015.XI.005"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury_2015_7965" style="display:none"><pre>@article{Choudhury_2015_7965,
  author = {Choudhury, Sanjiban and Scherer, Sebastian and Bagnell, J. Andrew},
  doi = {10.15607/RSS.2015.XI.005},
  isbn = {9780992374716},
  issn = {2330765X},
  journal = {Robotics: Science and Systems},
  title = {Theoretical limits of speed and resolution for kinodynamic planning in a poisson forest},
  volume = {11},
  year = {2015}
}
</pre></div>
<div id="bChoudhury_2015_7965" style="display:none"><pre>The performance of a state lattice motion planning algorithm depends critically on the resolution of the lattice to ensure a balance between solution quality and computation time. There is currently no theoretical basis for selecting the resolution because of its dependence on the robot dynamics and the distribution of obstacles. In this paper, we examine the problem of motion planning on a resolution constrained lattice for a robot with non-linear dynamics operating in an environment with randomly generated disc shaped obstacles sampled from a homogeneous Poisson process. We present a unified framework for computing explicit solutions to two problems - i) the critical planning resolution which guarantees the existence of an infinite collision free trajectory in the search graph ii) the critical speed limit which guarantees infinite collision free motion. In contrast to techniques used by Karaman and Frazzoli [11], we use a novel approach that maps the problem to parameters of directed asymmetric hexagonal lattice bond percolation. Since standard percolation theory offers no results for this lattice, we map the lattice to an infinite absorbing Markov chain and use results pertaining to its survival to obtain bounds on the parameters. As a result, we are able to derive theoretical expressions that relate the non-linear dynamics of a robot, the resolution of the search graph and the density of the Poisson process. We validate the theoretical bounds using Monte-Carlo simulations for single integrator and curvature constrained systems and are able to validate the previous results presented by Karaman and Frazzoli [11] independently using the novel connections introduced in this paper.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Maturana_2015_8004">[17]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>VoxNet: A 3D Convolutional Neural Network for real-time object recognition</b>.</div><div class="csl-block csl-author">By Maturana, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Hamburg, Germany, pp. 922–928, 2015.</div></div></span>
    <br />
<button class="button0" onclick="toggleMaturana_2015_8004()">bibtex</button>

<script>
    function toggleMaturana_2015_8004() {
        var x= document.getElementById('aMaturana_2015_8004');
        // console.log("haha %o",typeof Maturana_2015_8004);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Maturana_2015_8004()">abstract</button>


<script>
    function toggle2Maturana_2015_8004() {
        var x= document.getElementById('bMaturana_2015_8004');
        // console.log("haha %o",typeof Maturana_2015_8004);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2015.7353481"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMaturana_2015_8004" style="display:none"><pre>@inproceedings{Maturana_2015_8004,
  address = {Hamburg, Germany},
  author = {Maturana, Daniel and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2015.7353481},
  isbn = {9781479999941},
  issn = {21530866},
  month = sep,
  pages = {922--928},
  title = {{VoxNet:} A {3D} Convolutional Neural Network for real-time object recognition},
  year = {2015}
}
</pre></div>
<div id="bMaturana_2015_8004" style="display:none"><pre>Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.</pre></div>
</li></ul.no-bullet>

<h1 id="2014">2014</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Arora:2014ul">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>A principled approach to enable safe and high performance maneuvers for autonomous rotorcraft</b>.</div><div class="csl-block csl-author">By Arora, S., Choudhury, S., Althoff, D. and Scherer, S.</div><div class="csl-block csl-event">In <i>Annual Forum Proceedings - AHS International</i>, Montreal, CANvol. 4, , pp. 3228–3236, 2014.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2014ul()">bibtex</button>

<script>
    function toggleArora2014ul() {
        var x= document.getElementById('aArora:2014ul');
        // console.log("haha %o",typeof Arora:2014ul);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Arora2014ul()">abstract</button>


<script>
    function toggle2Arora2014ul() {
        var x= document.getElementById('bArora:2014ul');
        // console.log("haha %o",typeof Arora:2014ul);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aArora:2014ul" style="display:none"><pre>@inproceedings{Arora:2014ul,
  address = {Montreal, CAN},
  author = {Arora, Sankalp and Choudhury, Sanjiban and Althoff, Daniel and Scherer, Sebastian},
  booktitle = {Annual Forum Proceedings - AHS International},
  isbn = {9781632666918},
  issn = {15522938},
  month = may,
  pages = {3228--3236},
  title = {A principled approach to enable safe and high performance maneuvers for autonomous rotorcraft},
  volume = {4},
  year = {2014}
}
</pre></div>
<div id="bArora:2014ul" style="display:none"><pre>Autonomous rotorcraft are required to operate in cluttered, unknown, and unstructured environments. Guaranteeing the safety of these systems is critical for their successful deployment. Current methodologies for evaluating or ensuring safety either do not guarantee safety or severely limit the performance of rotorcraft. To design a guaranteed safe rotorcraft, we have defined safety for an autonomous rotorcraft flying in unknown environments given sensory and dynamic constraints. We have developed an approach that ensures the vehicle’s safety while pushing the limits of safe operation of the vehicle. Furthermore, the presented safety definition and the presented approach are independent of the vehicle and planning algorithm used on the rotorcraft. In this paper we present a real time algorithm to guarantee the safety of the rotorcraft through a diverse set of emergency maneuvers. We prove that the related trajectory set diversity problem is monotonic and sub-modular which enables us to develop an efficient, bounded sub-optimal trajectory set generation algorithm. We present safety results for the autonomous Unmanned Little Bird Helicopter flying at speeds of up to 56m/s in partially-known environments. Through months of flight testing the helicopter has been avoiding trees, performing autonomous landing, avoiding mountains while being guaranteed safe. We also present simulation results of the helicopter flying in the Grand Canyon, with no prior map of the environment. Copyright \textcopyright 2014 by the American Helicopter Society International, Inc. All rights reserved.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Fang:2014be">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Experimental study of odometry estimation methods using RGB-D cameras</b>.</div><div class="csl-block csl-author">By Fang, Z. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Chicago, IL, pp. 680–687, 2014.</div></div></span>
    <br />
<button class="button0" onclick="toggleFang2014be()">bibtex</button>

<script>
    function toggleFang2014be() {
        var x= document.getElementById('aFang:2014be');
        // console.log("haha %o",typeof Fang:2014be);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Fang2014be()">abstract</button>


<script>
    function toggle2Fang2014be() {
        var x= document.getElementById('bFang:2014be');
        // console.log("haha %o",typeof Fang:2014be);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2014.6942632"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aFang:2014be" style="display:none"><pre>@inproceedings{Fang:2014be,
  address = {Chicago, IL},
  author = {Fang, Zheng and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2014.6942632},
  isbn = {9781479969340},
  issn = {21530866},
  pages = {680--687},
  title = {Experimental study of odometry estimation methods using {RGB-D} cameras},
  year = {2014}
}
</pre></div>
<div id="bFang:2014be" style="display:none"><pre>Lightweight RGB-D cameras that can provide rich 2D visual and 3D point cloud information are well suited to the motion estimation of indoor micro aerial vehicles (MAVs). In recent years, several RGB-D visual odometry methods which process data from the sensor in different ways have been proposed. However, it is unclear which methods are preferable for online odometry estimation on a computation-limited, fast moving MAV in practical indoor environments. This paper presents a detailed analysis and comparison of several state-of-the-art real-time odometry estimation methods in a variety of challenging scenarios, with a special emphasis on the trade-off among accuracy, robustness and computation speed. An experimental comparison is conducted using public available benchmark datasets and author-collected datasets including long corridors, illumination changing environments and fast motion scenarios. Experimental results present both quantitative and qualitative differences among these methods and provide some guidelines on choosing the ’right’ algorithm for an indoor MAV according to the quality of the RGB-D data and environment characteristics.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury_2014_7684">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning Motion Planning Assumptions</b>.</div><div class="csl-block csl-author">By Vemula, A., Choudhury, S. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-14-14, Aug-2014</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury_2014_7684()">bibtex</button>

<script>
    function toggleChoudhury_2014_7684() {
        var x= document.getElementById('aChoudhury_2014_7684');
        // console.log("haha %o",typeof Choudhury_2014_7684);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury_2014_7684()">abstract</button>


<script>
    function toggle2Choudhury_2014_7684() {
        var x= document.getElementById('bChoudhury_2014_7684');
        // console.log("haha %o",typeof Choudhury_2014_7684);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.ri.cmu.edu/pub_files/2014/8/LearningMotionPlanningAssumptions.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aChoudhury_2014_7684" style="display:none"><pre>@techreport{Choudhury_2014_7684,
  address = {Pittsburgh, PA},
  author = {Vemula, Anirudh and Choudhury, Sanjiban and Scherer, Sebastian},
  institution = {Carnegie Mellon University},
  month = aug,
  number = {CMU-RI-TR-14-14},
  pages = {11},
  title = {Learning Motion Planning Assumptions},
  url = {https://www.ri.cmu.edu/pub_files/2014/8/LearningMotionPlanningAssumptions.pdf},
  year = {2014}
}
</pre></div>
<div id="bChoudhury_2014_7684" style="display:none"><pre>The performance of a motion planning algorithm is intrinsically linked with applications that respect the assumptions being made. However, the mapping of these assumptions to actual environments is not always transparent. For example, a gradient descent algorithm is capable of tackling a complex opti- mization problem if some assurance of absence of bad local minimas can be ensured - however detecting the local minimas beforehand is very challenging. The state of the art technique relies on an expert to analyze the application, deduce assumptions that the planner can leverage and subsequently make key design decisions. In this work, we make an attempt to learn a mapping from environments to specific planning assumptions. This paper presents a diverse ensemble of planners that exploit very different aspects of the planning problem. A classifier is then trained to approximate the mapping from environment to performance difference between a pair of planners. Preliminary results hints at the role played by convexity, whilst also demonstrating the difficulty of the classification task at hand.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chambers:2014kr">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Robust multi-sensor fusion for micro aerial vehicle navigation in GPS-degraded/denied environments</b>.</div><div class="csl-block csl-author">By Chambers, A., Scherer, S., Yoder, L., Jain, S., Nuske, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings of the American Control Conference</i>, Portland, OR, pp. 1892–1899, 2014.</div></div></span>
    <br />
<button class="button0" onclick="toggleChambers2014kr()">bibtex</button>

<script>
    function toggleChambers2014kr() {
        var x= document.getElementById('aChambers:2014kr');
        // console.log("haha %o",typeof Chambers:2014kr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chambers2014kr()">abstract</button>


<script>
    function toggle2Chambers2014kr() {
        var x= document.getElementById('bChambers:2014kr');
        // console.log("haha %o",typeof Chambers:2014kr);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ACC.2014.6859341"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChambers:2014kr" style="display:none"><pre>@inproceedings{Chambers:2014kr,
  address = {Portland, OR},
  author = {Chambers, Andrew and Scherer, Sebastian and Yoder, Luke and Jain, Sezal and Nuske, Stephen and Singh, Sanjiv},
  booktitle = {Proceedings of the American Control Conference},
  doi = {10.1109/ACC.2014.6859341},
  isbn = {9781479932726},
  issn = {07431619},
  keywords = {Autonomous systems,Filtering,Vision-based control},
  pages = {1892--1899},
  title = {Robust multi-sensor fusion for micro aerial vehicle navigation in {GPS}-degraded/denied environments},
  year = {2014}
}
</pre></div>
<div id="bChambers:2014kr" style="display:none"><pre>State estimation for Micro Air Vehicles (MAVs) is challenging because sensing instrumentation carried on-board is severely limited by weight and power constraints. In addition, their use close to and inside structures and vegetation means that GPS signals can be degraded or all together absent. Here we present a navigation system suited for use on MAVs that seamlessly fuses any combination of GPS, visual odometry, inertial measurements, and/or barometric pressure. We focus on robustness against real-world conditions and evaluate performance in challenging field experiments. Results demonstrate that the proposed approach is effective at providing a consistent state estimate even during multiple sensor failures and can be used for mapping, planning, and control. \textcopyright 2014 American Automatic Control Council.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2014tq">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>The planner ensemble and trajectory executive: A high performance motion planning system with guaranteed safety</b>.</div><div class="csl-block csl-author">By Choudhury, S., Arora, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Annual Forum Proceedings - AHS International</i>, Montreal, CANvol. 4, , pp. 2872–2891, 2014.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2014tq()">bibtex</button>

<script>
    function toggleChoudhury2014tq() {
        var x= document.getElementById('aChoudhury:2014tq');
        // console.log("haha %o",typeof Choudhury:2014tq);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2014tq()">abstract</button>


<script>
    function toggle2Choudhury2014tq() {
        var x= document.getElementById('bChoudhury:2014tq');
        // console.log("haha %o",typeof Choudhury:2014tq);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChoudhury:2014tq" style="display:none"><pre>@inproceedings{Choudhury:2014tq,
  address = {Montreal, CAN},
  author = {Choudhury, Sanjiban and Arora, Sankalp and Scherer, Sebastian},
  booktitle = {Annual Forum Proceedings - AHS International},
  isbn = {9781632666918},
  issn = {15522938},
  month = may,
  pages = {2872--2891},
  title = {The planner ensemble and trajectory executive: A high performance motion planning system with guaranteed safety},
  volume = {4},
  year = {2014}
}
</pre></div>
<div id="bChoudhury:2014tq" style="display:none"><pre>Autonomous helicopters are required to fly at a wide range of speed close to ground and eventually land in an unprepared cluttered area. Existing planning systems for unmanned rotorcrafts are capable of flying in unmapped environments, however they are restricted to a specific operating regime dictated by the underlying planning algorithm. We address the problem of planning a trajectory that is computed in real time, respects the dynamics of the helicopter, and keeps the vehicle safe in an unmapped environment with a finite horizon sensor. We have developed a planning system that is capable of doing this by running competing planners in parallel. This paper presents a planning architecture that consists of a trajectory executive - A low latency, verifiable component - That selects plans from a planner ensemble and ensures safety by maintaining emergency maneuvers. Here we report results with an autonomous helicopter that flies missions several kilometers long through unmapped terrain at speeds of upto 56 m/s and landing in clutter. In over 6 months of flight testing, the system has avoided unmapped mountains, popup no fly zones, and has come into land while avoiding trees and buildings in a cluttered landing zone. We also present results from simulation where the same system is flown in challenging obstacle regions - In all cases the system always remains safe and accomplishes the mission. As a result, the system showcases the ability to have a high performance in all environments while guaranteeing safety. \textcopyright 2014 by the American Helicopter Society international Inc. All rights reserved.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Maturana_2014_7899">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Visual Odometry in Smoke Occluded Environments</b>.</div><div class="csl-block csl-author">By Agarwal, A., Maturana, D. and Scherer, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-15-07, Jul-2014</div></div></span>
    <br />
<button class="button0" onclick="toggleMaturana_2014_7899()">bibtex</button>

<script>
    function toggleMaturana_2014_7899() {
        var x= document.getElementById('aMaturana_2014_7899');
        // console.log("haha %o",typeof Maturana_2014_7899);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Maturana_2014_7899() {
        var x= document.getElementById('bMaturana_2014_7899');
        // console.log("haha %o",typeof Maturana_2014_7899);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="https://www.ri.cmu.edu/pub_files/2014/7/aditya_tr.pdf"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aMaturana_2014_7899" style="display:none"><pre>@techreport{Maturana_2014_7899,
  address = {Pittsburgh, PA},
  author = {Agarwal, Aditya and Maturana, Daniel and Scherer, Sebastian},
  institution = {Carnegie Mellon University},
  month = jul,
  number = {CMU-RI-TR-15-07},
  pages = {CMU--RI--TR--15--07},
  title = {Visual Odometry in Smoke Occluded Environments},
  url = {https://www.ri.cmu.edu/pub{\_}files/2014/7/aditya{\_}tr.pdf},
  year = {2014}
}
</pre></div>
<div id="bMaturana_2014_7899" style="display:none"><pre></pre></div>
</li></ul.no-bullet>

<h1 id="2013">2013</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Choudhury:2013vz">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous emergency landing of a helicopter: Motion planning with hard time-constraints</b>.</div><div class="csl-block csl-author">By Choudhury, S., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Annual Forum Proceedings - AHS International</i>, Phoenix, AZvol. 3, , pp. 2236–2249, 2013.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2013vz()">bibtex</button>

<script>
    function toggleChoudhury2013vz() {
        var x= document.getElementById('aChoudhury:2013vz');
        // console.log("haha %o",typeof Choudhury:2013vz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2013vz()">abstract</button>


<script>
    function toggle2Choudhury2013vz() {
        var x= document.getElementById('bChoudhury:2013vz');
        // console.log("haha %o",typeof Choudhury:2013vz);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChoudhury:2013vz" style="display:none"><pre>@inproceedings{Choudhury:2013vz,
  address = {Phoenix, AZ},
  author = {Choudhury, Sanjiban and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {Annual Forum Proceedings - AHS International},
  issn = {15522938},
  month = may,
  pages = {2236--2249},
  title = {Autonomous emergency landing of a helicopter: Motion planning with hard time-constraints},
  volume = {3},
  year = {2013}
}
</pre></div>
<div id="bChoudhury:2013vz" style="display:none"><pre>Engine malfunctions during helicopter flight poses a large risk to pilot and crew. Without a quick and coordinated reaction, such situations lead to a complete loss of control. An autonomous landing system is capable of reacting quickly to regain control, however current emergency landing methods focus only on the offline generation of dynamically feasible trajectories while ignoring the more severe constraints faced while autonomously landing a real helicopter during an unplanned engine failure. We address the problem of autonomously landing a helicopter while considering a realistic context: hard time-constraints, challenging terrain, sensor limitations and availability of pilot contextual knowledge. We designed a planning system that deals with all these factors by being able to compute alternate routes (AR) in a rapid fashion. This paper presents an algorithm, RRT*-AR, building upon the optimal sampling-based algorithm RRT* to generate AR in realtime while maintaining optimality guarantees and examines its performance for simulated failures occurring in mountainous terrain. After over 4500 trials, RRT*-AR outperformed RRT* by providing the human 280% more options 67% faster on average. As a result, it provides a much wider safety margin for unaccounted disturbances, and a more secure environment for a pilot. \textcopyright 2013 by the American Helicopter Society International, Inc. All rights reserved.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Mori:2013dm">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>First results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehicles</b>.</div><div class="csl-block csl-author">By Mori, T. and Scherer, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Karlsruhe, Germany, pp. 1750–1757, 2013.</div></div></span>
    <br />
<button class="button0" onclick="toggleMori2013dm()">bibtex</button>

<script>
    function toggleMori2013dm() {
        var x= document.getElementById('aMori:2013dm');
        // console.log("haha %o",typeof Mori:2013dm);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Mori2013dm()">abstract</button>


<script>
    function toggle2Mori2013dm() {
        var x= document.getElementById('bMori:2013dm');
        // console.log("haha %o",typeof Mori:2013dm);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2013.6630807"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aMori:2013dm" style="display:none"><pre>@inproceedings{Mori:2013dm,
  address = {Karlsruhe, Germany},
  author = {Mori, Tomoyuki and Scherer, Sebastian},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2013.6630807},
  isbn = {9781467356411},
  issn = {10504729},
  month = may,
  pages = {1750--1757},
  title = {First results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehicles},
  year = {2013}
}
</pre></div>
<div id="bMori:2013dm" style="display:none"><pre>Obstacle avoidance is desirable for lightweight micro aerial vehicles and is a challenging problem since the payload constraints only permit monocular cameras and obstacles cannot be directly observed. Depth can however be inferred based on various cues in the image. Prior work has examined optical flow, and perspective cues, however these methods cannot handle frontal obstacles well. In this paper we examine the problem of detecting obstacles right in front of the vehicle. We developed a method to detect relative size changes of image patches that is able to detect size changes in the absence of optical flow. The method uses SURF feature matches in combination with template matching to compare relative obstacle sizes with different image spacing. We present results from our algorithm in autonomous flight tests on a small quadrotor. We are able to detect obstacles with a frame-to-frame enlargement of 120% with a high confidence and confirmed our algorithm in 20 successful flight experiments. In future work, we will improve the control algorithms to avoid more complicated obstacle configurations. \textcopyright 2013 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Arora:2013dg">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Infrastructure-free shipdeck tracking for autonomous landing</b>.</div><div class="csl-block csl-author">By Arora, S., Jain, S., Scherer, S., Nuske, S., Chamberlain, L. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Karlsruhe, Germany, pp. 323–330, 2013.</div></div></span>
    <br />
<button class="button0" onclick="toggleArora2013dg()">bibtex</button>

<script>
    function toggleArora2013dg() {
        var x= document.getElementById('aArora:2013dg');
        // console.log("haha %o",typeof Arora:2013dg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Arora2013dg()">abstract</button>


<script>
    function toggle2Arora2013dg() {
        var x= document.getElementById('bArora:2013dg');
        // console.log("haha %o",typeof Arora:2013dg);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2013.6630595"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aArora:2013dg" style="display:none"><pre>@inproceedings{Arora:2013dg,
  address = {Karlsruhe, Germany},
  author = {Arora, Sankalp and Jain, Sezal and Scherer, Sebastian and Nuske, Stephen and Chamberlain, Lyle and Singh, Sanjiv},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2013.6630595},
  isbn = {9781467356411},
  issn = {10504729},
  month = may,
  pages = {323--330},
  title = {Infrastructure-free shipdeck tracking for autonomous landing},
  year = {2013}
}
</pre></div>
<div id="bArora:2013dg" style="display:none"><pre>Shipdeck landing is one of the most challenging tasks for a rotorcraft. Current autonomous rotorcraft use shipdeck mounted transponders to measure the relative pose of the vehicle to the landing pad. This tracking system is not only expensive but renders an unequipped ship unlandable. We address the challenge of tracking a shipdeck without additional infrastructure on the deck. We present two methods based on video and lidar that are able to track the shipdeck starting at a considerable distance from the ship. This redundant sensor design enables us to have two independent tracking systems. We show the results of the tracking algorithms in three different environments - field testing results on actual helicopter flights, in simulation with a moving shipdeck for lidar based tracking and in laboratory using an occluded, and, moving scaled model of a landing deck for camera based tracking. The complimentary modalities allow shipdeck tracking under varying conditions. \textcopyright 2013 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chamberlain:2013">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Robocopters to the rescue</b>. </div><div class="csl-block csl-author">By Chamberlain, L. and Scherer, S.</div><div class="csl-block csl-event">In <i>IEEE Spectrum</i>, vol. 50, no. 10, pp. 28–33, 2013.</div></div></span>
    <br />
<button class="button0" onclick="toggleChamberlain2013()">bibtex</button>

<script>
    function toggleChamberlain2013() {
        var x= document.getElementById('aChamberlain:2013');
        // console.log("haha %o",typeof Chamberlain:2013);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chamberlain2013()">abstract</button>


<script>
    function toggle2Chamberlain2013() {
        var x= document.getElementById('bChamberlain:2013');
        // console.log("haha %o",typeof Chamberlain:2013);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/MSPEC.2013.6607012"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChamberlain:2013" style="display:none"><pre>@article{Chamberlain:2013,
  author = {Chamberlain, Lyle and Scherer, Sebastian},
  doi = {10.1109/MSPEC.2013.6607012},
  issn = {00189235},
  journal = {IEEE Spectrum},
  number = {10},
  pages = {28--33},
  title = {Robocopters to the rescue},
  volume = {50},
  year = {2013}
}
</pre></div>
<div id="bChamberlain:2013" style="display:none"><pre>We’re standing on the edge of the hot Arizona tarmac, radio in hand, holding our breath as the helicopter passes 50 meters overhead. We watch as the precious sensor on its blunt nose scans every detail of the area, the test pilot and engineer looking down with coolly professional curiosity as they wait for the helicopter to decide where to land. They¿re just onboard observers. The helicopter itself is in charge here. \textcopyright 1964-2012 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2013ek">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>RRT*-AR: Sampling-based alternate routes planning with applications to autonomous emergency landing of a helicopter</b>.</div><div class="csl-block csl-author">By Choudhury, S., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Karlsruhe, Germany, pp. 3947–3952, 2013.</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2013ek()">bibtex</button>

<script>
    function toggleChoudhury2013ek() {
        var x= document.getElementById('aChoudhury:2013ek');
        // console.log("haha %o",typeof Choudhury:2013ek);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2013ek()">abstract</button>


<script>
    function toggle2Choudhury2013ek() {
        var x= document.getElementById('bChoudhury:2013ek');
        // console.log("haha %o",typeof Choudhury:2013ek);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2013.6631133"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChoudhury:2013ek" style="display:none"><pre>@inproceedings{Choudhury:2013ek,
  address = {Karlsruhe, Germany},
  author = {Choudhury, Sanjiban and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2013.6631133},
  isbn = {9781467356411},
  issn = {10504729},
  month = may,
  pages = {3947--3952},
  title = {{RRT*-AR:} {Sampling}-based alternate routes planning with applications to autonomous emergency landing of a helicopter},
  year = {2013}
}
</pre></div>
<div id="bChoudhury:2013ek" style="display:none"><pre>Engine malfunctions during helicopter flight poses a large risk to pilot and crew. Without a quick and coordinated reaction, such situations lead to a complete loss of control. An autonomous landing system could react quicker to regain control, however current emergency landing methods only generate dynamically feasible trajectories without considering obstacles. We address the problem of autonomously landing a helicopter while considering a realistic context: multiple potential landing zones, geographical terrain, sensor limitations and pilot contextual knowledge. We designed a planning system to generate alternate routes (AR) that respect these factors till touchdown exploiting the human-in-loop to make a choice. This paper presents an algorithm, RRT*-AR, building upon the optimal sampling-based algorithm RRT* to generate AR in realtime and examines its performance for simulated failures occurring in mountainous terrain, while maintaining optimality guarantees. After over 4500 trials, RRT*-AR outperformed RRT* by providing the human 280% more options 67% faster on average. As a result, it provides a much wider safety margin for unaccounted disturbances, and a more secure environment for a pilot. Using AR, the focus can now shift on delivering safety guarantees and handling uncertainties in these situations. \textcopyright 2013 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Cover:2013bv">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Sparse Tangential Network (SPARTAN): Motion planning for micro aerial vehicles</b>.</div><div class="csl-block csl-author">By Cover, H., Choudhury, S., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Karlsruhe, Germany, pp. 2820–2825, 2013.</div></div></span>
    <br />
<button class="button0" onclick="toggleCover2013bv()">bibtex</button>

<script>
    function toggleCover2013bv() {
        var x= document.getElementById('aCover:2013bv');
        // console.log("haha %o",typeof Cover:2013bv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Cover2013bv()">abstract</button>


<script>
    function toggle2Cover2013bv() {
        var x= document.getElementById('bCover:2013bv');
        // console.log("haha %o",typeof Cover:2013bv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2013.6630967"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aCover:2013bv" style="display:none"><pre>@inproceedings{Cover:2013bv,
  address = {Karlsruhe, Germany},
  author = {Cover, Hugh and Choudhury, Sanjiban and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2013.6630967},
  isbn = {9781467356411},
  issn = {10504729},
  month = may,
  pages = {2820--2825},
  title = {Sparse Tangential Network {(SPARTAN)}: Motion planning for micro aerial vehicles},
  year = {2013}
}
</pre></div>
<div id="bCover:2013bv" style="display:none"><pre>Micro aerial vehicles operating outdoors must be able to maneuver through both dense vegetation and across empty fields. Existing approaches do not exploit the nature of such an environment. We have designed an algorithm which plans rapidly through free space and is efficiently guided around obstacles. In this paper we present SPARTAN (Sparse Tangential Network) as an approach to create a sparsely connected graph across a tangential surface around obstacles. We find that SPARTAN can navigate a vehicle autonomously through an outdoor environment producing plans 172 times faster than the state of the art (RRT*). As a result SPARTAN can reliably deliver safe plans, with low latency, using the limited computational resources of a lightweight aerial vehicle. \textcopyright 2013 IEEE.</pre></div>
</li></ul.no-bullet>

<h1 id="2012">2012</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Scherer:2012ke">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Autonomous landing at unprepared sites by a full-scale helicopter</b>. </div><div class="csl-block csl-author">By Scherer, S., Chamberlain, L. and Singh, S.</div><div class="csl-block csl-event">In <i>Robotics and Autonomous Systems</i>, vol. 60, no. 12, pp. 1545–1562, Dec. 2012.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2012ke()">bibtex</button>

<script>
    function toggleScherer2012ke() {
        var x= document.getElementById('aScherer:2012ke');
        // console.log("haha %o",typeof Scherer:2012ke);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2012ke()">abstract</button>


<script>
    function toggle2Scherer2012ke() {
        var x= document.getElementById('bScherer:2012ke');
        // console.log("haha %o",typeof Scherer:2012ke);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1016/j.robot.2012.09.004"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer:2012ke" style="display:none"><pre>@article{Scherer:2012ke,
  author = {Scherer, Sebastian and Chamberlain, Lyle and Singh, Sanjiv},
  doi = {10.1016/j.robot.2012.09.004},
  issn = {09218890},
  journal = {Robotics and Autonomous Systems},
  keywords = {3D perception,Landing zone selection,Lidar,Rotorcraft,UAV},
  month = dec,
  number = {12},
  pages = {1545--1562},
  title = {Autonomous landing at unprepared sites by a full-scale helicopter},
  volume = {60},
  year = {2012}
}
</pre></div>
<div id="bScherer:2012ke" style="display:none"><pre>Helicopters are valuable since they can land at unprepared sites; however, current unmanned helicopters are unable to select or validate landing zones (LZs) and approach paths. For operation in unknown terrain it is necessary to assess the safety of a LZ. In this paper, we describe a lidar-based perception system that enables a full-scale autonomous helicopter to identify and land in previously unmapped terrain with no human input. We describe the problem, real-time algorithms, perception hardware, and results. Our approach has extended the state of the art in terrain assessment by incorporating not only plane fitting, but by also considering factors such as terrain/skid interaction, rotor and tail clearance, wind direction, clear approach/abort paths, and ground paths. In results from urban and natural environments we were able to successfully classify LZs from point cloud maps. We also present results from 8 successful landing experiments with varying ground clutter and approach directions. The helicopter selected its own landing site, approaches, and then proceeds to land. To our knowledge, these experiments were the first demonstration of a full-scale autonomous helicopter that selected its own landing zones and landed. \textcopyright 2012 Elsevier Ltd. All rights reserved.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Scherer:jy">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>First results in autonomous landing and obstacle avoidance by a full-scale helicopter</b>.</div><div class="csl-block csl-author">By Scherer, S., Chamberlain, L. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, St. Paul, MN, pp. 951–956, 2012.</div></div></span>
    <br />
<button class="button0" onclick="toggleSchererjy()">bibtex</button>

<script>
    function toggleSchererjy() {
        var x= document.getElementById('aScherer:jy');
        // console.log("haha %o",typeof Scherer:jy);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Schererjy()">abstract</button>


<script>
    function toggle2Schererjy() {
        var x= document.getElementById('bScherer:jy');
        // console.log("haha %o",typeof Scherer:jy);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2012.6225215"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer:jy" style="display:none"><pre>@inproceedings{Scherer:jy,
  address = {St. Paul, MN},
  author = {Scherer, Sebastian and Chamberlain, Lyle and Singh, Sanjiv},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2012.6225215},
  isbn = {9781467314039},
  issn = {10504729},
  month = may,
  pages = {951--956},
  title = {First results in autonomous landing and obstacle avoidance by a full-scale helicopter},
  year = {2012}
}
</pre></div>
<div id="bScherer:jy" style="display:none"><pre>Currently deployed unmanned rotorcraft rely on carefully preplanned missions and operate from prepared sites and thus avoid the need to perceive and react to the environment. Here we consider the problems of finding suitable but previously unmapped landing sites given general coordinates of the goal and planning collision free trajectories in real time to land at the "optimal" site. This requires accurate mapping, fast landing zone evaluation algorithms, and motion planning. We report here on the sensing, perception and motion planning integrated onto a full-scale helicopter that flies completely autonomously. We show results from 8 experiments for landing site selection and 5 runs at obstacles. These experiments have demonstrated the first autonomous full-scale helicopter that successfully selects its own landing sites and avoids obstacles. \textcopyright 2012 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Choudhury:2012ua">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Realtime alternate routes planning: the RRT*-AR algorithm</b>.</div><div class="csl-block csl-author">By Choudhury, S., Scherer, S. and Singh, S.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-12-2, 2012</div></div></span>
    <br />
<button class="button0" onclick="toggleChoudhury2012ua()">bibtex</button>

<script>
    function toggleChoudhury2012ua() {
        var x= document.getElementById('aChoudhury:2012ua');
        // console.log("haha %o",typeof Choudhury:2012ua);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Choudhury2012ua()">abstract</button>


<script>
    function toggle2Choudhury2012ua() {
        var x= document.getElementById('bChoudhury:2012ua');
        // console.log("haha %o",typeof Choudhury:2012ua);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://repository.cmu.edu/robotics/918/?utm_source=repository.cmu.edu/robotics/918&amp;utm_medium=PDF&amp;utm_campaign=PDFCoverPages"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aChoudhury:2012ua" style="display:none"><pre>@techreport{Choudhury:2012ua,
  address = {Pittsburgh, PA},
  author = {Choudhury, Sanjiban and Scherer, Sebastian and Singh, Sanjiv},
  institution = {Carnegie Mellon University},
  number = {CMU-RI-TR-12-2},
  title = {Realtime alternate routes planning: the {RRT*-AR} algorithm},
  url = {http://repository.cmu.edu/robotics/918/?utm{\_}source=repository.cmu.edu/robotics/918{\&amp;}utm{\_}medium=PDF{\&amp;}utm{\_}campaign=PDFCoverPages},
  year = {2012}
}
</pre></div>
<div id="bChoudhury:2012ua" style="display:none"><pre>Motion planning in the most general sense is an optimization problem with a single elusive best solution. However attempting to find a single answer isn’t often the most desired approach. On the one hand, the reason is theoretical - planners often get trapped in local minimas because the cost function has many valleys or dynamics are too complex to fully exploit. On the other hand, there are many practical deterrents - unmapped obstacles might require the system to switch quickly to another plan, unmodelled dynamics can make a computed plan infeasible, or the system may have a human-in-loop who has a vote in the decision process. In situations where the current plan is no longer desirable, a new plan has to be planned. The re-planning time induces a reaction latency which might result in mission failure. We advocate the use of alternate routes (AR), a set of spatially different, locally optimal paths, as a powerful tool to address several of the afore-mentioned issues. By enforcing the routes to be spatially separated, appearance of unexpected obstacles has less chance of rendering all trajectories to be infeasible. In such cases, alternate routes act as a set of backup options which can be switched to instantly. This reduces reaction latency allowing the system to operate with a lower risk. This paper presents an algorithm, RRT*-AR, to generate alternate routes in real time by making tradeoffs in exploitation for exploration, precision for speed and leveraging assumptions about the vehicle and environment constraints. In the case of emergency landing of a helicopter, RRT*-AR outperformed RRT* by providing the human 280% more flight paths 67% faster on average. By planning multiple routes to potential landing zones, the planner was able to seamlessly switch to a new landing site without having to replan.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Scherer:2012hk">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>River mapping from a flying robot: State estimation, river detection, and obstacle mapping</b>. </div><div class="csl-block csl-author">By Scherer, S., Rehder, J., Achar, S., Cover, H., Chambers, A., Nuske, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Autonomous Robots</i>, vol. 33, no. 1-2, pp. 189–214, 2012.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2012hk()">bibtex</button>

<script>
    function toggleScherer2012hk() {
        var x= document.getElementById('aScherer:2012hk');
        // console.log("haha %o",typeof Scherer:2012hk);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2012hk()">abstract</button>


<script>
    function toggle2Scherer2012hk() {
        var x= document.getElementById('bScherer:2012hk');
        // console.log("haha %o",typeof Scherer:2012hk);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1007/s10514-012-9293-0"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer:2012hk" style="display:none"><pre>@article{Scherer:2012hk,
  author = {Scherer, Sebastian and Rehder, Joern and Achar, Supreeth and Cover, Hugh and Chambers, Andrew and Nuske, Stephen and Singh, Sanjiv},
  doi = {10.1007/s10514-012-9293-0},
  issn = {09295593},
  journal = {Autonomous Robots},
  keywords = {3D ladar scanning,3D obstacle mapping,Micro aerial vehicles,Self supervised learning,Visual localization},
  number = {1-2},
  pages = {189--214},
  title = {River mapping from a flying robot: State estimation, river detection, and obstacle mapping},
  volume = {33},
  year = {2012}
}
</pre></div>
<div id="bScherer:2012hk" style="display:none"><pre>Accurately mapping the course and vegetation along a river is challenging, since overhanging trees block GPS at ground level and occlude the shore line when viewed from higher altitudes. We present a multimodal perception system for the active exploration and mapping of a river from a small rotorcraft. We describe three key components that use computer vision, laser scanning, inertial sensing and intermittant GPS to estimate the motion of the rotorcraft, detect the river without a prior map, and create a 3D map of the riverine environment. Our hardware and software approach is cognizant of the need to perform multi-kilometer missions below tree level with size, weight and power constraints. We present experimental results along a 2 km loop of river using a surrogate perception payload. Overall we can build an accurate 3D obstacle map and a 2D map of the river course and width from light onboard sensing. \textcopyright 2012 Springer Science+Business Media, LLC.</pre></div>
</li></ul.no-bullet>

<h1 id="2011">2011</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Scherer:2011tw">[1]<b>Low-Altitude Operation of Unmanned Rotorcraft</b>, PhD thesis, The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 2011</span>
    <br />
<button class="button0" onclick="toggleScherer2011tw()">bibtex</button>

<script>
    function toggleScherer2011tw() {
        var x= document.getElementById('aScherer:2011tw');
        // console.log("haha %o",typeof Scherer:2011tw);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2011tw()">abstract</button>


<script>
    function toggle2Scherer2011tw() {
        var x= document.getElementById('bScherer:2011tw');
        // console.log("haha %o",typeof Scherer:2011tw);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>




<a href="http://ezproxy.net.ucf.edu/login?url=http://search.proquest.com/docview/884995854?accountid=10003%5Cnhttp://sfx.fcla.edu/ucf?url_ver=Z39.88-2004&amp;rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&amp;genre=dissertations+&amp;+theses&amp;sid=ProQ:ProQuest+Dissertations+&amp;+The"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aScherer:2011tw" style="display:none"><pre>@phdthesis{Scherer:2011tw,
  address = {Pittsburgh, PA},
  author = {Scherer, Sebastian},
  booktitle = {ProQuest Dissertations and Theses},
  isbn = {9781124819532},
  keywords = {0538:Aerospace engineering,0771:Robotics,0800:Artificial intelligence,Aerospace engineering,Applied sciences,Artificial intelligence,Low-altitude operation,Motion planning,Obstacle avoidance,Robotics,Rotorcraft,Unmanned aerial vehicles},
  pages = {138},
  school = {The Robotics Institute, Carnegie Mellon University},
  title = {Low-Altitude Operation of Unmanned Rotorcraft},
  url = {http://ezproxy.net.ucf.edu/login?url=http://search.proquest.com/docview/884995854?accountid=10003{\%}5Cnhttp://sfx.fcla.edu/ucf?url{\_}ver=Z39.88-2004{\&amp;}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:dissertation{\&amp;}genre=dissertations+{\&amp;}+theses{\&amp;}sid=ProQ:ProQuest+Dissertations+{\&amp;}+The},
  volume = {3468040},
  year = {2011},
  number = {CMU-RI-TR-11-03}
}
</pre></div>
<div id="bScherer:2011tw" style="display:none"><pre>Currently deployed unmanned rotorcraft rely on preplanned missions or teleoperation and do not actively incorporate information about obstacles, landing sites, wind, position uncertainty, and other aerial vehicles during online motion planning. Prior work has successfully addressed some tasks such as obstacle avoidance at slow speeds, or landing at known to be good locations. However, to enable autonomous missions in cluttered environments, the vehicle has to react quickly to previously unknown obstacles, respond to changing environmental conditions, and find unknown landing sites. We consider the problem of enabling autonomous operation at low-altitude with contributions to four problems. First we address the problem of fast obstacle avoidance for a small aerial vehicle and present results from over a 1000 rims at speeds up to 10 m/s. Fast response is achieved through a reactive algorithm whose response is learned based on observing a pilot. Second, we show an algorithm to update the obstacle cost expansion for path planning quickly and demonstrate it on a micro aerial vehicle, and an autonomous helicopter avoiding obstacles. Next, we examine the mission of finding a place to land near a ground goal. Good landing sites need to he detected and found and the final touch down goal is unknown. To detect the landing sites we convey a model based algorithm for landing sites that incorporates many helicopter relevant constraints such as landing sites, approach, abort, and ground paths in 3D range data. The landing site evaluation algorithm uses a patch-based coarse evaluation for slope and roughness, and a fine evaluation that fits a 3D model of the helicopter and landing gear to calculate a goodness measure. The data are evaluated in real-time to enable the helicopter to decide on a place to land. We show results from urban, vegetated, and desert environments, and demonstrate the first autonomous helicopter that selects its own landing sites. We present a generalized planning framework that enables reaching a goal point, searching for unknown landing sites, and approaching a landing zone. In the framework, sub-objective functions, constraints, and a state machine define the mission and behavior of an UAV. As the vehicle gathers information by moving through the environment, the objective functions account for this new information. The operator in this framework can directly specify his intent as an objective function that defines the mission rather than giving a sequence of pre-specified goal points. This allows the robot to react to new information received and adjust its path accordingly. The objective is used in a combined coarse planning and trajectory optimization algorithm to determine the best patch the robot should take. We show simulated results for several different missions and in particular focus on active landing zone search. We presented several effective approaches for perception and action for low-altitude flight and demonstrated their effectiveness in field experiments on three autonomous aerial vehicles: a 1m quadrocopter, a 36m helicopter, and a hill-size helicopter. These techniques permit rotorcraft to operate where they have their greatest advantage: In unstructured, unknown environments at low-altitude.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Scherer_2011_6890">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Multiple-objective motion planning for unmanned aerial vehicles</b>.</div><div class="csl-block csl-author">By Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, San Francisco, CA, pp. 2207–2214, 2011.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer_2011_6890()">bibtex</button>

<script>
    function toggleScherer_2011_6890() {
        var x= document.getElementById('aScherer_2011_6890');
        // console.log("haha %o",typeof Scherer_2011_6890);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer_2011_6890()">abstract</button>


<script>
    function toggle2Scherer_2011_6890() {
        var x= document.getElementById('bScherer_2011_6890');
        // console.log("haha %o",typeof Scherer_2011_6890);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2011.6048126"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer_2011_6890" style="display:none"><pre>@inproceedings{Scherer_2011_6890,
  address = {San Francisco, CA},
  author = {Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2011.6048126},
  isbn = {9781612844541},
  month = sep,
  pages = {2207--2214},
  title = {Multiple-objective motion planning for unmanned aerial vehicles},
  year = {2011}
}
</pre></div>
<div id="bScherer_2011_6890" style="display:none"><pre>Here we consider the problem of low-flying rotorcraft that must perform various missions such as navigating to specific goal points while avoiding obstacles, looking for acceptable landing sites or performing continuous surveillance. Not all of such missions can be expressed as safe, goal seeking, partly because in many cases there isn’t an obvious goal. Rather than developing singular solutions to each mission, we seek a generalized formulation that enables us to express a wider range of missions. Here we propose a framework that allows for multiple objectives to be considered simultaneously and discuss corresponding planning algorithms that are capable of running in realtime on autonomous air vehicles. The algorithms create a set of initial hypotheses that are then optimized by a sub-gradient based trajectory algorithm that optimizes the multiple objectives, producing dynamically feasible trajectories. We have demonstrated the feasibility of our approach with changing cost functions based on newly discovered information. We report on results in simulation of a system that is tasked with navigating safely between obstacles while searching for an acceptable landing site. \textcopyright 2011 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Molero_Fernandez_2011_6866">[3]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Navigation and Control for Micro Aerial Vehicles in GPS-Denied Environments</b>.</div><div class="csl-block csl-author">By Molero, R., Scherer, S. and Chamberlain, L.J.</div>Carnegie Mellon University, Pittsburgh, PA<div class="csl-block csl-author">Technical Report #CMU-RI-TR-10-08, Jun-2011</div></div></span>
    <br />
<button class="button0" onclick="toggleMolero_Fernandez_2011_6866()">bibtex</button>

<script>
    function toggleMolero_Fernandez_2011_6866() {
        var x= document.getElementById('aMolero_Fernandez_2011_6866');
        // console.log("haha %o",typeof Molero_Fernandez_2011_6866);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Molero_Fernandez_2011_6866()">abstract</button>


<script>
    function toggle2Molero_Fernandez_2011_6866() {
        var x= document.getElementById('bMolero_Fernandez_2011_6866');
        // console.log("haha %o",typeof Molero_Fernandez_2011_6866);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aMolero_Fernandez_2011_6866" style="display:none"><pre>@techreport{Molero_Fernandez_2011_6866,
  address = {Pittsburgh, PA},
  author = {Molero, Rudolph and Scherer, Sebastian and Chamberlain, Lyle J},
  booktitle = {CMU-RI-TR-10-08},
  institution = {Carnegie Mellon University},
  keywords = {GPS-denied,MAV,UAV,control,navigation},
  month = jun,
  number = {CMU-RI-TR-10-08},
  title = {Navigation and Control for Micro Aerial Vehicles in {GPS}-Denied Environments},
  year = {2011}
}
</pre></div>
<div id="bMolero_Fernandez_2011_6866" style="display:none"><pre>Micro-air vehicles have been increasingly employed in diverse research projects in both military and civilian applications. That is because their high maneuverability and accurate mobility. Many of them have been successfully used in outdoor areas, while some have been operated indoors. However, very few have dedicated especial attention to the case of high pitch and roll movements while doing scan-line based odometry. In this paper, we present a general approach consisting of algorithms that enable small aerial robots to fly indoors. We solve the overall problem of large movement change in pitch and roll angles by improving the standard scan matching algorithm. We also validate the effectiveness of the upgraded algorithm by a set of experiments that demonstrate the ability of a small quad-rotor to autonomously operate in cluttered indoor scenarios.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chambers_2011_6894">[4]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Perception for a river mapping robot</b>.</div><div class="csl-block csl-author">By Chambers, A., Achar, S., Nuske, S., Rehder, J., Kitt, B., Chamberlain, L., Haines, J., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, San Francisco, CA, pp. 227–234, 2011.</div></div></span>
    <br />
<button class="button0" onclick="toggleChambers_2011_6894()">bibtex</button>

<script>
    function toggleChambers_2011_6894() {
        var x= document.getElementById('aChambers_2011_6894');
        // console.log("haha %o",typeof Chambers_2011_6894);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chambers_2011_6894()">abstract</button>


<script>
    function toggle2Chambers_2011_6894() {
        var x= document.getElementById('bChambers_2011_6894');
        // console.log("haha %o",typeof Chambers_2011_6894);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2011.6048799"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aChambers_2011_6894" style="display:none"><pre>@inproceedings{Chambers_2011_6894,
  address = {San Francisco, CA},
  author = {Chambers, Andrew and Achar, Supreeth and Nuske, Stephen and Rehder, J{\"{o}}rn and Kitt, Bernd and Chamberlain, Lyle and Haines, Justin and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2011.6048799},
  isbn = {9781612844541},
  month = sep,
  pages = {227--234},
  title = {Perception for a river mapping robot},
  year = {2011}
}
</pre></div>
<div id="bChambers_2011_6894" style="display:none"><pre>Rivers with heavy vegetation are hard to map from the air. Here we consider the task of mapping their course and the vegetation along the shores with the specific intent of determining river width and canopy height. A complication in such riverine environments is that only intermittent GPS may be available depending on the thickness of the surrounding canopy. We present a multimodal perception system to be used for the active exploration and mapping of a river from a small rotorcraft flying a few meters above the water. We describe three key components that use computer vision, laser scanning, and inertial sensing to follow the river without the use of a prior map, estimate motion of the rotorcraft, ensure collision-free operation, and create a three dimensional representation of the riverine environment. While the ability to fly simplifies the navigation problem, it also introduces an additional set of constraints in terms of size, weight and power. Hence, our solutions are cognizant of the need to perform multi-kilometer missions with a small payload. We present experimental results along a 2km loop of river using a surrogate system. \textcopyright 2011 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chamberlain:2011uj">[5]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Self-aware helicopters: Full-scale automated landing and obstacle avoidance in unmapped environments</b>.</div><div class="csl-block csl-author">By Chamberlain, L., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Annual Forum Proceedings - AHS International</i>, Virginia Beachvol. 4, , pp. 3210–3219, 2011.</div></div></span>
    <br />
<button class="button0" onclick="toggleChamberlain2011uj()">bibtex</button>

<script>
    function toggleChamberlain2011uj() {
        var x= document.getElementById('aChamberlain:2011uj');
        // console.log("haha %o",typeof Chamberlain:2011uj);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Chamberlain2011uj()">abstract</button>


<script>
    function toggle2Chamberlain2011uj() {
        var x= document.getElementById('bChamberlain:2011uj');
        // console.log("haha %o",typeof Chamberlain:2011uj);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChamberlain:2011uj" style="display:none"><pre>@inproceedings{Chamberlain:2011uj,
  address = {Virginia Beach},
  author = {Chamberlain, Lyle and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {Annual Forum Proceedings - AHS International},
  isbn = {9781617828812},
  issn = {15522938},
  month = mar,
  pages = {3210--3219},
  title = {Self-aware helicopters: {Full}-scale automated landing and obstacle avoidance in unmapped environments},
  volume = {4},
  year = {2011}
}
</pre></div>
<div id="bChamberlain:2011uj" style="display:none"><pre>In this paper we present a perception and autonomy package that for the first time allows a full-scale unmanned helicopter (the Boeing Unmanned Little Bird) to automatically fly through unmapped, obstacle-laden terrain, find a landing zone, and perform a safe landing near a casualty, all with no human control or input. The system also demonstrates the ability to avoid obstacles while in low-altitude flight. The perception system consists of a 3D LADAR mapping unit with sufficient range, accuracy, and bandwidth to bring autonomous flight into the realm of full-scale aircraft. Efficient evaluation of this data and fast planning algorithms provide the aircraft with safe flight trajectories in real-time. We show the results of several fully autonomous landing and obstacle avoidance missions. Copyright \textcopyright 2011, American Helicopter Society International, Inc. All rights reserved.</pre></div>
</li>
<li><div class="text-justify">
    <span id="achar11">[6]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Self-supervised segmentation of river scenes</b>.</div><div class="csl-block csl-author">By Achar, S., Sankaran, B., Nuske, S., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Shanghai, China, pp. 6227–6232, 2011.</div></div></span>
    <br />
<button class="button0" onclick="toggleachar11()">bibtex</button>

<script>
    function toggleachar11() {
        var x= document.getElementById('aachar11');
        // console.log("haha %o",typeof achar11);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2achar11()">abstract</button>


<script>
    function toggle2achar11() {
        var x= document.getElementById('bachar11');
        // console.log("haha %o",typeof achar11);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ICRA.2011.5980157"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aachar11" style="display:none"><pre>@inproceedings{achar11,
  address = {Shanghai, China},
  author = {Achar, Supreeth and Sankaran, Bharath and Nuske, Stephen and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ICRA.2011.5980157},
  isbn = {9781612843865},
  issn = {10504729},
  month = may,
  pages = {6227--6232},
  title = {Self-supervised segmentation of river scenes},
  year = {2011}
}
</pre></div>
<div id="bachar11" style="display:none"><pre>Here we consider the problem of automatically segmenting images taken from a boat or low-flying aircraft. Such a capability is important for autonomous river following and mapping. The need for accurate segmentation in a wide variety of riverine environments challenges the state of the art vision-based methods that have been used in more structured environments such as roads and highways. Apart from the lack of structure, the principal difficulty is the large spatial and temporal variations in the appearance of water in the presence of nearby vegetation and with reflections from the sky. We propose a self-supervised method to segment images into ’sky’, ’river’ and ’shore’ (vegetation + structures) regions. Our approach uses assumptions about river scene structure to learn appearance models based on features like color, texture and image location which are used to segment the image. We validated our algorithm by testing on four datasets captured under varying conditions on different rivers. Our self-supervised algorithm had higher accuracy rates than a supervised alternative, often significantly more accurate, and does not need to be retrained to work under different conditions. \textcopyright 2011 IEEE.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Chambers:2011vm">[7]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Perception for a River Mapping Robot</b>.</div><div class="csl-block csl-author">By Chambers, A., Achar, S., Nuske, S., Rehder, J., Kitt, B., Chamberlain, L., Haines, J., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>Workshop on 3D Exploration, Mapping, and Surveillance with Aerial Robots, RSS</i>2011.</div></div></span>
    <br />
<button class="button0" onclick="toggleChambers2011vm()">bibtex</button>

<script>
    function toggleChambers2011vm() {
        var x= document.getElementById('aChambers:2011vm');
        // console.log("haha %o",typeof Chambers:2011vm);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2Chambers2011vm() {
        var x= document.getElementById('bChambers:2011vm');
        // console.log("haha %o",typeof Chambers:2011vm);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aChambers:2011vm" style="display:none"><pre>@inproceedings{Chambers:2011vm,
  author = {Chambers, Andrew and Achar, Supreeth and Nuske, Stephen and Rehder, Joern and Kitt, Bernd and Chamberlain, Lyle and Haines, Justin and Scherer, Sebastian and Singh, Sanjiv},
  title = {Perception for a River Mapping Robot},
  booktitle = {Workshop on 3D Exploration, Mapping, and Surveillance with Aerial Robots, RSS},
  year = {2011},
  month = jul
}
</pre></div>
<div id="bChambers:2011vm" style="display:none"><pre></pre></div>
</li></ul.no-bullet>

<h1 id="2010">2010</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Scherer:2010wi">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Online assessment of landing sites</b>.</div><div class="csl-block csl-author">By Scherer, S., Chamberlain, L. and Singh, S.</div><div class="csl-block csl-event">In <i>AIAA Infotech at Aerospace 2010</i>, Atlanta2010.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2010wi()">bibtex</button>

<script>
    function toggleScherer2010wi() {
        var x= document.getElementById('aScherer:2010wi');
        // console.log("haha %o",typeof Scherer:2010wi);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2010wi()">abstract</button>


<script>
    function toggle2Scherer2010wi() {
        var x= document.getElementById('bScherer:2010wi');
        // console.log("haha %o",typeof Scherer:2010wi);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.2514/6.2010-3358"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer:2010wi" style="display:none"><pre>@inproceedings{Scherer:2010wi,
  address = {Atlanta},
  author = {Scherer, Sebastian and Chamberlain, Lyle and Singh, Sanjiv},
  booktitle = {AIAA Infotech at Aerospace 2010},
  doi = {10.2514/6.2010-3358},
  isbn = {9781600867439},
  month = apr,
  title = {Online assessment of landing sites},
  year = {2010}
}
</pre></div>
<div id="bScherer:2010wi" style="display:none"><pre>Assessing a landing zone (LZ) reliably is essential for safe operation of vertical takeoff and landing (VTOL) aerial vehicles that land at unimproved locations. Currently an operator has to rely on visual assessment to make an approach decision; however. visual information from afar is insufficient to judge slope and detect small obstacles. Prior work has modeled LZ quality based on plane fitting, which only partly represents the interaction between vehicle and ground. Our approach consists of a coarse evaluation based on slope and roughness criteria, a fine evaluation for skid contact, and body clearance of a location. We investigated whether the evaluation is correct for using terrain maps collected from a helicopter. This paper defines the problem of evaluation, describes our incremental real-time algorithm, and discusses the efectiveness of our approach. In results from urban and natural environments, we were able to successfully classify LZs from point cloud maps collected on a helicopter. The presented method enables detailed assessment of LZs without an landing approach, thereby improving safety. Still, the method assumes low-noise point cloud data. We intend to increase robustness to outliers while still detecting small obstacles in future work.</pre></div>
</li></ul.no-bullet>

<h1 id="2009">2009</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Scherer:2009hu">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Efficient C-space and cost function updates in 3D for unmanned aerial vehicles</b>.</div><div class="csl-block csl-author">By Scherer, S., Ferguson, D. and Singh, S.</div><div class="csl-block csl-event">In <i>Proceedings - IEEE International Conference on Robotics and Automation</i>, Kobe, Japan, pp. 2049–2054, 2009.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2009hu()">bibtex</button>

<script>
    function toggleScherer2009hu() {
        var x= document.getElementById('aScherer:2009hu');
        // console.log("haha %o",typeof Scherer:2009hu);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2009hu()">abstract</button>


<script>
    function toggle2Scherer2009hu() {
        var x= document.getElementById('bScherer:2009hu');
        // console.log("haha %o",typeof Scherer:2009hu);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/ROBOT.2009.5152790"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer:2009hu" style="display:none"><pre>@inproceedings{Scherer:2009hu,
  address = {Kobe, Japan},
  author = {Scherer, Sebastian and Ferguson, Dave and Singh, Sanjiv},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  doi = {10.1109/ROBOT.2009.5152790},
  isbn = {9781424427895},
  issn = {10504729},
  month = may,
  pages = {2049--2054},
  title = {Efficient {C}-space and cost function updates in {3D} for unmanned aerial vehicles},
  year = {2009}
}
</pre></div>
<div id="bScherer:2009hu" style="display:none"><pre>When operating in partially-known environments, autonomous vehicles must constantly update their maps and plans based on new sensor information. Much focus has been placed on developing efficient incremental planning algorithms that are able to efficiently replan when the map and associated cost function changes. However, much less attention has been placed on efficiently updating the cost function used by these planners, which can represent a significant portion of the time spent replanning. In this paper, we present the Limited Incremental Distance Transform algorithm, which can be used to efficiently update the cost function used for planning when changes in the environment are observed. Using this algorithm it is possible to plan paths in a completely incremental way starting from a list of changed obstacle classifications. We present results comparing the algorithm to the Euclidean distance transform and a mask-based incremental distance transform algorithm. Computation time is reduced by an order of magnitude for a UAV application. We also provide example results from an autonomous micro aerial vehicle with on-board sensing and computing.\textcopyright 2009 IEEE.</pre></div>
</li></ul.no-bullet>

<h1 id="2008">2008</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Scherer:2008kx">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Flying fast and low among obstacles: Methodology and experiments</b>. </div><div class="csl-block csl-author">By Scherer, S., Singh, S., Chamberlain, L. and Elgersma, M.</div><div class="csl-block csl-event">In <i>International Journal of Robotics Research</i>, vol. 27, no. 5, pp. 549–574, 2008.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2008kx()">bibtex</button>

<script>
    function toggleScherer2008kx() {
        var x= document.getElementById('aScherer:2008kx');
        // console.log("haha %o",typeof Scherer:2008kx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2008kx()">abstract</button>


<script>
    function toggle2Scherer2008kx() {
        var x= document.getElementById('bScherer:2008kx');
        // console.log("haha %o",typeof Scherer:2008kx);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1177/0278364908090949"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aScherer:2008kx" style="display:none"><pre>@article{Scherer:2008kx,
  author = {Scherer, Sebastian and Singh, Sanjiv and Chamberlain, Lyle and Elgersma, Mike},
  doi = {10.1177/0278364908090949},
  issn = {02783649},
  journal = {International Journal of Robotics Research},
  keywords = {Aerial robotics,Learning},
  number = {5},
  pages = {549--574},
  title = {Flying fast and low among obstacles: {Methodology} and experiments},
  volume = {27},
  year = {2008}
}
</pre></div>
<div id="bScherer:2008kx" style="display:none"><pre>Safe autonomous flight is essential for widespread acceptance of aircraft that must fly close to the ground. We have developed a method of collision avoidance that can be used in three dimensions in much the same way as autonomous ground vehicles that navigate over unexplored terrain. Safe navigation is accomplished by a combination of online environmental sensing, path planning and collision avoidance. Here we outline our methodology and report results with an autonomous helicopter that operates at low elevations in uncharted environments, some of which are densely populated with obstacles such as buildings, trees and wires. We have recently completed over 700 successful runs in which the helicopter traveled between coarsely specified waypoints separated by hundreds of meters, at speeds of up to 10 m s-1 at elevations of 5-11 m above ground level. The helicopter safely avoids large objects such as buildings and trees but also wires as thin as 6 mm. We believe this represents the first time an air vehicle has traveled this fast so close to obstacles. The collision avoidance method learns to avoid obstacles by observing the performance of a human operator. \textcopyright SAGE Publications 2008.</pre></div>
</li></ul.no-bullet>

<h1 id="2007">2007</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="SchererSinghChamberlain07">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Flying Fast and Low Among Obstacles</b>.</div><div class="csl-block csl-author">By Scherer, S., Singh, S., Chamberlain, L. and Saripalli, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Robotics and Automation ICRA</i>, Rome, Italy, pp. 2023–2029, 2007.</div></div></span>
    <br />
<button class="button0" onclick="toggleSchererSinghChamberlain07()">bibtex</button>

<script>
    function toggleSchererSinghChamberlain07() {
        var x= document.getElementById('aSchererSinghChamberlain07');
        // console.log("haha %o",typeof SchererSinghChamberlain07);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<script>
    function toggle2SchererSinghChamberlain07() {
        var x= document.getElementById('bSchererSinghChamberlain07');
        // console.log("haha %o",typeof SchererSinghChamberlain07);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aSchererSinghChamberlain07" style="display:none"><pre>@inproceedings{SchererSinghChamberlain07,
  author = {Scherer, Sebastian and Singh, Sanjiv and Chamberlain, Lyle and Saripalli, Srikanth},
  booktitle = {IEEE International Conference on Robotics and Automation ICRA},
  title = {{Flying Fast and Low Among Obstacles}},
  year = {2007},
  address = {Rome, Italy},
  month = may,
  pages = {2023--2029}
}
</pre></div>
<div id="bSchererSinghChamberlain07" style="display:none"><pre></pre></div>
</li>
<li><div class="text-justify">
    <span id="Urmson_2007_6906">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Tartan Racing: A Multi-Modal Approach to the DARPA Urban Challenge</b>.</div><div class="csl-block csl-author">By Urmson, C., Anhalt, J., Bagnell, D., Baker, C., Bittner, R., Dolan, J., Duggins, D., Ferguson, D., Galatali, T., Geyer, H., Gittleman, M., Harbaugh, S., Hebert, M., Howard, T.M., Kelly, A., Kohanbash, D., Likhachev, M., Miller, N., Peterson, K., Rajkumar, R., Rybski, P., Salesky, B., Scherer, S., Seo, Y.-W., Simmons, R., Singh, S., Snider, J., Stentz, A., Whittaker, W.R. and Ziglar, J.</div>Carnegie Mellon University<div class="csl-block csl-author">Apr-2007</div></div></span>
    <br />
<button class="button0" onclick="toggleUrmson_2007_6906()">bibtex</button>

<script>
    function toggleUrmson_2007_6906() {
        var x= document.getElementById('aUrmson_2007_6906');
        // console.log("haha %o",typeof Urmson_2007_6906);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Urmson_2007_6906()">abstract</button>


<script>
    function toggle2Urmson_2007_6906() {
        var x= document.getElementById('bUrmson_2007_6906');
        // console.log("haha %o",typeof Urmson_2007_6906);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1184/R1/6561125.v1"><input type="button" class="button1" value="doi" /></a>



<a href="https://kilthub.cmu.edu/articles/journal_contribution/Tartan_Racing_A_Multi-Modal_Approach_to_the_DARPA_Urban_Challenge/6561125/1"><input type="button" class="button4" value="link" /></a>


<!--  -->
</div>

<div id="aUrmson_2007_6906" style="display:none"><pre>@techreport{Urmson_2007_6906,
  author = {Urmson, Christopher and Anhalt, Joshua and Bagnell, Drew and Baker, Christopher and Bittner, Robert and Dolan, John and Duggins, Dave and Ferguson, David and Galatali, Tugrul and Geyer, Hartmut and Gittleman, Michele and Harbaugh, Sam and Hebert, Martial and Howard, Thomas M and Kelly, Alonzo and Kohanbash, David and Likhachev, Maxim and Miller, Nick and Peterson, Kevin and Rajkumar, Raj and Rybski, Paul and Salesky, Bryan and Scherer, Sebastian and Seo, Young-Woo and Simmons, R and Singh, Sanjiv and Snider, Jarrod and Stentz, Anthony and Whittaker, William Red and Ziglar, Jason},
  title = {{Tartan Racing:} A Multi-Modal Approach to the {DARPA} Urban Challenge},
  year = {2007},
  url = {https://kilthub.cmu.edu/articles/journal_contribution/Tartan_Racing_A_Multi-Modal_Approach_to_the_DARPA_Urban_Challenge/6561125/1},
  doi = {10.1184/R1/6561125.v1},
  publisher = {Carnegie Mellon University},
  month = apr
}
</pre></div>
<div id="bUrmson_2007_6906" style="display:none"><pre>The Urban Challenge represents a technological leap beyond the previous Grand Challenges. The challenge encompasses three primary behaviors: driving on roads, handling intersections and maneuvering in zones. In implementing urban driving we have decomposed the problem into five components. Mission Planning determines an efficient route through an urban network of roads. A behavioral layer executes the route through the environment, adapting to local traffic and exceptional situations as necessary. A motion planning layer safeguards the robot by considering the feasible trajectories available, and selecting the best option. Perception combines data from lidar, radar and vision systems to estimate the location of other vehicles, static obstacles and the shape of the road. Finally, the robot is a mechatronic system engineered to provide the power, sensing and mobility necessary to navigate an urban course. Rigorous component and system testing evaluates progress using standardized tests. Observations from these experiments shape the design of subsequent development spirals and enable the rapid detection and correction of bugs. The system described in the paper exhibits a majority of the basic navigation and traffic skills required for the Urban Challenge. From these building blocks more advanced capabilities will quickly develop.</pre></div>
</li></ul.no-bullet>

<h1 id="2006">2006</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Hamner:2006tu">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning obstacle avoidance parameters from operator behavior</b>. </div><div class="csl-block csl-author">By Hamner, B., Singh, S. and Scherer, S.</div><div class="csl-block csl-event">In <i>Journal of Field Robotics</i>, vol. 23, no. 11-12, pp. 1037–1058, 2006.</div></div></span>
    <br />
<button class="button0" onclick="toggleHamner2006tu()">bibtex</button>

<script>
    function toggleHamner2006tu() {
        var x= document.getElementById('aHamner:2006tu');
        // console.log("haha %o",typeof Hamner:2006tu);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Hamner2006tu()">abstract</button>


<script>
    function toggle2Hamner2006tu() {
        var x= document.getElementById('bHamner:2006tu');
        // console.log("haha %o",typeof Hamner:2006tu);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1002/rob.20171"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aHamner:2006tu" style="display:none"><pre>@article{Hamner:2006tu,
  author = {Hamner, Bradley and Singh, Sanjiv and Scherer, Sebastian},
  doi = {10.1002/rob.20171},
  issn = {15564959},
  journal = {Journal of Field Robotics},
  number = {11-12},
  pages = {1037--1058},
  title = {Learning obstacle avoidance parameters from operator behavior},
  volume = {23},
  year = {2006}
}
</pre></div>
<div id="bHamner:2006tu" style="display:none"><pre>This paper concerns an outdoor mobile robot that learns to avoid collisions by observing a human driver operate a vehicle equipped with sensors that continuously produce a map of the local environment. We have implemented steering control that models human behavior in trying to avoid obstacles while trying to follow a desired path. Here we present the formulation for this control system and its independent parameters and then show how these parameters can be automatically estimated by observing a human driver. We also present results from operation on an autonomous robot as well as in simulation, and compare the results from our method to another commonly used learning method. We find that the proposed method generalizes well and is capable of learning from a small number of samples. \textcopyright 2007 Wiley Periodicals, Inc.</pre></div>
</li>
<li><div class="text-justify">
    <span id="Hamner_2006_5535">[2]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Learning to drive among obstacles</b>.</div><div class="csl-block csl-author">By Hamner, B., Scherer, S. and Singh, S.</div><div class="csl-block csl-event">In <i>IEEE International Conference on Intelligent Robots and Systems</i>, Beijing, China, pp. 2663–2669, 2006.</div></div></span>
    <br />
<button class="button0" onclick="toggleHamner_2006_5535()">bibtex</button>

<script>
    function toggleHamner_2006_5535() {
        var x= document.getElementById('aHamner_2006_5535');
        // console.log("haha %o",typeof Hamner_2006_5535);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Hamner_2006_5535()">abstract</button>


<script>
    function toggle2Hamner_2006_5535() {
        var x= document.getElementById('bHamner_2006_5535');
        // console.log("haha %o",typeof Hamner_2006_5535);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>


<a href="http://doi.org/10.1109/IROS.2006.281987"><input type="button" class="button1" value="doi" /></a>




<!--  -->
</div>

<div id="aHamner_2006_5535" style="display:none"><pre>@inproceedings{Hamner_2006_5535,
  address = {Beijing, China},
  author = {Hamner, Bradley and Scherer, Sebastian and Singh, Sanjiv},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS.2006.281987},
  isbn = {142440259X},
  month = oct,
  pages = {2663--2669},
  title = {Learning to drive among obstacles},
  year = {2006}
}
</pre></div>
<div id="bHamner_2006_5535" style="display:none"><pre>This paper reports on an outdoor mobile robot that learns to avoid collisions by observing a human driver operate a vehicle equipped with sensors that continuously produce a map of the local environment. We have implemented steering control that models human behavior in trying to avoid obstacles while trying to follow a desired path. Here we present the formulation for this control system and its independent parameters, and then show how these parameters can be automatically estimated by observation of a human driver. We present results from experiments with a vehicle (both real and simulated) that avoids obstacles while following a prescribed path at speeds up to 4 m/sec. We compare the proposed method with another method based on Principal Component Analysis, a commonly used learning technique. We find that the proposed method generalizes well and is capable of learning from a small number of examples. \textcopyright 2006 IEEE.</pre></div>
</li></ul.no-bullet>

<h1 id="2005">2005</h1>
<ul.no-bullet class="bibliography"><li><div class="text-justify">
    <span id="Scherer:2005tv">[1]<div class="csl-block csl-content"><div class="csl-block csl-title"><b>Model checking of robotic control systems</b>.</div><div class="csl-block csl-author">By Scherer, S., Lerda, F. and Clarke, E.M.</div><div class="csl-block csl-event">In <i>European Space Agency, (Special Publication) ESA SP</i>, Munich, Germanyno. 603, , pp. 371–378, 2005.</div></div></span>
    <br />
<button class="button0" onclick="toggleScherer2005tv()">bibtex</button>

<script>
    function toggleScherer2005tv() {
        var x= document.getElementById('aScherer:2005tv');
        // console.log("haha %o",typeof Scherer:2005tv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>

<button class="button3" onclick="toggle2Scherer2005tv()">abstract</button>


<script>
    function toggle2Scherer2005tv() {
        var x= document.getElementById('bScherer:2005tv');
        // console.log("haha %o",typeof Scherer:2005tv);
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
    </script>





<!--  -->
</div>

<div id="aScherer:2005tv" style="display:none"><pre>@inproceedings{Scherer:2005tv,
  address = {Munich, Germany},
  author = {Scherer, S. and Lerda, F. and Clarke, E. M.},
  booktitle = {European Space Agency, (Special Publication) ESA SP},
  issn = {03796566},
  keywords = {Control Systems,Java,Model Checking,Software Testing,Verification},
  month = sep,
  number = {603},
  pages = {371--378},
  title = {Model checking of robotic control systems},
  year = {2005}
}
</pre></div>
<div id="bScherer:2005tv" style="display:none"><pre>Reliable software is important for robotic applications. We propose a new method for the verification of control software based on Java PathFinder, a discrete model checker developed at NASA Ames Research Center. Our extension of Java PathFinder supports modeling of a real-time scheduler and a physical system, defined in terms of differential equations. This approach not only is able to detect programming errors, like null-pointer dereferences, but also enables the verification of control software whose correctness depends on the physical, real-time environment. We applied this method to the control software of a line-following robot. The verified source code, written in Java, can be executed without any modifications on the microcontroller of the actual robot. Performance evaluation and bug finding are demonstrated on this example.</pre></div>
</li></ul.no-bullet>

</div>
                </div>
                
            </div>
        </div>
    </section>
     <style>
  #blocks {
      width:100%;
      height:60px;
      margin:0 auto;
      /* background-color: #ffe; */
  }
  #block1 {
      height:33.33%;
      width:30%;
      /* background: red; */
      float: left;
  }
  #block2 {
      height:33.33%;
      width:40%;
      /* background: yellow; */
      float: left;
  }
  #block3 {
      height:33.33%;
      width:30%;
      /* background: green; */
      float: right;
  }
</style>

<footer class="footer">
    <div class="container">
        <!-- 
        <div class="columns is-multiline">
            
            <div class="column has-text-centered">
                <div>
                    <a href="/" class="link">Home</a>
                </div>
            </div>
            
            <div class="column has-text-centered">
                <div>
                    <a href="/blog/" class="link">Blog</a>
                </div>
            </div>
            
            <div class="column has-text-centered">
                <div>
                    <a href="/products/" class="link">Products</a>
                </div>
            </div>
            
            <div class="column has-text-centered">
                <div>
                    <a href="/privacy-policy/" class="link">Privacy Policy</a>
                </div>
            </div>
            
        </div>
         -->
        <div id="blocks">
          <div id="block1"><img src="/img/logos/Horizontal@2x.png" alt="AirLab Logo" style="width:60%;"></div>
          <div id="block2">
            <center>

              <!-- <a class="button" itemprop="email" href="https://www.facebook.com" target="_blank">
                <i class="fab fa-github"></i>
              </a> -->
              <!-- <a class="button" itemprop="email" href="https://www.facebook.com" target="_blank">
                <i class="fab fa-facebook"</i>
              </a> -->
              <a class="button" itemprop="facebook" href="https://www.facebook.com/airlabcmu/" target="_blank">
                <i class="fab fa-facebook fa-lg" style="height:100%;"></i>
              </a>
              <a class="button" itemprop="twitter" href="https://www.twitter.com/airlabcmu/" target="_blank">
                <i class="fab fa-twitter fa-lg"></i>
              </a>
              <a class="button" itemprop="medium" href="https://medium.com/airlabcmu" target="_blank">
                <i class="fab fa-medium fa-lg"></i>
              </a>
              <a class="button" itemprop="github" href="https://github.com/castacks" target="_blank">
                <i class="fab fa-github fa-lg"></i>
              </a>
              <a class="button" itemprop="bitbucket" href="https://bitbucket.org/castacks/" target="_blank">
                <i class="fab fa-bitbucket fa-lg"></i>
              </a>
              <br>
              <br>
              <p class="">&copy; 2021 | Built using the <a href="https://github.com/chrisrhymes/bulma-clean-theme">Bulma Clean Theme</a></p>
            </center>
          </div>
          <div id="block3"><img src="/img/riLogo2019.svg" alt="RI Logo" style="float: right;"></div>
        </div>
        <!-- <div>
          <a href="" class="button is-large"><div class="icon"><i class="fab fa-facebook"</i></div></a>
        </div> -->
        <!-- <div class="content is-small has-text-centered">
            <p class="">© 2020</p>
        </div> -->
    </div>


</footer>
 
    <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>

</html>
